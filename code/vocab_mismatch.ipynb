{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrambling Sentences in Different GLUE tasks\n",
    "This file helps you scramble sentences in different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from vocab_mismatch_utils import *\n",
    "from data_formatter_utils import *\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import unicodedata\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n",
    "from transformers.utils import logging\n",
    "import torch\n",
    "logger = logging.get_logger(__name__)\n",
    "import numpy as np\n",
    "import copy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from word_forms.word_forms import get_word_forms\n",
    "from functools import partial\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 20}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "FILENAME_CONFIG = {\n",
    "    \"sst3\" : \"sst-tenary\",\n",
    "    \"cola\" : \"cola\",\n",
    "    \"mnli\" : \"mnli\",\n",
    "    \"snli\" : \"snli\",\n",
    "    \"mrpc\" : \"mrpc\",\n",
    "    \"qnli\" : \"qnli\",\n",
    "    \"conll2003\" : \"conll2003\"\n",
    "}\n",
    "TASK_CONFIG = {\n",
    "    \"wiki-text\": (\"text\", None),\n",
    "    \"sst3\": (\"text\", None),\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"snli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"conll2003\" : (\"words\", None)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inline Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizer helps you to get piece length for each token\n",
    "modified_tokenizer = ModifiedBertTokenizer(\n",
    "    vocab_file=\"../data-files/bert_vocab.txt\")\n",
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "\n",
    "def token_stats_mapping(task, example):\n",
    "    # for tasks that have single sentence\n",
    "    if task == \"sst3\" or task == \"wiki-text\" or task == \"cola\":\n",
    "        original_sentence = example[TASK_CONFIG[task][0]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            if len(original_sentence.strip()) != 0:\n",
    "                tokens, token_dict = modified_tokenizer.tokenize(original_sentence)\n",
    "                for token, pieces in token_dict.items():\n",
    "                    if token in token_frequency_map.keys():\n",
    "                        token_frequency_map[token] = token_frequency_map[token] + 1\n",
    "                    else:\n",
    "                        token_frequency_map[token] = 1\n",
    "                    _len = len(pieces)\n",
    "                    if _len in token_by_length.keys():\n",
    "                        if token not in token_by_length[_len]:\n",
    "                            token_by_length[_len].append(token)\n",
    "                    else:\n",
    "                        token_by_length[_len] = [token]\n",
    "    # for tasks that have two sentences\n",
    "    elif task == \"mrpc\" or task == \"mnli\" or task == \"snli\" or task == \"qnli\":\n",
    "        original_sentence = example[TASK_CONFIG[task][0]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            tokens, token_dict = modified_tokenizer.tokenize(original_sentence)\n",
    "            for token, pieces in token_dict.items():\n",
    "                if token in token_frequency_map.keys():\n",
    "                    token_frequency_map[token] = token_frequency_map[token] + 1\n",
    "                else:\n",
    "                    token_frequency_map[token] = 1\n",
    "                _len = len(pieces)\n",
    "                if _len in token_by_length.keys():\n",
    "                    if token not in token_by_length[_len]:\n",
    "                        token_by_length[_len].append(token)\n",
    "                else:\n",
    "                    token_by_length[_len] = [token]\n",
    "                \n",
    "        original_sentence = example[TASK_CONFIG[task][1]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            tokens, token_dict = modified_tokenizer.tokenize(original_sentence)\n",
    "            for token, pieces in token_dict.items():\n",
    "                if token in token_frequency_map.keys():\n",
    "                    token_frequency_map[token] = token_frequency_map[token] + 1\n",
    "                else:\n",
    "                    token_frequency_map[token] = 1\n",
    "                _len = len(pieces)\n",
    "                if _len in token_by_length.keys():\n",
    "                    if token not in token_by_length[_len]:\n",
    "                        token_by_length[_len].append(token)\n",
    "                else:\n",
    "                    token_by_length[_len] = [token]\n",
    "    else:\n",
    "        print(f\"task={task} not supported yet!\")\n",
    "    return example\n",
    "\n",
    "def generate_vocab_match_no_frequency_iv(token_by_length, token_frequency_map):\n",
    "    vocab_match = {}\n",
    "    tokens = list(task_token_frequency_map.keys())\n",
    "    tokens_copy = copy.deepcopy(tokens)\n",
    "    random.shuffle(tokens_copy)\n",
    "    for i in range(len(tokens)):\n",
    "        vocab_match[tokens[i]] = tokens_copy[i]\n",
    "    return vocab_match\n",
    "\n",
    "def generate_vocab_match_frequency_iv(token_by_length, token_frequency_map):\n",
    "    vocab_match = {}\n",
    "    for _, tokens in token_by_length.items():\n",
    "        tokens_copy = copy.deepcopy(tokens)\n",
    "        \n",
    "        # token_frequency_map, token_lemma_map)\n",
    "        \n",
    "        token_freq_tu = []\n",
    "        for t in tokens:\n",
    "            token_freq_tu.append((t, token_frequency_map[t]))\n",
    "        token_freq_tu = sorted(token_freq_tu, key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        matched_to = set([])\n",
    "        for i in trange(0, len(token_freq_tu)):\n",
    "            found = False\n",
    "            for j in range(0, len(token_freq_tu)):\n",
    "                word_i = token_freq_tu[i][0]\n",
    "                word_j = token_freq_tu[j][0]\n",
    "                if i != j and word_j not in matched_to and \\\n",
    "                    levenshteinDistance(word_i, word_j) > 0.3:\n",
    "                    matched_to.add(word_j)\n",
    "                    vocab_match[word_i] = word_j\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                vocab_match[word_i] = word_i\n",
    "            \n",
    "    return vocab_match\n",
    "\n",
    "def generate_vocab_match_abstract(token_frequency_map):\n",
    "    vocab_match = {}\n",
    "    vocab_list = []\n",
    "    for k,v in token_frequency_map.items():\n",
    "        vocab_list.append(k)\n",
    "    random.shuffle(vocab_list)\n",
    "    abstract_matches = []\n",
    "    for i in range(0, 4):\n",
    "        az_list = []\n",
    "        for j in range(ord('a'), ord('z')+1):\n",
    "            az_list.append(chr(j))\n",
    "        abstract_matches.append(az_list)\n",
    "    from itertools import product\n",
    "    abstract_matches = product(*abstract_matches)\n",
    "    abstract_english_pool = []\n",
    "    for match in abstract_matches:\n",
    "        abstract_english_pool.append(\"\".join(match))\n",
    "    for i in range(0, len(vocab_list)):\n",
    "        vocab_match[vocab_list[i]] = abstract_english_pool[i]\n",
    "    return vocab_match\n",
    "\n",
    "def generate_vocab_match_no_frequency_oov(wiki_token_frequency_map, \n",
    "                                          token_frequency_map,\n",
    "                                          match_high=False, \n",
    "                                          match_similar=False):\n",
    "    vocab_match = {}\n",
    "    wiki_vocab_to_use = []\n",
    "    if match_similar:\n",
    "        in_vocab_rank = []\n",
    "        for k, v in token_frequency_map.items():\n",
    "            in_vocab_rank.append(k)\n",
    "        wiki_tuples = []\n",
    "        for k, v in wiki_token_frequency_map.items():\n",
    "            if k not in task_token_frequency_map.keys():\n",
    "                wiki_tuples.append((k, v))\n",
    "        wiki_tuples = random.sample(wiki_tuples, k=len(in_vocab_rank))\n",
    "        wiki_tuples = sorted(wiki_tuples, key=lambda x: (x[1],x[1]), reverse=True)\n",
    "        for i in range(len(in_vocab_rank)):\n",
    "            vocab_match[in_vocab_rank[i]] = wiki_tuples[i][0]\n",
    "    else:\n",
    "        if not match_high:\n",
    "            for k, v in wiki_token_frequency_map.items():\n",
    "                if k not in task_token_frequency_map:\n",
    "                    if v == 1:\n",
    "                        wiki_vocab_to_use.append(k)\n",
    "            random.shuffle(wiki_vocab_to_use)\n",
    "            freq_idx = 0\n",
    "            for k, v in task_token_frequency_map.items():\n",
    "                vocab_match[k] = wiki_vocab_to_use[freq_idx]\n",
    "                freq_idx += 1\n",
    "        else:\n",
    "            in_vocab_rank = []\n",
    "            for k, v in token_frequency_map.items():\n",
    "                in_vocab_rank.append(k)\n",
    "            in_vocab_rank = in_vocab_rank[::-1] # reverse the order\n",
    "            idx = 0\n",
    "            for k, v in wiki_token_frequency_map.items():\n",
    "                if k not in task_token_frequency_map:\n",
    "                    wiki_vocab_to_use.append(k)\n",
    "                    idx += 1\n",
    "                    if idx == len(in_vocab_rank):\n",
    "                        break\n",
    "            assert len(wiki_vocab_to_use)==len(in_vocab_rank)\n",
    "            for i in range(len(wiki_vocab_to_use)):\n",
    "                vocab_match[in_vocab_rank[i]] = wiki_vocab_to_use[i]\n",
    "    return vocab_match\n",
    "\n",
    "def random_corrupt(task, tokenizer, vocab_match, example):\n",
    "    # for tasks that have single sentence\n",
    "    if task == \"sst3\" or task == \"wiki-text\" or task == \"cola\":\n",
    "        original_sentence = example[TASK_CONFIG[task][0]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            corrupted_sentence = corrupt_translator(original_sentence, tokenizer, vocab_match)\n",
    "            example[TASK_CONFIG[task][0]] = corrupted_sentence\n",
    "    # for tasks that have two sentences\n",
    "    elif task == \"mrpc\" or task == \"mnli\" or task == \"snli\" or task == \"qnli\":\n",
    "        original_sentence = example[TASK_CONFIG[task][0]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            corrupted_sentence = corrupt_translator(original_sentence, tokenizer, vocab_match)\n",
    "            example[TASK_CONFIG[task][0]] = corrupted_sentence\n",
    "        \n",
    "        original_sentence = example[TASK_CONFIG[task][1]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            corrupted_sentence = corrupt_translator(original_sentence, tokenizer, vocab_match)\n",
    "            example[TASK_CONFIG[task][1]] = corrupted_sentence\n",
    "    else:\n",
    "        print(f\"task={task} not supported yet!\")\n",
    "    return example\n",
    "\n",
    "def plot_dist(vocab, map1, map2, facecolor='b', post_fix=\"mismatched\"):\n",
    "    freq_diff = []\n",
    "    for k, v in vocab.items():\n",
    "        diff = abs(map1[k] - map2[v])\n",
    "        # print(diff)\n",
    "        freq_diff.append(diff)\n",
    "    fig = plt.figure(figsize=(8,3.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    g = ax.hist(freq_diff, bins=50, facecolor=facecolor, alpha=0.8)\n",
    "    # plt.grid(True)\n",
    "    # plt.grid(color='black', linestyle='-.')\n",
    "    import matplotlib.ticker as mtick\n",
    "    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n",
    "    ax.set_yscale('log')\n",
    "    plt.xlabel(f\"Difference in Frequencies\")\n",
    "    # plt.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n",
    "    plt.ylabel(\"Frequency (LOG)\")\n",
    "    ax.xaxis.set_major_formatter(mtick.FormatStrFormatter('%1.0e'))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8db49d2e254853a75e8b6d68e8903e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2180.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2d7e3adf384863a65cf2689857666d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1468.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 8.51 MiB, post-processed: Unknown size, total: 13.14 MiB) to /afs/cs.stanford.edu/u/wuzhengx/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/26b70ce2b0f32cb35a27151dbfa2dbe88c82bcdaf8f29433bcdc612a9b314e83...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f460c2ff642747a9baed0e6d90b92fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=649539.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a015e14c43944939ae0656c75cbd5899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=162714.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcf3a5984bb4a65bbde706098c7ad52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=145897.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /afs/cs.stanford.edu/u/wuzhengx/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/26b70ce2b0f32cb35a27151dbfa2dbe88c82bcdaf8f29433bcdc612a9b314e83. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({'train': Dataset(features: {'id': Value(dtype='string', id=None), 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'chunk': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, num_rows: 14041), 'validation': Dataset(features: {'id': Value(dtype='string', id=None), 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'chunk': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, num_rows: 3250), 'test': Dataset(features: {'id': Value(dtype='string', id=None), 'words': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'chunk': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, num_rows: 3453)})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wiki-Text Frequency for OOV\n",
    "\n",
    "If you want to explore out-of-task vocab swapping, you can look into these lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = False # turn it on if you want to run. It will take a long time to finish.\n",
    "if wiki:\n",
    "    wiki_datasets = DatasetDict.load_from_disk(os.path.join(external_output_dirname, \"wikitext-15M\"))\n",
    "    wiki_train_df = wiki_datasets['train']\n",
    "    wiki_eval_df = wiki_datasets['validation']\n",
    "    wiki_test_df = wiki_datasets['test']\n",
    "\n",
    "    token_by_length = {}\n",
    "    token_frequency_map = {}\n",
    "    wiki_train_df = wiki_train_df.map(partial(token_stats_mapping, \"wiki-text\"))\n",
    "    wiki_eval_df = wiki_eval_df.map(partial(token_stats_mapping, \"wiki-text\"))\n",
    "    wiki_test_df = wiki_test_df.map(partial(token_stats_mapping, \"wiki-text\"))\n",
    "    token_frequency_map = sorted(token_frequency_map.items(), key=operator.itemgetter(1), reverse=True) # copy\n",
    "    wiki_token_frequency_map = OrderedDict(token_frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mismatched vocab!\n",
    "Simply set your task name below for new tasks! Be sure to have your dataset downloaded into the right folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task setups\n",
    "task_name = \"cola\"\n",
    "# random seeds\n",
    "# WARNING: this may change your results as well. Try it a few different seeds.\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Get frequency distribution of the dataset.\n",
    "\n",
    "Note that we are cheating here a little bit. For tasks that have a pair of sentences, we consider the word frequency all together for both sentences. This is the worst case scenario as well, since two sentences may have different vocab distributions! If this works, we are confident more fine-grained scrambling should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                    f\"{FILENAME_CONFIG[task_name]}-train.tsv\"), \n",
    "                       delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-dev.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-test.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_by_length = {} # overwrite this everytime for a new dataset\n",
    "token_frequency_map = {} # overwrite this everytime for a new dataset\n",
    "train_df = train_df.map(partial(token_stats_mapping, task_name))\n",
    "eval_df = eval_df.map(partial(token_stats_mapping, task_name))\n",
    "test_df = test_df.map(partial(token_stats_mapping, task_name))\n",
    "task_token_by_length = OrderedDict(token_by_length)\n",
    "task_token_frequency_map = sorted(token_frequency_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "task_token_frequency_map = OrderedDict(task_token_frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2(a): Frequency matched in-vocab swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mismatch script\n",
    "vocab_match_frequency_iv = \\\n",
    "    generate_vocab_match_frequency_iv(task_token_by_length, \n",
    "                                      task_token_frequency_map)\n",
    "# plot the frequency distribution afterwards (the diff)\n",
    "plot_dist(vocab_match_frequency_iv, task_token_frequency_map, task_token_frequency_map, \n",
    "          facecolor='b', post_fix=\"matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to the disk if things looking good!\n",
    "corrupted_train_dataset = train_df.map(partial(random_corrupt, \n",
    "                                               task_name,\n",
    "                                               modified_basic_tokenizer, \n",
    "                                               vocab_match_frequency_iv))\n",
    "corrupted_validation_dataset = eval_df.map(partial(random_corrupt, \n",
    "                                                   task_name,\n",
    "                                                   modified_basic_tokenizer, \n",
    "                                                   vocab_match_frequency_iv))\n",
    "corrupted_test_dataset = test_df.map(partial(random_corrupt, \n",
    "                                             task_name,\n",
    "                                             modified_basic_tokenizer, \n",
    "                                             vocab_match_frequency_iv))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out an example sentence to show the scrambling.\n",
    "example = train_df[random.randint(0, len(train_df)-1)]\n",
    "for name in TASK_CONFIG[task_name]:\n",
    "    if name != None:\n",
    "        example_sentence = example[name]\n",
    "        print(f\"original {name}: {example_sentence}\")\n",
    "        corrupted = corrupt_translator(example_sentence, modified_basic_tokenizer, vocab_match_frequency_iv)\n",
    "        print(f\"scrambled {name}: {corrupted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2(a): Frequency mis-matched in-vocab swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mismatch script\n",
    "vocab_match_no_frequency_iv = \\\n",
    "    generate_vocab_match_no_frequency_iv(task_token_by_length, \n",
    "                                       task_token_frequency_map)\n",
    "# plot the frequency distribution afterwards (the diff)\n",
    "plot_dist(vocab_match_no_frequency_iv, task_token_frequency_map, task_token_frequency_map, \n",
    "          facecolor='r', post_fix=\"mismatched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to the disk if things looking good!\n",
    "corrupted_train_dataset = train_df.map(partial(random_corrupt, \n",
    "                                               task_name,\n",
    "                                               modified_basic_tokenizer, \n",
    "                                               vocab_match_no_frequency_iv))\n",
    "corrupted_validation_dataset = eval_df.map(partial(random_corrupt, \n",
    "                                                   task_name,\n",
    "                                                   modified_basic_tokenizer, \n",
    "                                                   vocab_match_no_frequency_iv))\n",
    "corrupted_test_dataset = test_df.map(partial(random_corrupt, \n",
    "                                             task_name,\n",
    "                                             modified_basic_tokenizer, \n",
    "                                             vocab_match_no_frequency_iv))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-mismatched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out an example sentence to show the scrambling.\n",
    "for name in TASK_CONFIG[task_name]:\n",
    "    if name != None:\n",
    "        example_sentence = example[name]\n",
    "        print(f\"original {name}: {example_sentence}\")\n",
    "        corrupted = corrupt_translator(example_sentence, modified_basic_tokenizer, vocab_match_no_frequency_iv)\n",
    "        print(f\"scrambled {name}: {corrupted}\")\n",
    "        unordered = corrupted.split(\" \")\n",
    "        random.shuffle(unordered)\n",
    "        unordered = \" \".join(unordered)\n",
    "        print(f\"reordered scrambled {name}: {unordered}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3. Not Even Word, Go Symbolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mismatch script\n",
    "vocab_match_abstract = \\\n",
    "    generate_vocab_match_abstract(task_token_frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to the disk if things looking good!\n",
    "corrupted_train_dataset = train_df.map(partial(random_corrupt, \n",
    "                                               task_name,\n",
    "                                               modified_basic_tokenizer, \n",
    "                                               vocab_match_abstract))\n",
    "corrupted_validation_dataset = eval_df.map(partial(random_corrupt, \n",
    "                                                   task_name,\n",
    "                                                   modified_basic_tokenizer, \n",
    "                                                   vocab_match_abstract))\n",
    "corrupted_test_dataset = test_df.map(partial(random_corrupt, \n",
    "                                             task_name,\n",
    "                                             modified_basic_tokenizer, \n",
    "                                             vocab_match_abstract))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out an example sentence to show the scrambling.\n",
    "for name in TASK_CONFIG[task_name]:\n",
    "    if name != None:\n",
    "        example_sentence = example[name]\n",
    "        print(f\"original {name}: {example_sentence}\")\n",
    "        corrupted = corrupt_translator(example_sentence, modified_basic_tokenizer, vocab_match_abstract)\n",
    "        print(f\"scrambled {name}: {corrupted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Step 4: Maybe let us mismatch with out-of-vocab English tokens. Essentially the frequency is 0 in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mismatch script\n",
    "vocab_match_oov = generate_vocab_match_no_frequency_oov(\n",
    "    wiki_token_frequency_map,\n",
    "    task_token_frequency_map,\n",
    "    match_high=True, \n",
    "    match_similar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to the disk if things looking good!\n",
    "corrupted_train_dataset = train_df.map(partial(random_corrupt, \n",
    "                                               task_name,\n",
    "                                               modified_basic_tokenizer, \n",
    "                                               vocab_match_oov))\n",
    "corrupted_validation_dataset = eval_df.map(partial(random_corrupt, \n",
    "                                                   task_name,\n",
    "                                                   modified_basic_tokenizer, \n",
    "                                                   vocab_match_oov))\n",
    "corrupted_test_dataset = test_df.map(partial(random_corrupt, \n",
    "                                             task_name,\n",
    "                                             modified_basic_tokenizer, \n",
    "                                             vocab_match_oov))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-matched-oov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anything you want to demo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
