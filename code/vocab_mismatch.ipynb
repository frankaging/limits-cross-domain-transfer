{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### these scripts will mismatch original BERT vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab_mismatch_utils import *\n",
    "from data_formatter_utils import *\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import unicodedata\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n",
    "from transformers.utils import logging\n",
    "import torch\n",
    "logger = logging.get_logger(__name__)\n",
    "import numpy as np\n",
    "import copy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "from word_forms.word_forms import get_word_forms\n",
    "\n",
    "seed = 42\n",
    "# set seeds again at start\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "font = {'family' : 'Times New Roman',\n",
    "        'size'   : 30}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizer helps you to get piece length for each token\n",
    "modified_tokenizer = ModifiedBertTokenizer(\n",
    "    vocab_file=\"../data-files/bert_vocab.txt\")\n",
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "\n",
    "# translator to try it out!\n",
    "def corrupt_translator(in_string, tokenizer, vocab_match):\n",
    "    tokens = tokenizer.tokenize(in_string)\n",
    "    translate_tokens = [vocab_match[t] for t in tokens]\n",
    "    out_string = \" \".join(translate_tokens).replace(\" ##\", \"\").strip()\n",
    "    return out_string\n",
    "\n",
    "def token_stats_mapping(task, example):\n",
    "    if task == \"sst3\" or task == \"wiki-text\":\n",
    "        original_sentence = example['text']\n",
    "        if len(original_sentence.strip()) != 0:\n",
    "            tokens, token_dict = modified_tokenizer.tokenize(original_sentence)\n",
    "            for token, pieces in token_dict.items():\n",
    "                if token in token_frequency_map.keys():\n",
    "                    token_frequency_map[token] = token_frequency_map[token] + 1\n",
    "                else:\n",
    "                    token_frequency_map[token] = 1\n",
    "                _len = len(pieces)\n",
    "                if _len in token_by_length.keys():\n",
    "                    if token not in token_by_length[_len]:\n",
    "                        token_by_length[_len].append(token)\n",
    "                else:\n",
    "                    token_by_length[_len] = [token]\n",
    "    elif task == \"cola\":\n",
    "        original_sentence = example['sentence']\n",
    "        tokens, token_dict = modified_tokenizer.tokenize(original_sentence)\n",
    "        for token, pieces in token_dict.items():\n",
    "            if token in token_frequency_map.keys():\n",
    "                token_frequency_map[token] = token_frequency_map[token] + 1\n",
    "            else:\n",
    "                token_frequency_map[token] = 1\n",
    "            _len = len(pieces)\n",
    "            if _len in token_by_length.keys():\n",
    "                if token not in token_by_length[_len]:\n",
    "                    token_by_length[_len].append(token)\n",
    "            else:\n",
    "                token_by_length[_len] = [token]\n",
    "    elif task == \"mrpc\":\n",
    "        original_sentence = example['sentence1']\n",
    "        tokens, token_dict = modified_tokenizer.tokenize(original_sentence)\n",
    "        for token, pieces in token_dict.items():\n",
    "            if token in token_frequency_map.keys():\n",
    "                token_frequency_map[token] = token_frequency_map[token] + 1\n",
    "            else:\n",
    "                token_frequency_map[token] = 1\n",
    "            _len = len(pieces)\n",
    "            if _len in token_by_length.keys():\n",
    "                if token not in token_by_length[_len]:\n",
    "                    token_by_length[_len].append(token)\n",
    "            else:\n",
    "                token_by_length[_len] = [token]\n",
    "                \n",
    "        original_sentence = example['sentence2']\n",
    "        tokens, token_dict = modified_tokenizer.tokenize(original_sentence)\n",
    "        for token, pieces in token_dict.items():\n",
    "            if token in token_frequency_map.keys():\n",
    "                token_frequency_map[token] = token_frequency_map[token] + 1\n",
    "            else:\n",
    "                token_frequency_map[token] = 1\n",
    "            _len = len(pieces)\n",
    "            if _len in token_by_length.keys():\n",
    "                if token not in token_by_length[_len]:\n",
    "                    token_by_length[_len].append(token)\n",
    "            else:\n",
    "                token_by_length[_len] = [token]\n",
    "    return example\n",
    "\n",
    "def token_lemma_mapping(word_dict):\n",
    "    token_lemma_map = {}\n",
    "    for k, v in word_dict.items():\n",
    "        external_forms = get_word_forms(k)\n",
    "        all_lemmas = set([])\n",
    "        for e_k, e_v in external_forms.items():\n",
    "            all_lemmas = all_lemmas.union(e_v)\n",
    "        token_lemma_map[k] = all_lemmas\n",
    "    return token_lemma_map\n",
    "\n",
    "def generate_vocab_match_frequency_iv(token_by_length, token_frequency_map, token_lemma_map):\n",
    "    vocab_match = {}\n",
    "    for _, tokens in token_by_length.items():\n",
    "        tokens_copy = copy.deepcopy(tokens)\n",
    "        \n",
    "        # token_frequency_map, token_lemma_map)\n",
    "        \n",
    "        token_freq_tu = []\n",
    "        for t in tokens:\n",
    "            token_freq_tu.append((t, token_frequency_map[t]))\n",
    "        token_freq_tu = sorted(token_freq_tu, key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        matched_to = set([])\n",
    "        for i in trange(0, len(token_freq_tu)):\n",
    "            found = False\n",
    "            for j in range(0, len(token_freq_tu)):\n",
    "                word_i = token_freq_tu[i][0]\n",
    "                word_j = token_freq_tu[j][0]\n",
    "                if i != j and word_j not in matched_to and \\\n",
    "                    len(token_lemma_map[word_i].intersection(token_lemma_map[word_j])) == 0 and \\\n",
    "                    levenshteinDistance(word_i, word_j) > 0.3:\n",
    "                    matched_to.add(word_j)\n",
    "                    vocab_match[word_i] = word_j\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                vocab_match[word_i] = word_i\n",
    "            \n",
    "    return vocab_match\n",
    "\n",
    "def generate_vocab_match_low_band_oov(task_token_frequency_map, wiki_token_frequency_map):\n",
    "    \"\"\"\n",
    "    we are not even considering length here, may need to consider as a factor\n",
    "    \"\"\"\n",
    "    tokens = list(task_token_frequency_map.keys())\n",
    "    tokens_copy = copy.deepcopy(tokens)\n",
    "    random.shuffle(tokens_copy)\n",
    "    low_band_freq_vocab = []\n",
    "    for k, v in wiki_token_frequency_map.items():\n",
    "        if v <= 2:\n",
    "            low_band_freq_vocab.append(k)\n",
    "    random.shuffle(low_band_freq_vocab)\n",
    "    vocab_match = {}\n",
    "    for i in range(len(tokens)):\n",
    "        vocab_match[tokens[i]] = low_band_freq_vocab[i]\n",
    "    return vocab_match\n",
    "\n",
    "def plot_dist(vocab, map1, map2):\n",
    "    freq_diff = []\n",
    "    for k, v in vocab.items():\n",
    "        diff = abs(map1[k] - map2[v])\n",
    "        print(diff)\n",
    "        freq_diff.append(diff)\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    g = ax.hist(freq_diff, bins=100, facecolor='r')\n",
    "    plt.grid(True)\n",
    "    plt.grid(color='black', linestyle='-.')\n",
    "    import matplotlib.ticker as mtick\n",
    "    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2e'))\n",
    "    ax.set_yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def random_corrupt(tokenizer, vocab_match, example):\n",
    "    original_sentence = example['text']\n",
    "    corrupted_sentence = corrupt_translator(original_sentence, tokenizer, vocab_match)\n",
    "    example['text'] = corrupted_sentence\n",
    "    return example\n",
    "\n",
    "def generate_vocab_match_high_band_oov(task_token_frequency_map, wiki_token_frequency_map):\n",
    "    \"\"\"\n",
    "    we are not even considering length here, may need to consider as a factor\n",
    "    \"\"\"\n",
    "    high_band_freq_vocab = []\n",
    "    count = len(task_token_frequency_map)\n",
    "    for k, v in wiki_token_frequency_map.items():\n",
    "        if count == 0:\n",
    "            break\n",
    "        high_band_freq_vocab.append(k)\n",
    "        count -= 1\n",
    "    high_band_freq_vocab = high_band_freq_vocab[::-1]\n",
    "    vocab_match = {}\n",
    "    tokens = list(task_token_frequency_map.keys())\n",
    "    for i in range(len(tokens)):\n",
    "        vocab_match[tokens[i]] = high_band_freq_vocab[i]\n",
    "    return vocab_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get mismatched vocab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wiki-Text Frequency for OOV**: Ger frequency for out-of-vocab words in wiki-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets = DatasetDict.load_from_disk(os.path.join(external_output_dirname, \"wikitext-15M\"))\n",
    "wiki_train_df = wiki_datasets['train']\n",
    "wiki_eval_df = wiki_datasets['validation']\n",
    "wiki_test_df = wiki_datasets['test']\n",
    "\n",
    "token_frequency_map = {} # overwrite this everytime for a new dataset\n",
    "wiki_train_df = wiki_train_df.map(partial(token_frequency_mapping, \"wiki-text\"))\n",
    "wiki_eval_df = wiki_eval_df.map(partial(token_frequency_mapping, \"wiki-text\"))\n",
    "wiki_test_df = wiki_test_df.map(partial(token_frequency_mapping, \"wiki-text\"))\n",
    "token_frequency_map = sorted(token_frequency_map.items(), key=operator.itemgetter(1), reverse=True) # copy\n",
    "wiki_token_frequency_map = OrderedDict(token_frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e60cb8506474924816ab1c807cca91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159274.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefb45fc13f448a8a8aaae41a7359abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b2764f97cc47bd8bce7d550b9d56c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# task setups\n",
    "task_name = \"sst3\"\n",
    "FILENAME_CONFIG = {\n",
    "    \"sst3\" : \"sst-tenary\"\n",
    "}\n",
    "\n",
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                    f\"{FILENAME_CONFIG[task_name]}-train.tsv\"), \n",
    "                       delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-dev.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-test.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "token_by_length = {} # overwrite this everytime for a new dataset\n",
    "token_frequency_map = {} # overwrite this everytime for a new dataset\n",
    "train_df = train_df.map(partial(token_stats_mapping, task_name))\n",
    "eval_df = eval_df.map(partial(token_stats_mapping, task_name))\n",
    "test_df = test_df.map(partial(token_stats_mapping, task_name))\n",
    "task_token_by_length = OrderedDict(token_by_length)\n",
    "task_token_frequency_map = sorted(token_frequency_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "task_token_frequency_map = OrderedDict(task_token_frequency_map)\n",
    "task_token_lemma_map = token_lemma_mapping(vocab_match_by_piece_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S1: Frequency-matched in-vocab swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10136/10136 [00:07<00:00, 1267.61it/s]\n",
      "100%|██████████| 2389/2389 [00:00<00:00, 6072.44it/s] \n",
      "100%|██████████| 4430/4430 [00:01<00:00, 3403.14it/s] \n",
      "100%|██████████| 743/743 [00:00<00:00, 12135.01it/s]\n",
      "100%|██████████| 111/111 [00:00<00:00, 16109.61it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 11724.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5652.70it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_match_frequency_iv = \\\n",
    "    generate_vocab_match_frequency_iv(task_token_by_length, \n",
    "                                      task_token_frequency_map, \n",
    "                                      task_token_lemma_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAF9CAYAAACH0lvIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df3wU13ku8OeNsC0KCbENqeXg2k5ximyapLYoqC2FEuOgpqrBbTCiwBU1oZVa+sPt3qaQuA61VLdKSXrJXW4IDQr2RYQ6xQlJF8d1AuWmwgbspsUWDSR1DImagO1giyBjKe/9Y0bWrLS7MyPNvLPSPN/PZz/Mao7OHj2rXV7NzpwjqgoiIiIiP29KegBEREQ0NrBoICIiokBYNBAREVEgLBqIiIgoEBYNREREFAiLBiIiIgpkQtIDKCdTp07VG264IbL+enp6MHny5Mj6o+KYtQ3mbIM522HWwx07duycqk4rtI9Fg8cNN9yAo0ePJj0MIiKixIjId4rt48cTMVq5cmXSQ0gNZm2DOdtgznaYdTjCGSEH1dTUKI80EBFRmonIMVWtKbSPRxpitGTJkqSHkBrM2gZztsGc7TDrcFg0xOjMmTNJDyE1mLUN5myDOdth1uGwaCAiIqJAWDQQERFRICwaiIiIKBAWDTHKZrNJDyE1mLUN5myDOdth1uGwaIhRV1dX0kNIDWZtgznbYM52mHU4LBoAiEi9iGw7f/58pP1WVVVF2h8Vx6xtMGcbzNkOsw6HRQMAVd2nquumTJkSab+9vb2R9kfFMWsbzNkGc7bDrMNh0RCjTZs2JT2E1GDWNpizDeZsh1mHw6KBiIiIAuEql3E6dgwQKd2Ga38QEdEYwSMNREREFAiLhhjNTnoAKTJ7NtO2wJxtMGc7zDocLo3tEfnS2H4fTQD8eIKIiMpKqpbGFpErRKRLRFRE+pIcy8okHzxlVq5k2haYsw3mbIdZhzPujjSIyF8C2AhAAPSrauCTPXmkgYiI0i41RxpEZBaAPwPw6aTHAgCLkx5AiixezLQtMGcbzNkOsw5n3BQNIvImANsBvASncEjcuaQHkCLnzjFtC8zZBnO2w6zDGU/zNKwHMAdAg6r+UIJ8NEBERESBhTrSICIVIjJLRBpFZIuIdIrIj9yTDlVE7g87AHHcLSJfEpEzIvKaiHSLyBMislZEfAsbEfkpAA8A2K+qu8OOgYiIiPyFPdKwB8BdUT24iFwJ4BEAC4fsusa9LQTQJCJLVfWFEl39HwAVAJqjGhsRERHlC3tOQ8WQ+y8BODmSBxaRywF8AYMFw2kAHwHQACADYGCR81sB5ETkLUX6+S0AdQA2qep/jWQscdmR9ABSZMcOpm2BOdtgznaYdThhi4anADwI4AMA3qGqVwNoHeFjNwGY524/DeDdqvqAqu5W1Y/BKRYec/ffDKegyCMiUwF8AsBxAH87wnHEpjPpAaRIZyfTtsCcbTBnO8w6nFAfT6jqSAuEPO55ChsHugWwWlVfHvJYvSKyGsC3AUwCsF5EHlTVFz3NPg7gagC/rqqvRzG2KM1MegApMnMm07bAnG0wZzvMOpykrp5YCGCau/2Eqj5bqJGq/kBEdgO4B8AVAO4E8BlPk3oAFwH8VZGrJSpE5IC7/Ueq+m8RjD2wbssHS7nubqZtgTnbYM52mHU4Sc3TcIdne79PW+/+QrNw/ASA+QVuAwbuvzX8MEenzfoBU6ytjWlbYM42mLMdZh1OUkcaZnm2j/m09c7r7P0+qGrRQkBEFCGnkSYiIqLikjrS8E7P9vM+bc8A6He3bxLO2kRERJSIpIoG7xGCknN4qmofgFfcuxPgnBRJRERExpI6dD/Zs90boP1FAFe6228G0BPVQERkHYB1AHDVVVehpiZ/Ya/6+no0NzejtbUVbW1tmDt37rA+crkcstksVqxYgVwuh507dwJwJp6oAbAMzuQTWTiXjCzwfrP7eAcOHEBLSwuam5vR0dGBPXv25D3G6tWrUVdXh127dqG5uRl1dXXDxnH48GFkMhls2LAB2WwW+/bty9u/bt061NbWIpfLoaGhAUuXLs3bX1FRgSeffBJNTU1oa2tDS0sLHn/88bw269evR3V1NQ4fPoy6ujo0NDTk7Z80aRIOHjyIxsZGbN++HZlMBocOHcprk8lkUFVVhRMnTqC2thZr1qzJ2z916lTs378fK1euxMMPP4ympiYcOXIkr819992HyspKdHd3o7q6GqdPn8577qZPn45HH330jT4aGxtx/PjxvD5aW1vR29uLixcvoqqqCvfee2/e/hkzZmD37t1YtWoVHnroISxfvhynTp3Ka7N582Z0d3dj4sSJqKysxIYNG/L2z5o1C+3t7W+MY8mSJThz5kxem2w2i66uLlRVVaG3txebNm3K2z979mxs3br1jT4WL148bL78HTt2oLOzEzNnzkR3d/ewz2nnzZuHtrY2rF27Fu3t7Zg/fz4uXLiQ16ajowO5XA5z585FV1cXtmzZkrd/0aJFmDNnDpqamrB161bMmTMH/f39eW327t2Ljo4O1NXVobOzE9u2bcvbP5rX04Bly5ahoaEB2WwWGzduxIIFC4b1MdZfTwO/z0m9npqb8+fJG8+vp3Pnzg1737d6PW3cuBGZTKbsX09eo14aW0QaMTiP0UdV9f4A33MJwGXu3cvcowml2n8XwLXu3WtVNZbTXaNeGrtPxL8q49LYkejr68OECTx9JW7M2QZztsOshyvHpbG9RwoqA7Sf6Nl+NeKxxGZt0gNIkbVrmbYF5myDOdth1uEkdaTh2wBudO/eqKrPl2g7Ac5HGBUAXgdwhY520MMfox5A/YwZMz548uSIZsUu1rF/Gx5pICKiMlKORxq+6dm+waftdAyueXEq6oIBAFR1n6qumzJlSqT9zvdvQhGZP59pW2DONpizHWYdTlJFg/cMmtt82nqrneNFW5WhC/5NKCJDT0KieDBnG8zZDrMOJ6mi4THP9vt82npngfSbPZKIiIhiklTR8DUAZ93t20XklkKNRORtAJa7d3vhLKUdORGpF5Ft58+fj6N7IiKicSGRosG9xLLFvSsAdorIld42IlIJ4LMYnMzpk0NWuIxyPLGc00BERDSehLo4VURuhLPipNe7PNsL3asdvD6vqs8U6G4rgN8AMA/ArQC+ISKfAnAKzsmP9wCodts+B+CBMGMtBx1JDyBFOjqYtgXmbIM522HW4YSd0eJ6OJMaFjPPvXmdAjCsaFDVSyJyJ4BH4CyVfR0KFwZPA1iqqmPus4McgJuSHkRK5HI53HQT044bc7bBnO0w63CSOqcBAKCqLwO4Hc55C18G8D0AlwB8H8BX4UzvPEdVX0hskKMwfEJPikuh6VMpeszZBnO2w6zDCVU0qOoBVZWQt3afPlVVP6eqv6aqb1fVK1T1GlV9r6p+2m+K6SjEdSJkV6S9USldXUzbAnO2wZztMOtwEj3SUC7iOhFyi38TisjQxWAoHszZBnO2w6zDYdFAREREgbBoICIiokBYNBAREVEgLBoQ34mQiyLtjUpZtIhpW2DONpizHWYdzqiXxh5Pampq9OjRo5H11yOCyX6NmH8kenp6MHmyb9o0SszZBnO2w6yHK8elsVMhk/QAUiSTYdoWmLMN5myHWYfDIw0eUR9pgIh/G+ZPRERlhEcaEjIn6QGkyJw5TNsCc7bBnO0w63BYNMSoP+kBpEh/P9O2wJxtMGc7zDocFg2I7+oJIiKi8YRFA+KbRpqIiGg8YdFAREREgbBoiNHepAeQInv3Mm0LzNkGc7bDrMNh0RCjjqQHkCIdHUzbAnO2wZztMOtwWDTEqC7pAaRIXR3TtsCcbTBnO8w6HBYNMepMegAp0tnJtC0wZxvM2Q6zDodFA+K75HJbpL1RKdu2MW0LzNkGc7bDrMNh0QBecklERBQEiwYiIiIKhEUDERERBcKiIUb1SQ8gRerrmbYF5myDOdth1uFwaWyPqJfGPiuCaX6NmH8kzp49i2nTfNOmUWLONpizHWY9HJfGTkhr0gNIkdZWpm2BOdtgznaYdTg80uAR9ZGGPhFM8GvE/CPR19eHCRN806ZRYs42mLMdZj0cjzQkZG7SA0iRuXOZtgXmbIM522HW4bBoICIiokBYNCC+GSGJiIjGExYN4IyQREREQbBoICIiokBYNMQol/QAUiSXY9oWmLMN5myHWYfDoiFG2aQHkCLZLNO2wJxtMGc7zDocFg0xWpH0AFJkxQqmbYE522DOdph1OCwaYsSDXnZ4iNEGc7bBnO0w63BYNMRoZ9IDSJGdO5m2BeZsgznbYdbhsGggIiKiQFg0EBERUSAsGoiIiCgQFg0xWpb0AFJk2TKmbYE522DOdph1OCwaYtSQ9ABSpKGBaVtgzjaYsx1mHQ6LBsS3YBWnDLHDCVpsMGcbzNkOsw5HVDXpMZSNmpoaPXr0aGT99Yhgsl8j5h+Jnp4eTJ7smzaNEnO2wZztMOvhROSYqtYU2scjDTFakPQAUmTBggVJDyEVmLMN5myHWYfDooGIiIgCYdFAREREgbBoICIiokBYNBAREVEgvHrCg1dPjF08A9oGc7bBnO0w6+HG/dUTIvLnIvIFETklIq+IyGsi8ryItIvILUmNqyWpB06hlhambYE522DOdph1OOPiSIOI9AL4MYB/B/A998uzANwE4BKApar6T379RH2k4bQIrvNrNA7yLwenT5/Gddf5pk2jxJxtMGc7zHq4cX+kAcAiAFeq6lxVvUtV7wLwMwD+AMDlAP5eRCZYD6rD+gFTrKODaVtgzjaYsx1mHc64KBpU9ZCqvjbka6qqWwB8C8A1AG62Htce6wdMsT17mLYF5myDOdth1uGMi6LBx+vuv5cSHQUREdEYF7poEJEKEZklIo0iskVEOkXkRyKi7u3+EfQpInK3iHxJRM64JzJ2i8gTIrJ2pB8tiMgqOB9TnHRvRERENEIj+c94D4C7ohqAiFwJ4BEAC4fsusa9LQTQJCJLVfUFn74+AuCnAUwCUA3gFjgnRjaoan9UYyYiIkqjkRQNFUPuvwTgRThXKoQiIpcD+AKAee6XTgPYBuAUgOkAfhvOf/63AsiJSK2qvlKiy/cDmOO5/x0Aq1X1WNixRWF1Eg+aUqtXM20LzNkGc7bDrMMZyTkNTwF4EMAHALxDVa8G0DrCx2/CYMHwNIB3q+oDqrpbVT8Gp1h4zN1/M4CPlOrMvXpCAFwFZ5HJ5wEcFJGNIxzfqNQl8aApVVfHtC0wZxvM2Q6zDid00aCqrar656r6iKr+10gf2D1PYeA/c4VzRODlIY/VC+cP9gvul9aLyNUBxviyqh4E8D44czf8pYjMHulYR2qX9QOm2K5dTNsCc7bBnO0w63CSvHpiIYBp7vYTqvpsoUaq+gMAu927VwC4M+gDuJdhfg6AAKgf+VBHptn6AVOsuZlpW2DONpizHWYdTpJFwx2e7f0+bb37F4d8nLPuv9NKtooBD3rZ4SFGG8zZBnO2w6zDSbJomOXZ9jtR0Tu386yirQqb7/77rZDfR0RERB7mUyt7vNOz/bxP2zMA+uFcuXGTiIi6i2aIyHvhnBPxtYGvuV+/HM4nBA0AfoTBjziIiIhoBJIsGt7q2T5XqqGq9onIKwCuhDPmSQB63N0/C+DjAL4vIk8DeBnORxE/C2eeh144J1meiXb4RERE6ZJk0eBdwLw3QPuLcIoGAHgzBouGHIAqOJdu/hyAqwG8BufoxecAbFHVoh9NiMg6AOsA4KqrrkJNTf7CXvX19WhubkZrayva2towd+7cYX3kcjlks1msWLECuVwOO3fuBAB0AagBsAzO4Y4snMtFFni/2X28AwcOoKWlBc3Nzejo6Bg2H/rq1atRV1eHXbt2obm5ueDncIcPH0Ymk8GGDRuQzWaxb9++vP3r1q1DbW0tcrkcGhoasHTp0rz9FRUVePLJJ9HU1IS2tja0tLTg8ccfz2uzfv16VFdX4/Dhw6irq0NDQ0Pe/kmTJuHgwYNobGzE9u3bkclkcOjQobw2mUwGVVVVOHHiBGpra7FmzZq8/VOnTsX+/fuxcuVKPPzww2hqasKRI0fy2tx3332orKxEd3c3qqur0dXVlffcTZ8+HY8++ugbfTQ2NuL48eN5fbS2tqK3txcXL15EVVUV7r333rz9M2bMwO7du7Fq1So89NBDWL58OU6dOpXXZvPmzeju7sbEiRNRWVmJDRs25O2fNWsW2tvb3xjHkiVLcOZMfv2azWbR1dWFqqoq9Pb2YtOmTXn7Z8+eja1bt77Rx+LFi3HuXH6dvWPHDnR2dmLmzJno7u5GW1tb3v558+ahra0Na9euRXt7O+bPn48LFy7kteno6EAul8PcuXPR1dWFLVu25O1ftGgR+vv70dTUhK1bt2LOnDno78+fM23v3r3o6OhAXV0dOjs7sW3btrz9o3k9DVi2bBkaGhqQzWaxceNGLFiwYFgfY/31NPD7nNTraejJgeP59XTy5Mlh7/tWr6eNGzcik8mU/evJK5KlsUWkEcAO9+5HVfX+AN9zCcBl7t3LVLXPp/13AVzr3r1WVbtHNtriol4au0/Evyrj0tiR6Ovrw4QJSdbA6cCcbTBnO8x6uHJdGrvHs10ZoP1Ez/arEY8lFpmkB5AimQzTtsCcbTBnO8w6nCSPNHwbwI3u3RtV9fkSbSfA+QijAs6qlVdoFAMf7L8eQP2MGTM+ePJkdOtanRXxv86TRxoicfbsWUybZn5VbeowZxvM2Q6zHq5cjzR807N9g0/b6Rhc8+JUlAUDAKjqPlVdN2XKlCi7RTbS3qiUbJZpW2DONpizHWYdTpJFg/csmtt82nornuNFW5WZff5NKCJDT1KjeDBnG8zZDrMOJ8mi4THP9vt82npngfSbPZKIiIhikGTR8DUMTvF8u4jcUqiRiLwNwHL3bi+cpbQjJSL1IrLt/PnzUXdNREQ0biRWNLiXWLa4dwXAThG50ttGRCoBfBbOZE4A8ElVfTGGscRyTgMREdF4EvriVBG5EcA9Q778Ls/2QvdqB6/Pq+ozBbrbCuA34EzMdCuAb4jIpwCcgnPy4z0Aqt22zwF4IOx4k7Qu6QGkyLp1TNsCc7bBnO0w63BGMqPF9XAmNixmnnvzOgVgWNGgqpdE5E4Aj8BZKvs6FC4MngawVFXH1OcHtUkPIEVqa5m2BeZsgznbYdbhJHlOAwBAVV8GcDuc8xa+DOB7AC4B+D6Ar8L5g32Oqr6Q2CBHKJf0AFIkl2PaFpizDeZsh1mHE8nkTmNdXJM7nRbBdX6NmH8kTp8+jeuu802bRok522DOdpj1cOU6uVPZiOtEyKX+TSgiQxcLongwZxvM2Q6zDodFAxEREQXCooGIiIgCYdFAREREgbBoQHwzQlb4N6GIVFQwbQvM2QZztsOsw+HVEx41NTV69OjR6DoU8W/D/ImIqIzw6omENCU9gBRpamLaFpizDeZsh1mHwyMNHlEfaegRwWS/Rsw/Ej09PZg82TdtGiXmbIM522HWw/FIQ0Ja/JtQRFpamLYF5myDOdth1uGwaIjR40kPIEUef5xpW2DONpizHWYdDosGxHf1BBER0XjCogHxTSNNREQ0nrBoICIiokBYNMRofdIDSJH165m2BeZsgznbYdbhsGiIUXXSA0iR6mqmbYE522DOdph1OCwaYnQ46QGkyOHDTNsCc7bBnO0w63BYNMSoLukBpEhdHdO2wJxtMGc7zDocFg2I75LLhkh7o1IaGpi2BeZsgznbYdbhsGgAL7kkIiIKgkUDERERBcKigYiIiAJh0RCjSUkPIEUmTWLaFpizDeZsh1mHw6WxPaJeGhsi/m2YPxERlREujZ2QxqQHkCKNjY1JDyEVmLMN5myHWYfDIw0eUR9p6BPBBL9GzD8SfX19mDDBN20aJeZsgznbYdbD8UhDQjJJDyBFMhmmbYE522DOdph1OCwaYnQo6QGkyKFDTNsCc7bBnO0w63BYNCC+GSGJiIjGExYN4IyQREREQbBoICIiokBYNMSIp9fY4clMNpizDeZsh1mHw6IhRlVJDyBFqqqYtgXmbIM522HW4bBoiNGJpAeQIidOMG0LzNkGc7bDrMNh0RCj2qQHkCK1tUzbAnO2wZztMOtwWDTEaE3SA0iRNWuYtgXmbIM522HW4bBoICIiokBYNBAREVEgLBqIiIgoEBYNMZqa9ABSZOpUpm2BOdtgznaYdThcGtsj6qWxIeLfhvkTEVEZ4dLYPuJasGplpL1RKStXMm0LzNkGc7bDrMPhkQYPHmkgIqK045GGhDQlPYAUaWpi2haYsw3mbIdZh8OiIUZHkh5Aihw5wrQtMGcbzNkOsw6HRQMREREFwqKBiIiIAmHRQERERIGwaIjRfUkPIEXuu49pW2DONpizHWYdDouGGFUmPYAUqaxk2haYsw3mbIdZhzPmiwYR+QkRWSIify8i/ykivSJyQUS+ISL3icjkpMbWndQDp1B3N9O2wJxtMGc7zDqcMV80AFgBYC+A3wbQD+CLAA4BuBHARwEcEZG3JTGw6iQeNKWqq5m2BeZsgznbYdbhjIei4XUA2wDcrKo3q+oyVV0M4GcAPANgJoBPJDGw5iQeNKWam5m2BeZsgznbYdbhjPmiQVU/q6q/o6pdQ77eDeD33Lt3icjl9qMjIiIaP8Z80eDjG+6/VwC4OsmBEBERjXWhiwYRqRCRWSLSKCJbRKRTRH4kIure7h9BnyIid4vIl0TkjIi8JiLdIvKEiKwVkQlh+3S9w/33dQAvjbAPIiIiAjCS/4z3ALgrqgGIyJUAHgGwcMiua9zbQgBNIrJUVV8I2f0fuv/uV9XXRjfS8KZbP2CKTZ/OtC0wZxvM2Q6zDif00tgi8iiAOz1fegnAiwBucu9/VFXvD9jX5QD+GcA890un4ZzUeArO/7m/jcGLEJ4DUKuqrwTs+1cBfAlAH4DZqvoNn2/h0thERJR6US+N/RSABwF8AMA7VPVqAK0jHFsTBguGpwG8W1UfUNXdqvoxALcCeMzdfzOAjwTpVERmAngYgAD40yAFQxxWJvGgKbVyJdO2wJxtMGc7zDqc0EcaCnYi0ghgh3s30JEG9zyF7wGYBkAB/KyqPlug3dsAfBvAJACvAXi7qr5Yot+3A/g6gOsB/I2q/lnQn4NHGoiIKO2iPtIQlYVwCgYAeKJQwQAAqvoDALvdu1cg/6ORPCJyFYCvwCkYtocpGOLQmOSDp0xjY2PSQ0gF5myDOdth1uEkWTTc4dne79PWu39xoQbudNE5OB9jPALgd0c1uggcT3oAKXL8ONO2wJxtMGc7zDqcJIuGWZ7tYz5tvZ8ZzBq6U0SuAPAFAD8P5xyI31LV/lGPkIiIiN6QZNHwTs/28z5tz8BZVwIAbhIZPFlARCoAdMD5uOMQgLtU9VKE4yQiIiKMbJ6GqLzVs32uVENV7RORVwBcCWfMkwD0uLt/H8BSTz9ZKXwC4oOqemLoF0VkHYB1AHDVVVehpib/3I/6+no0NzejtbUVbW1tmDt37rCOc7kcstksVqxYgVwuh507dwIAugDUAFgGoAFAFsBGAAu83+w+3oEDB9DS0oLm5mZ0dHRgz549eY+xevVq1NXVYdeuXWhubkZdXd2wcRw+fBiZTAYbNmxANpvFvn378vavW7cOtbW1yOVyaGhowNKlS/P2V1RU4Mknn0RTUxPa2trQ0tKCxx9/PK/N+vXrUV1djcOHD6Ourg4NDQ15+ydNmoSDBw+isbER27dvRyaTwaFDh/LaZDIZVFVV4cSJE6itrcWaNWvy9k+dOhX79+/HypUr8fDDD6OpqQlHjhzJa3PfffehsrIS3d3dqK6uRldXV95zN336dDz66KNv9NHY2DjsMGRrayt6e3tx8eJFVFVV4d57783bP2PGDOzevRurVq3CQw89hOXLl+PUqVN5bTZv3ozu7m5MnDgRlZWV2LBhQ97+WbNmob29/Y1xLFmyBGfOnMlrk81m0dXVhaqqKvT29mLTpk15+2fPno2tW7e+0cfixYtx7lz+S2bHjh3o7OzEzJkz0d3djba2trz98+bNQ1tbG9auXYv29nbMnz8fFy5cyGvT0dGBXC6HuXPnoqurC1u2bMnbv2jRIvT396OpqQlbt27FnDlz0N+ff0Bv79696OjoQF1dHTo7O7Ft27a8/aN5PQ1YtmwZGhoakM1msXHjRixYsGBYH2P99TTw+5zU62noegzj+fV08uTJYe/7Vq+njRs3IpPJlP3rySvJqycuAbjMvXuZqvb5tP8ugGvdu9e6a0vAnYHyLwIM81dU9UCpBlFfPfEVkbwTNwri1ROR+MpXvoI77vBNm0aJOdtgznaY9XDlevVEJFT1flWVALcD1mPrtX7AFOvtZdoWmLMN5myHWYeTZNHQ49muDNB+omf71YjHEouLSQ8gRS5eZNoWmLMN5myHWYeTZNHwQ8/21FIN3Ymg3uLefR3AhRLNQxORehHZdv78+Si7RVWkvVEpVVVM2wJztsGc7TDrcJIsGr7p2b7Bp+10ABXu9imN4kQMD1Xdp6rrpkyZEmW3uNe/CUVk6ElXFA/mbIM522HW4SRZNHhPvb3Np633hAzOxEFERJSAJIuGxzzb7/Np650F0m/2SCIiIopBkkXD1wCcdbdvF5FbCjVyF6xa7t7thTPzY6TiOqeBiIhoPEmsaHDnZWhx7wqAnSJypbeNiFQC+CycyZwA4JOlVrgcxVhiOadhRqS9USkzZjBtC8zZBnO2w6zDCT25k4jcCOCeIV9+F4B6d/sQgH8Zsv/zqvpMgb4uB/DPAOa5XzoN4FMATsE5+fEeANXuvucA/IKqxnY4gEtjExFR2kU9udP1cGZD9t7qPfvnFdj/7kIduWtE3Angq+6XrgPwAJylsD+GwYLhaQB1cRYMcViV9ABSZNUqpm2BOdtgznaYdTgjOdKwAM75CGGsUdX2En0KnCUaVgH4OTjzNrwM4Fk4BcQOv2mmR0NE6gHUz5gx44MnT56MsmP/NjzSQEREZSTSIw2qeiDgtM3eW7tPn6qqn1PVX1PVt6vqFap6jaq+V1U/HWfB4D5+LOc0LPdvQhFZvpxpW2DONpizHQuS2vMAABtoSURBVGYdzphfe6KcnfJvQhEZuloexYM522DOdph1OCwaiIiIKBAWDURERBQIiwZwciciIqIgWDQgvhMhN0faG5WyeTPTtsCcbTBnO8w6HBYNMepOegAp0t3NtC0wZxvM2Q6zDodFQ4wmJj2AFJk4kWlbYM42mLMdZh0Oi4YYVSY9gBSprGTaFpizDeZsh1mHw6IhRhuSHkCKbNjAtC0wZxvM2Q6zDodFA3j1BBERURAsGhDf1RNERETjCYsGIiIiCoRFQ4xmJT2AFJk1i2lbYM42mLMdZh1O6KWxx7Oamho9evRodB1yaWwiIhpjIl0am4JbmfQAUmTlSqZtgTnbYM52mHU4PNLgwSMNRESUdjzSkJAlSQ8gRZYsYdoWmLMN5myHWYfDogHxzdNwJtLeqJQzZ5i2BeZsgznbYdbhsGgA52kgIiIKgkUDERERBcKigYiIiAJh0RCjbNIDSJFslmlbYM42mLMdZh0Oi4YYdSU9gBTp6mLaFpizDeZsh1mHw6IhRlVJDyBFqqqYtgXmbIM522HW4bBoiFFv0gNIkd5epm2BOdtgznaYdTgsGmK0KekBpMimTUzbAnO2wZztMOtwWDQgvsmdiIiIxhMWDeDkTkREREGwaCAiIqJAWDTEaHbSA0iR2bOZtgXmbIM522HW4XBpbA8ujU1ERGnHpbETsjLpAaTIypVM2wJztsGc7TDrcHikwYNHGoiIKO14pCEhi5MeQIosXsy0LTBnG8zZDrMOh0VDjM4lPYAUOXeOaVtgzjaYsx1mHQ6LBiIiIgqERQMREREFwqKBiIiIAmHREKMdSQ8gRXbsYNoWmLMN5myHWYfDoiFGnUkPIEU6O5m2BeZsgznbYdbhsGhAfKtczoy0Nypl5kymbYE522DOdph1OCwaEN8ql92R9kaldHczbQvM2QZztsOsw2HREKO2pAeQIm1tTNsCc7bBnO0w63BYNBAREVEgLBqIiIgoEBYNREREFAiLhhjNS3oAKTJvHtO2wJxtMGc7zDocLo3tEfXS2H0imODXiPlHoq+vDxMm+KZNo8ScbTBnO8x6uHG/NLaI3CYiHxKRfxSR74qIikji/xuvTXoAKbJ2LdO2wJxtMGc7zDqccXGkQUQeBXDn0K+rqoTpJ+ojDZAADz8O8iciovFj3B9pgDNj8yYA9QCuAdCf7HAc85MeQIrMn8+0LTBnG8zZDrMOZ1x8kKOqf+29L0H+wjdwIekBpMiFC0zbAnO2wZztMOtwxsuRBiIiIopZ6KJBRCpEZJaINIrIFhHpFJEfDZx8KCL3j6BPEZG7ReRLInJGRF4TkW4ReUJE1orIuDgiQkRENJaN5D/jPQDuimoAInIlgEcALByy6xr3thBAk4gsVdUXonpcIiIiCmckH09UDLn/EoCTI3lwEbkcwBcwWDCcBvARAA0AMgC63K/fCiAnIm8ZyeMkpSPpAaRIRwfTtsCcbTBnO8w6nJEUDU8BeBDABwC8Q1WvBtA6wsdvwuDEiU8DeLeqPqCqu1X1Y3CKhcfc/TfDKSjGjFzSA0iRXI5pW2DONpizHWYdTuiPJ1R1pAVCHvc8hY0D3QJYraovD3msXhFZDeDbACYBWC8iD6rqi1GMIW5zkx5Aisydy7QtMGcbzNkOsw4nyasnFgKY5m4/oarPFmqkqj8AsNu9ewUKTOJUrrr8m1BEurqYtgXmbIM522HW4SRZNNzh2d7v09a7f3EMY4nFlqQHkCJbtjBtC8zZBnO2w6zDSbJomOXZPubT1ju386yirYiIiCg2SRYN7/RsP+/T9gwGp4a+ScplykciIqIUSXLSpLd6ts+VaqiqfSLyCoAr4Yx5EoCegf0i8n7kX1lR4X79sOdr21V1+9C+RWQdgHUAcNVVV6GmJn+Njvr6ejQ3N6O1tRVtbW0FT5rJ5XLIZrNYsWIFcrkcdu7cCcA5p6EGwDI415Bm4Zz5ucD7ze7jHThwAC0tLWhubkZHRwf27NmT9xirV69GXV0ddu3ahebmZtTV1Q0bx+HDh5HJZLBhwwZks1ns27cvb/+6detQW1uLXC6HhoYGLF26NG9/RUUFnnzySTQ1NaGtrQ0tLS14/PHH89qsX78e1dXVOHz4MOrq6tDQ0JC3f9KkSTh48CAaGxuxfft2ZDIZHDp0KK9NJpNBVVUVTpw4gdraWqxZsyZv/9SpU7F//36sXLkSDz/8MJqamnDkyJG8Nvfddx8qKyvR3d2N6upqdHV15T1306dPx6OPPvpGH42NjTh+/HheH62trejt7cXFixdRVVWFe++9N2//jBkzsHv3bqxatQoPPfQQli9fjlOnTuW12bx5M7q7uzFx4kRUVlZiw4YNeftnzZqF9vb2N8axZMkSnDlzJq9NNptFV1cXqqqq0Nvbi02bNuXtnz17NrZu3fpGH4sXL8a5c/kvmR07dqCzsxMzZ85Ed3c32tra8vbPmzcPbW1tWLt2Ldrb2zF//vxh0+d2dHQgl8th7ty56OrqGnbYdtGiRejv70dTUxO2bt2KOXPmoL8/f5mXvXv3oqOjA3V1dejs7MS2bdvy9o/m9TRg2bJlaGhoQDabxcaNG7FgwYJhfYz119PA73NSr6fm5ua8/eP59XTy5Mlh7/tWr6eNGzcik8mU/espj6qO+gagEc4VEArg/oDfc8nzPRMCtP+up31ViccvdvMd12233aZR+pCzhmXpG0XiQx/6UNJDSAXmbIM522HWwwE4qkX+n4xkaWwRaQSww737UVW9P8D3XAJwmXv3MlXt82n/XQDXunevVdXukY22uKiXxu4RwWS/RlwaOxI9PT2YPNk3bRol5myDOdth1sOV69LYPZ7tygDtJ3q2X414LLHIJD2AFMlkmLYF5myDOdth1uEkeaTh2wBudO/eqKrPl2g7AUAvnHMVXgdwhUYx8MH+6wHUz5gx44MnT45oRuxiHfu34ZEGIiIqI+V6pOGbnu0bfNpOx+CaF6eiLBgAQFX3qeq6KVOmRNkt5kTaG5UyZw7TtsCcbTBnO8w6nCSLBu+pt7f5tPVWPMeLtioz/f5NKCJDzzqmeDBnG8zZDrMOJ8mi4THP9vt82npngfSbPZKIiIhikGTR8DUAZ93t20XklkKNRORtAJa7d3vhLKUdKRGpF5Ft58+fj7rr0RMpfSMiIjKSWNHgXmLZ4t4VADtF5EpvGxGpBPBZOJM5AcAnNYYVLuM6p4GIiGg8CT0jpIjcCOCeIV9+l2d7oXu1g9fnVfWZAt1tBfAbAOYBuBXAN0TkUwBOwTn58R4A1W7b5wA8EHa8Sdqb9ABSZO9epm2BOdtgznaYdTgjOdJwPZzZkL23es/+eQX2v7tQR6p6Cc5S1191v3QdnMJgN4CPYbBgeBpAnaqW4ecHxXUkPYAU6ehg2haYsw3mbIdZh5PkOQ0AAFV9GcDtcM5b+DKA78GZYvr7cIqJdQDmqOoLcY0hrnMahs9mT3EptHYARY8522DOdph1OKGLBlU9oKoS8tbu06eq6udU9ddU9e2qeoWqXqOq71XVT/tNMT1acZ3T0Blpb1RKZyfTtsCcbTBnO8w6nMSPNIxn2/ybUESGrv5G8WDONpizHWYdDosGIiIiCoRFAxEREQXCogEJT+7EyZuIiGiMYNGA+E6ErPdvQhGpr2faFpizDeZsh1mHE8nS2ONFTU2NHj16NLL+zopgWmS9FcHnDwBw9uxZTJsWe9qpx5xtMGc7zHq4cl0ae9xrTXoAKdLayrQtMGcbzNkOsw6HRxo8oj7S0CcSfp7usPj8AQD6+vowYULsaacec7bBnO0w6+F4pCEhc5MeQIrMncu0LTBnG8zZDrMOh0UDynxpbCIiojLBogFcGpuIiCgIFg1EREQUCIsGIiIiCoRFQ4xySQ8gRXI5pm2BOdtgznaYdTgsGmKUTXoAKZLNMm0LzNkGc7bDrMNh0RCjFUkPIEVWrGDaFpizDeZsh1mHw6IhRjzoZYeHGG0wZxvM2Q6zDodFA+Kbp2FnpL1RKTt3Mm0LzNkGc7bDrMNh0QDO00BERBQEiwYiIiIKhEUDERERBcKiIUbLkh5AiixbxrQtMGcbzNkOsw6HRUOMGpIeQIo0NDBtC8zZBnO2w6zDYdEQI04ZYocTtNhgzjaYsx1mHY6oatJjKBs1NTV69OjRyPrrEcHkyHorgs8fAKCnpweTJ8eeduoxZxvM2Q6zHk5EjqlqTaF9PNIQowVJDyBFFixYkPQQUoE522DOdph1OCwaEN/kTkREROMJiwZwciciIqIgWDQQERFRICwaiIiIKBBePeHBqyfGLp4BbYM522DOdpj1cLx6IiEtSQ8gRVpamLYF5myDOdth1uHwSINH1EcaTovgush6K4LPHwDg9OnTuO662NNOPeZsgznbYdbD8UhDQjqSHkCKdHQwbQvM2QZztsOsw2HREKM9SQ8gRfbsYdoWmLMN5myHWYfDooGIiIgCYdFAREREgbBoICIiokBYNMRoddIDSJHVq5m2BeZsgznbYdbhsGiIUV3SA0iRujqmbYE522DOdph1OCwaEN8ql7si7W2EREZ/i3sMEdi1qyzSHvd8cy6H37dxgL/PdsZE1mX0muHkTh5RT+50VgTTIuutCL/nL4pfqNH+jviNIYLfwbNnz2LatNjTTj3fnMvh920c4O+znTGRtcF7aP7DcXKnRPCglx0eYrTBnG0wZzvMOhwWDURERBQIiwYiIiIKhEUDERERBcKigYiIiALh1RMeUV890SeCCZH1VgSvngAA9PX1YcKE2NNOPd+cy+H3bRzg77OdMZE1r55Ih0zSA0iRTIZpW2DONpizHWYdDo80eHCehhE+hh/O0zBucJ4GG/x9tjMmsuaRhnTIJj2AFMlmmbYF5myDOdth1uGwaIjRvqQHkCL79jFtC8zZBnO2w6zDYdFAREREgbBoICIiokBYNBAREVEgvHrCQ0TOAvhOhF1OBXAuwv6oOGZtgznbYM52mPVw16tqwUtKWDTESESOFrtshaLFrG0wZxvM2Q6zDocfTxAREVEgLBqIiIgoEBYN8dqW9ABShFnbYM42mLMdZh0Cz2kgIiKiQHikgYiIiAJh0UBERESBsGiIkDjuFpEvicgZEXlNRLpF5AkRWSsiZb5oezgiMkVElonIVhF5UkReFJHXReRlEfmGiGRFZHbIPheLyOdE5Dsi0isiPxCRr4vIH4vIpJB91YrIZ0TkWyLyIxF5SUSOiciHRWRqyL5micgWETkhIj0icl5E/kNEHhSR68P0FSUReUxE1HNrDPh9zNl/LL8oIp8UkePuz3TRzev/iUiriPxSgD6Yc/ExvMcdwzMi8kMR6XP//XcR2RYkX09fkb73luvzVhZUlbcIbgCuBPAEAC1xOwbgp5Iea0Q/7/8E0Ovz8w7cHgLwEz79XQGgw6efUwDeFWBsAmAzgB+X6Ou/ASwM+LP+KYBLJfp6BcDyBJ6D/1FgLI3MedS5TgXwDwF+r/+NOY8o3zcB+Dufn2fg1gGg0qe/yN57y/l5K5db4gMYDzcAlwP4F88vwgsAPgxgufsCfc6z71kAb0l6zBH8zNs9P9O3AHwKQBOADwBYB+BzAPo8bR4D8KYS/e32tD0HoBVAA4D1AJ707PsegOt8xvagp32P+wb1WwB+B8BXPPteBfAen75+19P+EoC/B7AawD0A9njeEF4HsNgw/7cBeNHzMwYtGphz6XH8JIDjnrE852bUCOA33Z9tC5z/OEoVDcy5+Bg+4RmDAvgigHvhvF/+AZz/tL3vHXtK9BXpe2+5Pm/ldEt8AOPhBuAPPb8AxwBcOWR/JYD9njZtSY85gp/50wC+BGB+iTbz3BfEwM+9pki7Oz1tvoMhfxHA+cvkM542/1DiMX/O88b3QxT4iwDA/Z6+noJ7FVGBdlUALnjeRG8v0KZxyBtWyb+KIsz/c+5jPg3nSI5v0cCcfTMVAAfdx+gD8PsoXegW/E+DOZfM+AYA/Z6M7yjS7lbkv3cU/E8VEb73luvzVm63xAcw1m8AJgD4gfvE/xjALUXavQ2DfxH2Arg66bGP8ue+MmC73/e8MA4WafOMp82vFmkz0X0hD7SbVaTdXk+b5iJtBPl/Nby/SLuPe9r8TYmfcY+n3e8ZZP/r7mP1A6gB0O55/MYS38ecS+fq/Sv8j0bRD3Mu3vdaT99FjyC4bT/mabu+wP5I33vL9Xkrt1viAxjrNwB3eJ70x33aeg/p/3bSYzfK5yc9P/NLBfbf5Nn/TZ++Puxpu6nA/jdj8DyL8yhxHgWAlZ6+dhbYLwDOeN6Qih6KBPBLnr7+JeY83+IZ19+5X2v3PH5jke9jzqV/ZgFw0u37FEocYfDphzmX/pk3ePpu8Wm7ztP2QwX2R/beW67PWzneePXE6N3h2d7v09a7f3EMYylHr3q2JxbY/z7P9mM+ffnlNx/OiUyA82b3oxJ9eR+rUF+3AHi7u/2sqp4u0de/wjl5DAB+UUTeXKLtaP2NO64zcN68gmLOpc0DMMPd3qWqPx5hP8y5tO97tm/yaevd31Vgf5TvveX6vJUdFg2jN8uzfcyn7dEi3zeeeX/OQsuOh8nv3+AckgeAm0VERtqXqnqXQZ8mIm8bRV8/hnNoE3BeU9Wl2o+UiPwynL++AOD3VfXVUu2HYM6l/bJn+ykReZOIrBGRgyJyzr3s7jsi0iEidxTthTn7ycE5ARMA7hKRRYUaicitcE4YBJwjQP9UoFmU773l+ryVHRYNo/dOz/bzPm3PYPCX7aYCv2zj0TrP9pcL7A+cn6r2Afiue3cSBv9yCt2Xy1vEvHPIvij7GjURqYRz8qkA2KuqXwjZBXMuzbs0cg+cEyI/A6eYuBrOX44/Bees/MdE5B9E5CcK9MOcS1DV7wH4M/duBYCviMgX3fkP7haR9SLSAefEwDfDufrh/ar6eoHuonzvLdfnreyMq8mGEvJWz/a5Ug1VtU9EXoFzXfEEOL9wPTGOLVEi8gsA1rh3e+GciDVU4PxcL8J58x743jOj7KvQ90bdVxT+As4byqtwLv8KizmXdo1n+1Nwsv4hnM/CnwFwGZwCYpW7/ZtwLve7c0g/zNmHqn5CRP4bwF/D+dnr3ZvXWQAbAfzfEof3o3zvLdfnrezwSMPoTfZs9wZof9GzHedn34kSkWvgnIU98Dv2EVU9U6BplPmVa1+jIiLvgXPNOQBsVNXvlmpfRLlmUy45e9+s3wnnZMifVdWMqu5S1c+q6j1wThIc+Lz/10Xk7iH9lGs25ZLzgM/DmZuh2O/yNDgTyA3N16tc8ym3rCPFooEi506z+gUMHrb7MoC/TW5EY5eIVMCZgGcCgCMA/neyIxq3hr4XNhYqclX1KTh/AQ/4w1hHNQ6JyE/DOS/gETjnN6yGM4fE5e6/qwH8F5wTUz8jIn+V0FCpABYNo+c9xFUZoL33CoIwJ7KNCe5n718E8PPul74O4G51ry8qIMr8yrWv0fgTOBPd9AH44CjO6i/XbMolZ29fz6nq10u03QFnciQA+HkR8f5lWa7ZlEXOInItgMMAboZzNKdGVR9S1f9W1dfdfx+Cc47Jt9xv+5CIvL9Ad+WaT1lkHRcWDaP3Q892ycVH3EVT3uLefR3O7GzjhohcDuAfASx0v/QUnElSSv2cgfNzXV3ke8u5rxERkRlwZo0DgI+r6jdG0V25ZpN4zgX68jvj/QKA/3TvVsCZ5bBQP+WUTbnk/GHP439YVV8q1Mj9uveS4kLn8UT53jses44FT4QcvW8CuNHdvgGlz5adDudNBgBOlfjre8wRkcvgLPJT537pGThz179S/LsAOPn9irt9g89jTMDgRx4XMPzz0G96tkv25bq+yPdG3ddI/Racv0IUQJ+IFJuX4V2e7XoRme5uf8U9nD4wJuZc3H9isNg9H6C9t80UzzZzLs17xOCffdp69/98gf1RvveW6/NWdlg0jN5xDE4MchuAAyXaei/rOh7XgKy5L6IOONMbA8B/AFikqi8H+HZvDrfBmd2wmPdg8IX/XIEX/tC+ihKRaRh8sZ5V1R+Moq83wZlrHnBm2ys0Ec1IiOffPw/4PXe5N8A5TDpQNDDn0v7dsz2laKvCbbwFBHMu7VrPtt8fFN5cCy1HHeV7b7k+b2WHH0+MnndGr/cVbeXwzvjlN4PZmOCeqPcwgN9wv/QcnIVwXiz+XXmizO8AgNfc7V8WkUIzUBZ6rEJ9PYvBy6hu8fz1XsgvYPDQ59dDTrpkhTmXlvNs+73RTwLwM+7d1+GctDeAOZfmLRSu82nr/Qu80PtJlFmX6/NWfpKex3qs3zCyRVMuYowvWOX+TG8C8FkMzp1+AsA1I+jnaU8fdUXaVCLYQjH/6GnTVKSNwDkZa6BdsQV+Nnva/HWJ8ZsuWFXg8ds9j9/InEec4796+v/FEu28i7B9lTmHyvigp+8NPm29azx8scD+SN97y/V5K7db4gMYDzcEW54152kzHpbGFjgzFA78TCcBXDvCvrxL0j6PwkvS/r2nzWiXpP0LT19PlejrWuQvJfzeAm0aPX2ZLY09ZAztnjE0MucR57hwyO/z2wu0mQ3nsHnR/1yYc8mMvSuJXiw0Brfde939A20/UKRdZO+95fq8ldst8QGMhxuc64v/ZciLbSOciUn+BM4h+4F9zwKYkvSYI/iZWz0/0yU4f30tCXAruOIbgN2e/s4BaIEzZe/vIX/52O+hxAp9bl8Petr3APgEgBVwprR+zLPvVQDv8enL+yZ3Cc4MgavgzHS5x/PG8DqcEz+TeC7aPWNs9GnLnEuPI+sZx8twFghrgDN3wHZ3bAP7tzHn0PleBuc8m4Ex9MOZ6Ol34Myy+Tvu/X5PmxwAKdJfpO+95fq8ldMt8QGMlxuc6Umf8PwiFLodw5Dqdaze4HxuV+pnLXa7oUh/V8A5mbLU955CgYq9QF8CZ8rqH5fo6/sAFgb8Wf8U+f9ZDL29AmB5gs9Fu2csjT5tmXPpMbwJwBafn0kB/C8AFcx5RBlfDefz+yDvF3sATPbpL7L33nJ+3srllvgAxtPN/SW5G8CX4FyG8xqA/3Z/oT8IYELSY4zwZz0Q8EUfqGjw9LvYfaN4Ac4UrGfhfNb8xwAmhRxjLZyJeL4F51Dny+6bx0cATA3Z1ywAn4RzaV6P+8b6H3D+mrg+4eei3ZNvY8DvYc6lxzEXzsdvJ+Ec0r8A53K4TwO4NUQ/zLn4GG6HsyjYs3A+8ulz//0PANtQ4rySAn1F+t5brs9bOdzE/aGIiIiISuIll0RERBQIiwYiIiIKhEUDERERBcKigYiIiAJh0UBERESBsGggIiKiQFg0EBERUSAsGoiIiCgQFg1EREQUCIsGIiIiCoRFAxEREQXCooGIiIgC+f8wTNnSZvcmVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_dist(vocab_match_frequency_iv, task_token_frequency_map, task_token_frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S2: Frequency-matched out-of-vocab swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10136/10136 [00:07<00:00, 1285.94it/s]\n",
      "100%|██████████| 2389/2389 [00:00<00:00, 6180.66it/s] \n",
      "100%|██████████| 4430/4430 [00:01<00:00, 3437.49it/s] \n",
      "100%|██████████| 743/743 [00:00<00:00, 14791.18it/s]\n",
      "100%|██████████| 111/111 [00:00<00:00, 28093.64it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 29013.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_vocab_match_frequency_oov(token_by_length, token_frequency_map, wiki_token_frequency_discount):\n",
    "    vocab_match = {}\n",
    "    for _, tokens in token_by_length.items():\n",
    "        tokens_copy = copy.deepcopy(tokens)\n",
    "        \n",
    "        # token_frequency_map, token_lemma_map)\n",
    "        \n",
    "        token_freq_tu = []\n",
    "        for t in tokens:\n",
    "            token_freq_tu.append((t, token_frequency_map[t]))\n",
    "        token_freq_tu = sorted(token_freq_tu, key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        matched_to = set([])\n",
    "        for i in trange(0, len(token_freq_tu)):\n",
    "            found = False\n",
    "            for j in range(0, len(wiki_token_frequency_discount)):\n",
    "                word_i = token_freq_tu[i][0]\n",
    "                word_j = wiki_token_frequency_discount[j][0]\n",
    "                if i != j and word_j not in matched_to and \\\n",
    "                    levenshteinDistance(word_i, word_j) > 0.3:\n",
    "                    matched_to.add(word_j)\n",
    "                    vocab_match[word_i] = word_j\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                vocab_match[word_i] = word_i\n",
    "            \n",
    "    return vocab_match\n",
    "\n",
    "discount = int(len(wiki_token_frequency_map)/len(task_token_frequency_map))\n",
    "wiki_token_frequency_discount = []\n",
    "wiki_token_frequency_discount_map = {}\n",
    "for k, v in wiki_token_frequency_map.items():\n",
    "    if k in task_token_frequency_map.keys():\n",
    "        continue\n",
    "    discounted = int(v/discount)\n",
    "    if discounted > 0:\n",
    "        wiki_token_frequency_discount.append((k, discounted))\n",
    "        wiki_token_frequency_discount_map[k] = discounted\n",
    "vocab_match_frequency_oov = \\\n",
    "    generate_vocab_match_frequency_oov(task_token_by_length, \n",
    "                                       task_token_frequency_map,\n",
    "                                       wiki_token_frequency_discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5525df92e64369855254a9fc4ba2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159274.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9207ed1fa40c4e46a1b999ed7c6556d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c941199de64dd9b9b8503d50422a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrupted_train_dataset = train_df.map(partial(random_corrupt, \n",
    "                                               modified_basic_tokenizer, \n",
    "                                               vocab_match_frequency_oov))\n",
    "corrupted_validation_dataset = eval_df.map(partial(random_corrupt, \n",
    "                                                   modified_basic_tokenizer, \n",
    "                                                   vocab_match_frequency_oov))\n",
    "corrupted_test_dataset = test_df.map(partial(random_corrupt, \n",
    "                                             modified_basic_tokenizer, \n",
    "                                             vocab_match_frequency_oov))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-S2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S3: Low-band Frequency-matched out-of-vocab swap\n",
    "all token maps to very lower frequency token in English texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_match_low_band_oov = generate_vocab_match_low_band_oov(task_token_frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_dist(vocab_match_low_band_oov, task_token_frequency_map, wiki_token_frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e4d7d3d67943aeb6e591833d3339b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159274.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e57b2165484260af67ed184c053fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7b17158acc480fb040df227ab51ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrupted_train_dataset = train_df.map(partial(random_corrupt, \n",
    "                                               modified_basic_tokenizer, \n",
    "                                               vocab_match_low_band_oov))\n",
    "corrupted_validation_dataset = eval_df.map(partial(random_corrupt, \n",
    "                                                   modified_basic_tokenizer, \n",
    "                                                   vocab_match_low_band_oov))\n",
    "corrupted_test_dataset = test_df.map(partial(random_corrupt, \n",
    "                                             modified_basic_tokenizer, \n",
    "                                             vocab_match_low_band_oov))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S4: Micro-frequency mismatch out-of-vocab swap\n",
    "all token maps to very lower frequency token in English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_match_high_band_oov = \\\n",
    "    generate_vocab_match_high_band_oov(task_token_frequency_map, \n",
    "                                       wiki_token_frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ad86c5adc448149c61e6bb97bb155c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159274.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4fba00c8584515944631941798b170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a11e9598db94c4ea2f18865c2b3182c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corrupted_train_dataset = train_df.map(partial(random_corrupt, \n",
    "                                               modified_basic_tokenizer, \n",
    "                                               vocab_match_high_band_oov))\n",
    "corrupted_validation_dataset = eval_df.map(partial(random_corrupt, \n",
    "                                                   modified_basic_tokenizer, \n",
    "                                                   vocab_match_high_band_oov))\n",
    "corrupted_test_dataset = test_df.map(partial(random_corrupt, \n",
    "                                             modified_basic_tokenizer, \n",
    "                                             vocab_match_high_band_oov))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-S4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S5: High-band frequency vocab swap\n",
    "TODO: currently, don't have a good way of doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S6: High-band frequency vocab swap\n",
    "TODO: currently, don't have a good way of doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S7: Dangerous Zone, let me swap everything randomly\n",
    "random everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9d0f7728304b67b4d425a9c66beec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159274.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dbb32f7d0a47d2b77f214a147cbbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f69ce4349a444ecbfc82d8ae03a9a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def choas_corrupt(tokens_pool, tokenizer, example):\n",
    "    original_sentence = example['text']\n",
    "    tokens = tokenizer.tokenize(original_sentence)\n",
    "    corrupted_sentence = random.choices(tokens_pool, k=len(tokens))\n",
    "    out_string = \" \".join(corrupted_sentence).replace(\" ##\", \"\").strip()\n",
    "    example['text'] = out_string\n",
    "    return example\n",
    "    \n",
    "tokens_pool = list(wiki_token_frequency_map.keys())\n",
    "random.shuffle(tokens_pool)\n",
    "tokens_pool = tokens_pool[:len(task_token_frequency_map)]\n",
    "corrupted_train_dataset = train_df.map(partial(choas_corrupt, \n",
    "                                               tokens_pool, \n",
    "                                               modified_basic_tokenizer))\n",
    "corrupted_validation_dataset = eval_df.map(partial(choas_corrupt, \n",
    "                                                   tokens_pool,\n",
    "                                                   modified_basic_tokenizer))\n",
    "corrupted_test_dataset = test_df.map(partial(choas_corrupt, \n",
    "                                             tokens_pool, \n",
    "                                             modified_basic_tokenizer))\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(f\"../data-files/{FILENAME_CONFIG[task_name]}-corrupted-S7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let us corrupt all datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupted SST-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_corrupt(example):\n",
    "    original_sentence = example['text']\n",
    "    corrupted_sentence = corrupt_translator(original_sentence, modified_basic_tokenizer, vocab_match_by_piece_length)\n",
    "    example['text'] = corrupted_sentence\n",
    "    return example\n",
    "\n",
    "corrupted_train_dataset = train_df.map(random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(random_corrupt)\n",
    "corrupted_test_dataset = test_df.map(random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/sst-tenary-corrupted-length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv(corrupted_train_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-length\", \"sst-tenary-train.tsv\"))\n",
    "write_tsv(corrupted_validation_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-length\", \"sst-tenary-dev.tsv\"))\n",
    "write_tsv(corrupted_test_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-length\", \"sst-tenary-test.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_corrupt_new(example):\n",
    "    original_sentence = example['text']\n",
    "    corrupted_sentence = corrupt_translator(original_sentence, modified_basic_tokenizer, vocab_match_by_piece_length_frequency_lemma)\n",
    "    example['text'] = corrupted_sentence\n",
    "    return example\n",
    "\n",
    "corrupted_train_dataset = train_df.map(random_corrupt_new)\n",
    "corrupted_validation_dataset = eval_df.map(random_corrupt_new)\n",
    "corrupted_test_dataset = test_df.map(random_corrupt_new)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/sst-tenary-corrupted-freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv(corrupted_train_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-freq\", \"sst-tenary-train.tsv\"))\n",
    "write_tsv(corrupted_validation_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-freq\", \"sst-tenary-dev.tsv\"))\n",
    "write_tsv(corrupted_test_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-freq\", \"sst-tenary-test.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_corrupt_plain(example):\n",
    "    original_sentence = example['text']\n",
    "    corrupted_sentence = corrupt_translator(original_sentence, modified_basic_tokenizer, vocab_match_plain)\n",
    "    example['text'] = corrupted_sentence\n",
    "    return example\n",
    "\n",
    "corrupted_train_dataset = train_df.map(random_corrupt_plain)\n",
    "corrupted_validation_dataset = eval_df.map(random_corrupt_plain)\n",
    "corrupted_test_dataset = test_df.map(random_corrupt_plain)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/sst-tenary-corrupted-plain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv(corrupted_train_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-plain\", \"sst-tenary-train.tsv\"))\n",
    "write_tsv(corrupted_validation_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-plain\", \"sst-tenary-dev.tsv\"))\n",
    "write_tsv(corrupted_test_dataset, output_filename=os.path.join(external_output_dirname, \"sst-tenary-corrupted-plain\", \"sst-tenary-test.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupted MRPC (TODO: fix bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_corrupt(example):\n",
    "    original_sentence1 = example['sentence1']\n",
    "    corrupted_sentence1 = corrupt_translator(original_sentence1, modified_basic_tokenizer, vocab_match_by_piece_length)\n",
    "    example['sentence1'] = corrupted_sentence1\n",
    "    \n",
    "    original_sentence2 = example['sentence2']\n",
    "    corrupted_sentence2 = corrupt_translator(original_sentence2, modified_basic_tokenizer, vocab_match_by_piece_length)\n",
    "    example['sentence2'] = corrupted_sentence2\n",
    "    return example\n",
    "\n",
    "corrupted_train_dataset = train_df.map(random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(random_corrupt)\n",
    "corrupted_test_dataset = test_df.map(random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/mrpc-corrupted-length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_corrupt_new(example):\n",
    "    original_sentence1 = example['sentence1']\n",
    "    corrupted_sentence1 = corrupt_translator(original_sentence1, modified_basic_tokenizer, vocab_match_by_piece_length_frequency_lemma)\n",
    "    example['sentence1'] = corrupted_sentence1\n",
    "    \n",
    "    original_sentence2 = example['sentence2']\n",
    "    corrupted_sentence2 = corrupt_translator(original_sentence2, modified_basic_tokenizer, vocab_match_by_piece_length_frequency_lemma)\n",
    "    example['sentence2'] = corrupted_sentence2\n",
    "    return example\n",
    "\n",
    "corrupted_train_dataset = train_df.map(random_corrupt_new)\n",
    "corrupted_validation_dataset = eval_df.map(random_corrupt_new)\n",
    "corrupted_test_dataset = test_df.map(random_corrupt_new)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/mrpc-corrupted-freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupted CoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_corrupt(example):\n",
    "    original_sentence = example['sentence']\n",
    "    corrupted_sentence = corrupt_translator(original_sentence, modified_basic_tokenizer, vocab_match_by_piece_length)\n",
    "    example['sentence'] = corrupted_sentence\n",
    "    return example\n",
    "\n",
    "corrupted_train_dataset = train_df.map(random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(random_corrupt)\n",
    "corrupted_test_dataset = test_df.map(random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/cola-corrupted-length\")\n",
    "\n",
    "write_tsv(corrupted_train_dataset, \n",
    "          output_filename=os.path.join(external_output_dirname, \"cola-corrupted-length\", \"cola-train.tsv\"), \n",
    "          fieldnames=['sentence', 'label'])\n",
    "write_tsv(corrupted_validation_dataset,\n",
    "          output_filename=os.path.join(external_output_dirname, \"cola-corrupted-length\", \"cola-dev.tsv\"),\n",
    "          fieldnames=['sentence', 'label'])\n",
    "write_tsv(corrupted_test_dataset, \n",
    "          output_filename=os.path.join(external_output_dirname, \"cola-corrupted-length\", \"cola-test.tsv\"), \n",
    "          fieldnames=['sentence', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_corrupt_new(example):\n",
    "    original_sentence = example['sentence']\n",
    "    corrupted_sentence = corrupt_translator(original_sentence, modified_basic_tokenizer, vocab_match_by_piece_length_frequency_lemma)\n",
    "    example['sentence'] = corrupted_sentence\n",
    "    return example\n",
    "\n",
    "corrupted_train_dataset = train_df.map(random_corrupt_new)\n",
    "corrupted_validation_dataset = eval_df.map(random_corrupt_new)\n",
    "corrupted_test_dataset = test_df.map(random_corrupt_new)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/cola-corrupted-freq\")\n",
    "\n",
    "write_tsv(corrupted_train_dataset, \n",
    "          output_filename=os.path.join(external_output_dirname, \"cola-corrupted-freq\", \"cola-train.tsv\"), \n",
    "          fieldnames=['sentence', 'label'])\n",
    "write_tsv(corrupted_validation_dataset,\n",
    "          output_filename=os.path.join(external_output_dirname, \"cola-corrupted-freq\", \"cola-dev.tsv\"),\n",
    "          fieldnames=['sentence', 'label'])\n",
    "write_tsv(corrupted_test_dataset, \n",
    "          output_filename=os.path.join(external_output_dirname, \"cola-corrupted-freq\", \"cola-test.tsv\"), \n",
    "          fieldnames=['sentence', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupted MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_map = {'entailment': 0, 'neutral' : 1, 'contradiction' : 2}\n",
    "def process_mnli(dataset, split):\n",
    "    data_all = []\n",
    "    for example in dataset[split]:\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        label = example['label']\n",
    "        if label in [0,1,2]:\n",
    "            data = {\"premise\" : premise, \n",
    "                    \"hypothesis\" : hypothesis, \n",
    "                    \"label\" : label}\n",
    "        data_all.append(data)\n",
    "    return data_all\n",
    "\n",
    "def mnli_write_tsv(*datasets, output_filename):\n",
    "    all_data = []\n",
    "    for dataset in datasets:\n",
    "        all_data += dataset\n",
    "    random.shuffle(all_data)\n",
    "    with open(output_filename, \"wt\") as f:\n",
    "        writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=['premise', 'hypothesis', 'label'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "        \n",
    "def mnli_random_corrupt(example):\n",
    "    original_premise = example['premise']\n",
    "    original_hypothesis = example['hypothesis']\n",
    "    if original_hypothesis == None:\n",
    "        original_hypothesis = \"\"\n",
    "    try:\n",
    "        original_premise_tokens = original_tokenizer.tokenize(original_premise)\n",
    "        original_hypothesis_tokens = original_tokenizer.tokenize(original_hypothesis)\n",
    "    except:\n",
    "        print(\"Please debug these sequence...\")\n",
    "        print(original_premise)\n",
    "        print(original_hypothesis)\n",
    "    corrupted_premise_tokens = []\n",
    "    corrupted_hypothesis_tokens = []\n",
    "    for ori_t in original_premise_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_premise_tokens.append(cor_t)\n",
    "    for ori_t in original_hypothesis_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_hypothesis_tokens.append(cor_t)\n",
    "\n",
    "    example['premise'] = original_tokenizer.convert_tokens_to_string(corrupted_premise_tokens)\n",
    "    example['hypothesis'] = original_tokenizer.convert_tokens_to_string(corrupted_hypothesis_tokens)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', 'mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train = process_mnli(dataset, \"train\")\n",
    "mnli_validation_matched = process_mnli(dataset, \"validation_matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_write_tsv(mnli_train, output_filename=os.path.join(external_output_dirname, \"mnli\", \"mnli-train.tsv\"))\n",
    "mnli_write_tsv(mnli_validation_matched, output_filename=os.path.join(external_output_dirname, \"mnli\", \"mnli-dev.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, \"mnli\", \"mnli-train.tsv\"), delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, \"mnli\", \"mnli-dev.tsv\"), delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "\n",
    "corrupted_train_dataset = train_df.map(mnli_random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(mnli_random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/mnli-corrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_validation_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupted CoLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cola(dataset, split):\n",
    "    data_all = []\n",
    "    for example in dataset[split]:\n",
    "        sentence = example['sentence']\n",
    "        label = example['label']\n",
    "        data = {\"sentence\" : sentence,\n",
    "                \"label\" : label}\n",
    "        data_all.append(data)\n",
    "    return data_all\n",
    "\n",
    "def cola_write_tsv(*datasets, output_filename):\n",
    "    all_data = []\n",
    "    for dataset in datasets:\n",
    "        all_data += dataset\n",
    "    random.shuffle(all_data)\n",
    "    with open(output_filename, \"wt\") as f:\n",
    "        writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=['sentence', 'label'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "        \n",
    "def cola_random_corrupt(example):\n",
    "    original_sentence = example['sentence']\n",
    "    original_tokens = original_tokenizer.tokenize(original_sentence)\n",
    "    corrupted_tokens = []\n",
    "    for ori_t in original_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_tokens.append(cor_t)\n",
    "    example['sentence'] = original_tokenizer.convert_tokens_to_string(corrupted_tokens)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_train = process_cola(dataset, \"train\")\n",
    "cola_validation = process_cola(dataset, \"validation\")\n",
    "cola_test = process_cola(dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cola_write_tsv(cola_train, output_filename=os.path.join(external_output_dirname, \"cola\", \"cola-train.tsv\"))\n",
    "cola_write_tsv(cola_validation, output_filename=os.path.join(external_output_dirname, \"cola\", \"cola-dev.tsv\"))\n",
    "cola_write_tsv(cola_test, output_filename=os.path.join(external_output_dirname, \"cola\", \"cola-test.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, \"cola\", \"cola-train.tsv\"), delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, \"cola\", \"cola-dev.tsv\"), delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, \"cola\", \"cola-test.tsv\"), delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "corrupted_train_dataset = train_df.map(cola_random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(cola_random_corrupt)\n",
    "corrupted_test_dataset = test_df.map(cola_random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset, \n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/cola-corrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupted SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_map = {'entailment': 0, 'neutral' : 1, 'contradiction' : 2}\n",
    "def process_snli(dataset, split):\n",
    "    data_all = []\n",
    "    for example in dataset[split]:\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        label = example['label']\n",
    "        if label in [0,1,2]:\n",
    "            data = {\"premise\" : premise, \n",
    "                    \"hypothesis\" : hypothesis, \n",
    "                    \"label\" : label}\n",
    "            data_all.append(data)\n",
    "    return data_all\n",
    "\n",
    "def snli_write_tsv(*datasets, output_filename):\n",
    "    all_data = []\n",
    "    for dataset in datasets:\n",
    "        all_data += dataset\n",
    "    random.shuffle(all_data)\n",
    "    with open(output_filename, \"wt\") as f:\n",
    "        writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=['premise', 'hypothesis', 'label'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "        \n",
    "def snli_random_corrupt(example):\n",
    "    original_premise = example['premise']\n",
    "    original_hypothesis = example['hypothesis']\n",
    "    if original_hypothesis == None:\n",
    "        original_hypothesis = \"\"\n",
    "    try:\n",
    "        original_premise_tokens = original_tokenizer.tokenize(original_premise)\n",
    "        original_hypothesis_tokens = original_tokenizer.tokenize(original_hypothesis)\n",
    "    except:\n",
    "        print(\"Please debug these sequence...\")\n",
    "        print(original_premise)\n",
    "        print(original_hypothesis)\n",
    "    corrupted_premise_tokens = []\n",
    "    corrupted_hypothesis_tokens = []\n",
    "    for ori_t in original_premise_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_premise_tokens.append(cor_t)\n",
    "    for ori_t in original_hypothesis_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_hypothesis_tokens.append(cor_t)\n",
    "\n",
    "    example['premise'] = original_tokenizer.convert_tokens_to_string(corrupted_premise_tokens)\n",
    "    example['hypothesis'] = original_tokenizer.convert_tokens_to_string(corrupted_hypothesis_tokens)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('snli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = process_snli(dataset, \"train\")\n",
    "snli_validation = process_snli(dataset, \"validation\")\n",
    "snli_test = process_snli(dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_write_tsv(snli_train, output_filename=os.path.join(external_output_dirname, \"snli\", \"snli-train.tsv\"))\n",
    "snli_write_tsv(snli_validation, output_filename=os.path.join(external_output_dirname, \"snli\", \"snli-dev.tsv\"))\n",
    "snli_write_tsv(snli_test, output_filename=os.path.join(external_output_dirname, \"snli\", \"snli-test.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, \"snli\", \"snli-train.tsv\"), delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, \"snli\", \"snli-dev.tsv\"), delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, \"snli\", \"snli-test.tsv\"), delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "corrupted_train_dataset = train_df.map(snli_random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(snli_random_corrupt)\n",
    "corrupted_test_dataset = test_df.map(snli_random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset,\n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/snli-corrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mrpc(dataset, split):\n",
    "    data_all = []\n",
    "    for example in dataset[split]:\n",
    "        premise = example['sentence1']\n",
    "        hypothesis = example['sentence2']\n",
    "        label = example['label']\n",
    "        data = {\"sentence1\" : premise, \n",
    "                \"sentence2\" : hypothesis, \n",
    "                \"label\" : label}\n",
    "        if label in [0,1]:\n",
    "            data_all.append(data)\n",
    "        else:\n",
    "            # print(\"Some data to look into...\")\n",
    "            # print(data)\n",
    "            continue\n",
    "    return data_all\n",
    "\n",
    "def mrpc_write_tsv(*datasets, output_filename):\n",
    "    all_data = []\n",
    "    for dataset in datasets:\n",
    "        all_data += dataset\n",
    "    random.shuffle(all_data)\n",
    "    with open(output_filename, \"wt\") as f:\n",
    "        writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=['sentence1', 'sentence2', 'label'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "        \n",
    "def mrpc_random_corrupt(example):\n",
    "    original_premise = example['sentence1']\n",
    "    original_hypothesis = example['sentence2']\n",
    "    if original_hypothesis == None:\n",
    "        original_hypothesis = \"\"\n",
    "    try:\n",
    "        original_premise_tokens = original_tokenizer.tokenize(original_premise)\n",
    "        original_hypothesis_tokens = original_tokenizer.tokenize(original_hypothesis)\n",
    "    except:\n",
    "        print(\"Please debug these sequence...\")\n",
    "        print(original_premise)\n",
    "        print(original_hypothesis)\n",
    "    corrupted_premise_tokens = []\n",
    "    corrupted_hypothesis_tokens = []\n",
    "    for ori_t in original_premise_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_premise_tokens.append(cor_t)\n",
    "    for ori_t in original_hypothesis_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_hypothesis_tokens.append(cor_t)\n",
    "\n",
    "    example['sentence1'] = original_tokenizer.convert_tokens_to_string(corrupted_premise_tokens)\n",
    "    example['sentence2'] = original_tokenizer.convert_tokens_to_string(corrupted_hypothesis_tokens)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpc_dataset = load_dataset('glue', 'mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpc_train = process_mrpc(mrpc_dataset, \"train\")\n",
    "mrpc_validation = process_mrpc(mrpc_dataset, \"validation\")\n",
    "mrpc_test = process_mrpc(mrpc_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrpc_write_tsv(mrpc_train, output_filename=os.path.join(external_output_dirname, \"mrpc\", \"mrpc-train.tsv\"))\n",
    "mrpc_write_tsv(mrpc_validation, output_filename=os.path.join(external_output_dirname, \"mrpc\", \"mrpc-dev.tsv\"))\n",
    "mrpc_write_tsv(mrpc_test, output_filename=os.path.join(external_output_dirname, \"mrpc\", \"mrpc-test.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, \"mrpc\", \"mrpc-train.tsv\"), delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, \"mrpc\", \"mrpc-dev.tsv\"), delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, \"mrpc\", \"mrpc-test.tsv\"), delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "corrupted_train_dataset = train_df.map(mrpc_random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(mrpc_random_corrupt)\n",
    "corrupted_test_dataset = test_df.map(mrpc_random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset,\n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/mrpc-corrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_qnli(dataset, split):\n",
    "    data_all = []\n",
    "    for example in dataset[split]:\n",
    "        premise = example['question']\n",
    "        hypothesis = example['sentence']\n",
    "        label = example['label']\n",
    "        data = {\"question\" : premise, \n",
    "                \"sentence\" : hypothesis, \n",
    "                \"label\" : label}\n",
    "        if label in [0,1]:\n",
    "            data_all.append(data)\n",
    "        else:\n",
    "            # print(\"Some data to look into...\")\n",
    "            # print(data)\n",
    "            continue\n",
    "    return data_all\n",
    "\n",
    "def qnli_write_tsv(*datasets, output_filename):\n",
    "    all_data = []\n",
    "    for dataset in datasets:\n",
    "        all_data += dataset\n",
    "    random.shuffle(all_data)\n",
    "    with open(output_filename, \"wt\") as f:\n",
    "        writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=['question', 'sentence', 'label'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_data)\n",
    "        \n",
    "def qnli_random_corrupt(example):\n",
    "    original_premise = example['question']\n",
    "    original_hypothesis = example['sentence']\n",
    "    if original_hypothesis == None:\n",
    "        original_hypothesis = \"\"\n",
    "    try:\n",
    "        original_premise_tokens = original_tokenizer.tokenize(original_premise)\n",
    "        original_hypothesis_tokens = original_tokenizer.tokenize(original_hypothesis)\n",
    "    except:\n",
    "        print(\"Please debug these sequence...\")\n",
    "        print(original_premise)\n",
    "        print(original_hypothesis)\n",
    "    corrupted_premise_tokens = []\n",
    "    corrupted_hypothesis_tokens = []\n",
    "    for ori_t in original_premise_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_premise_tokens.append(cor_t)\n",
    "    for ori_t in original_hypothesis_tokens:\n",
    "        if ori_t in token_mapping.keys():\n",
    "            cor_t = token_mapping[ori_t]\n",
    "        else:\n",
    "            cor_t = ori_t\n",
    "        corrupted_hypothesis_tokens.append(cor_t)\n",
    "\n",
    "    example['question'] = original_tokenizer.convert_tokens_to_string(corrupted_premise_tokens)\n",
    "    example['sentence'] = original_tokenizer.convert_tokens_to_string(corrupted_hypothesis_tokens)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_dataset = load_dataset('glue', 'qnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_train = process_qnli(qnli_dataset, \"train\")\n",
    "qnli_validation = process_qnli(qnli_dataset, \"validation\")\n",
    "qnli_test = process_qnli(qnli_dataset, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnli_write_tsv(qnli_train, output_filename=os.path.join(external_output_dirname, \"qnli\", \"qnli-train.tsv\"))\n",
    "qnli_write_tsv(qnli_validation, output_filename=os.path.join(external_output_dirname, \"qnli\", \"qnli-dev.tsv\"))\n",
    "qnli_write_tsv(qnli_test, output_filename=os.path.join(external_output_dirname, \"qnli\", \"qnli-test.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, \"qnli\", \"qnli-train.tsv\"), delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, \"qnli\", \"qnli-dev.tsv\"), delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, \"qnli\", \"qnli-test.tsv\"), delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)\n",
    "\n",
    "corrupted_train_dataset = train_df.map(qnli_random_corrupt)\n",
    "corrupted_validation_dataset = eval_df.map(qnli_random_corrupt)\n",
    "corrupted_test_dataset = test_df.map(qnli_random_corrupt)\n",
    "\n",
    "corrupted_datasets = DatasetDict({\"train\":corrupted_train_dataset, \n",
    "                                  \"validation\":corrupted_validation_dataset,\n",
    "                                  \"test\":corrupted_test_dataset})\n",
    "corrupted_datasets.save_to_disk(\"../data-files/qnli-corrupted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
