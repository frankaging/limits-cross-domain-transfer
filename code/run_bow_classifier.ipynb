{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab_mismatch_utils import *\n",
    "from data_formatter_utils import *\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Load modules, mainly huggingface basic model handlers.\n",
    "# Make sure you install huggingface and other packages properly.\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"../huggingface_cache/\" # Not overload common dir \n",
    "                                                           # if run in shared resources.\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import is_main_process, EvaluationStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(inoculation_data_path, eval_data_path=None, test_data_path=None,\n",
    "                inoculation_step_sample_size=1.0, \n",
    "                eval_sample_limit=-1, seed=42):\n",
    "    \"\"\"\n",
    "    eval_data_path is not needed if it is a saved_to_disk \n",
    "    huggingface dataset.\n",
    "    \n",
    "    return type is already a huggingface dataset.\n",
    "    \"\"\"\n",
    "    pd_format = True\n",
    "    if inoculation_data_path.split(\".\")[-1] != \"tsv\":\n",
    "        if len(inoculation_data_path.split(\".\")) > 1:\n",
    "            logger.info(f\"***** Loading pre-loaded datasets from the disk directly! *****\")\n",
    "            pd_format = False\n",
    "            datasets = DatasetDict.load_from_disk(inoculation_data_path)\n",
    "            inoculation_step_sample_size = int(len(datasets[\"train\"]) * inoculation_step_sample_size)\n",
    "            logger.info(f\"***** Inoculation Sample Count: %s *****\"%(inoculation_step_sample_size))\n",
    "            # this may not always start for zero inoculation\n",
    "            datasets[\"train\"] = datasets[\"train\"].shuffle(seed=seed)\n",
    "            inoculation_train_df = datasets[\"train\"].select(range(inoculation_step_sample_size))\n",
    "            eval_df = datasets[\"validation\"]\n",
    "            datasets[\"validation\"] = datasets[\"validation\"].shuffle(seed=seed)\n",
    "            if eval_sample_limit != -1:\n",
    "                datasets[\"validation\"] = datasets[\"validation\"].select(range(eval_sample_limit))\n",
    "        else:\n",
    "            logger.info(f\"***** Loading downloaded huggingface datasets: {inoculation_data_path}! *****\")\n",
    "            pd_format = False\n",
    "            if inoculation_data_path in [\"sst3\", \"cola\", \"mnli\", \"snli\", \"mrps\", \"qnli\"]:\n",
    "                pass\n",
    "            raise NotImplementedError()\n",
    "    else:\n",
    "        train_df = pd.read_csv(inoculation_data_path, delimiter=\"\\t\")\n",
    "        eval_df = pd.read_csv(eval_data_path, delimiter=\"\\t\")\n",
    "        test_df = pd.read_csv(test_data_path, delimiter=\"\\t\")\n",
    "        inoculation_step_sample_size = int(len(train_df) * inoculation_step_sample_size)\n",
    "        logger.info(f\"***** Inoculation Sample Count: %s *****\"%(inoculation_step_sample_size))\n",
    "        # this may not always start for zero inoculation\n",
    "        inoculation_train_df = train_df.sample(n=inoculation_step_sample_size, \n",
    "                                               replace=False, \n",
    "                                               random_state=seed) # seed here could not a little annoying.\n",
    "    if pd_format:\n",
    "        datasets = {}\n",
    "        datasets[\"train\"] = Dataset.from_pandas(inoculation_train_df)\n",
    "        datasets[\"validation\"] = Dataset.from_pandas(eval_df)\n",
    "        datasets[\"test\"] = Dataset.from_pandas(test_df)\n",
    "    else:\n",
    "        datasets = {}\n",
    "        datasets[\"train\"] = inoculation_train_df\n",
    "        datasets[\"validation\"] = eval_df\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_CONFIG = {\n",
    "    \"sst3\": (\"text\", None),\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"snli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\")\n",
    "}\n",
    "# WARNING: you dont need BERT tokenizer\n",
    "# original_vocab = load_bert_vocab(\"../data-files/bert_vocab.txt\")\n",
    "# original_tokenizer = transformers.BertTokenizer(\n",
    "#     vocab_file=\"../data-files/bert_vocab.txt\")\n",
    "# Just use some basic white space tokenizor here!\n",
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "max_length = 128\n",
    "per_device_train_batch_size = 128\n",
    "per_device_eval_batch_size = 128\n",
    "no_cuda = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count() if not no_cuda else 1 # 1 means just on cpu\n",
    "seed = 42\n",
    "lr = 1e-3\n",
    "num_train_epochs = 10\n",
    "task_name = \"sst3\"\n",
    "sentence1_key, sentence2_key = TASK_CONFIG[task_name]\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n",
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Test Sample Count (Verify): 2210 *****\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "datasets = get_dataset(f\"../data-files/{data_file_name}/{data_file_name}-train.tsv\", \n",
    "                       f\"../data-files/{data_file_name}/{data_file_name}-dev.tsv\", \n",
    "                       f\"../data-files/{data_file_name}/{data_file_name}-test.tsv\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "logger.info(f\"***** Test Sample Count (Verify): %s *****\"%(len(datasets[\"test\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:27<00:00, 5889.51it/s]\n",
      "100%|██████████| 1100/1100 [00:00<00:00, 2909.80it/s]\n",
      "100%|██████████| 2210/2210 [00:00<00:00, 2802.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the vocab file\n",
    "vocab_index = 0\n",
    "original_vocab = OrderedDict()\n",
    "if \"train\" in datasets:\n",
    "    for (ex_index, example) in enumerate(tqdm(datasets[\"train\"])):\n",
    "        if sentence2_key is None:\n",
    "            sentence_combined = example[sentence1_key]\n",
    "        else:\n",
    "            sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "        sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "        for token in sentence_tokens:\n",
    "            if token not in original_vocab.keys():\n",
    "                original_vocab[token] = vocab_index\n",
    "                vocab_index += 1\n",
    "train_data_only = False\n",
    "if not train_data_only:\n",
    "    if \"validation\" in datasets:\n",
    "        for (ex_index, example) in enumerate(tqdm(datasets[\"validation\"])):\n",
    "            if sentence2_key is None:\n",
    "                sentence_combined = example[sentence1_key]\n",
    "            else:\n",
    "                sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "            sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "            for token in sentence_tokens:\n",
    "                if token not in original_vocab.keys():\n",
    "                    original_vocab[token] = vocab_index\n",
    "                    vocab_index += 1\n",
    "\n",
    "    if \"test\" in datasets:\n",
    "        for (ex_index, example) in enumerate(tqdm(datasets[\"test\"])):\n",
    "            if sentence2_key is None:\n",
    "                sentence_combined = example[sentence1_key]\n",
    "            else:\n",
    "                sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "            sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "            for token in sentence_tokens:\n",
    "                if token not in original_vocab.keys():\n",
    "                    original_vocab[token] = vocab_index\n",
    "                    vocab_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 517/159274 [00:00<01:03, 2518.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: Surprisingly, considering that Baird is a former film editor, the movie is rather choppy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50460/159274 [00:18<00:41, 2626.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: achronological\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 100693/159274 [00:36<00:15, 3680.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: Show\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 150726/159274 [00:50<00:02, 3783.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: picked me up ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:52<00:00, 3005.65it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# BoW feature vectors for train split\n",
    "train_input_features = []\n",
    "train_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(datasets[\"train\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    if ex_index % 50000 == 0:\n",
    "        print(\"Example sentence: \" + sentence_combined)\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    train_input_features.append(bow_feature)\n",
    "    train_label_ids.append(example[\"label\"])\n",
    "    \n",
    "train_input_features = torch.stack(train_input_features, dim=0)\n",
    "train_input_features = torch.tensor(train_input_features, dtype=torch.float)\n",
    "train_label_ids = torch.tensor(train_label_ids, dtype=torch.long)\n",
    "train_data = TensorDataset(train_input_features, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:00<00:00, 1350.96it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# BoW feature vectors for validation split\n",
    "validation_input_features = []\n",
    "validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        if t in original_vocab.keys():\n",
    "            bow_feature[original_vocab[t]] += 1\n",
    "    validation_input_features.append(bow_feature)\n",
    "    validation_label_ids.append(example[\"label\"])\n",
    "\n",
    "validation_input_features = torch.stack(validation_input_features, dim=0)\n",
    "validation_input_features = torch.tensor(validation_input_features, dtype=torch.float)\n",
    "validation_label_ids = torch.tensor(validation_label_ids, dtype=torch.long)\n",
    "validation_data = TensorDataset(validation_input_features, validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=per_device_train_batch_size*n_gpu)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.classifier = nn.Linear(vocab_size, num_labels)\n",
    "    def forward(self, x, labels=None):\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockBERTBOWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(MockBERTBOWClassifier, self).__init__()\n",
    "        hidden_dim = 32\n",
    "        self.mock_bert = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.mock_activation = nn.Tanh()\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "    def forward(self, x, labels=None):\n",
    "        cls = self.mock_activation(self.mock_bert(x))\n",
    "        logits = self.classifier(cls)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some overriding fun stuffs!\n",
    "lr = 1e-3\n",
    "num_train_epochs = 10\n",
    "model = BOWClassifier(len(validation_label_ids.unique()), len(original_vocab))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:17 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.46552   0.37850   0.41753       428\n",
      "           1    0.45627   0.54054   0.49485       444\n",
      "           2    0.22124   0.21930   0.22026       228\n",
      "\n",
      "    accuracy                        0.41091      1100\n",
      "   macro avg    0.38101   0.37945   0.37755      1100\n",
      "weighted avg    0.41115   0.41091   0.40785      1100\n",
      "\n",
      "Macro-F1:  0.37754515040041176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:20 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67143   0.54907   0.60411       428\n",
      "           1    0.64751   0.76126   0.69979       444\n",
      "           2    0.32018   0.32018   0.32018       228\n",
      "\n",
      "    accuracy                        0.58727      1100\n",
      "   macro avg    0.54637   0.54350   0.54136      1100\n",
      "weighted avg    0.58897   0.58727   0.58388      1100\n",
      "\n",
      "Macro-F1:  0.5413605032662876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:27 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67521   0.55374   0.60847       428\n",
      "           1    0.64457   0.78829   0.70922       444\n",
      "           2    0.33010   0.29825   0.31336       228\n",
      "\n",
      "    accuracy                        0.59545      1100\n",
      "   macro avg    0.54996   0.54676   0.54369      1100\n",
      "weighted avg    0.59131   0.59545   0.58797      1100\n",
      "\n",
      "Macro-F1:  0.5436854379896822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:30 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67430   0.61916   0.64555       428\n",
      "           1    0.67823   0.79279   0.73105       444\n",
      "           2    0.38298   0.31579   0.34615       228\n",
      "\n",
      "    accuracy                        0.62636      1100\n",
      "   macro avg    0.57850   0.57591   0.57425      1100\n",
      "weighted avg    0.61550   0.62636   0.61801      1100\n",
      "\n",
      "Macro-F1:  0.5742522847204851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:33 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68345   0.66589   0.67456       428\n",
      "           1    0.69608   0.79955   0.74423       444\n",
      "           2    0.36994   0.28070   0.31920       228\n",
      "\n",
      "    accuracy                        0.64000      1100\n",
      "   macro avg    0.58316   0.58205   0.57933      1100\n",
      "weighted avg    0.62357   0.64000   0.62903      1100\n",
      "\n",
      "Macro-F1:  0.5793310029562648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:37 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67317   0.64486   0.65871       428\n",
      "           1    0.69006   0.79730   0.73981       444\n",
      "           2    0.35593   0.27632   0.31111       228\n",
      "\n",
      "    accuracy                        0.63000      1100\n",
      "   macro avg    0.57305   0.57282   0.56988      1100\n",
      "weighted avg    0.61423   0.63000   0.61940      1100\n",
      "\n",
      "Macro-F1:  0.5698780801735291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:42 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68148   0.64486   0.66267       428\n",
      "           1    0.68992   0.80180   0.74167       444\n",
      "           2    0.35754   0.28070   0.31450       228\n",
      "\n",
      "    accuracy                        0.63273      1100\n",
      "   macro avg    0.57632   0.57579   0.57294      1100\n",
      "weighted avg    0.61774   0.63273   0.62239      1100\n",
      "\n",
      "Macro-F1:  0.572942682396464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:49 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.69136   0.65421   0.67227       428\n",
      "           1    0.68037   0.81982   0.74362       444\n",
      "           2    0.38750   0.27193   0.31959       228\n",
      "\n",
      "    accuracy                        0.64182      1100\n",
      "   macro avg    0.58641   0.58199   0.57849      1100\n",
      "weighted avg    0.62394   0.64182   0.62797      1100\n",
      "\n",
      "Macro-F1:  0.5784908236853917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:54 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68841   0.66589   0.67696       428\n",
      "           1    0.69305   0.80856   0.74636       444\n",
      "           2    0.38690   0.28509   0.32828       228\n",
      "\n",
      "    accuracy                        0.64455      1100\n",
      "   macro avg    0.58945   0.58651   0.58387      1100\n",
      "weighted avg    0.62779   0.64455   0.63270      1100\n",
      "\n",
      "Macro-F1:  0.5838680648656895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:35:57 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.69031   0.68224   0.68625       428\n",
      "           1    0.70020   0.79955   0.74658       444\n",
      "           2    0.38235   0.28509   0.32663       228\n",
      "\n",
      "    accuracy                        0.64727      1100\n",
      "   macro avg    0.59095   0.58896   0.58649      1100\n",
      "weighted avg    0.63047   0.64727   0.63606      1100\n",
      "\n",
      "Macro-F1:  0.5864890597930369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:00 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.69340   0.68692   0.69014       428\n",
      "           1    0.71142   0.79955   0.75292       444\n",
      "           2    0.40678   0.31579   0.35556       228\n",
      "\n",
      "    accuracy                        0.65545      1100\n",
      "   macro avg    0.60387   0.60075   0.59954      1100\n",
      "weighted avg    0.64126   0.65545   0.64613      1100\n",
      "\n",
      "Macro-F1:  0.5995375418134667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:03 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71611   0.65421   0.68376       428\n",
      "           1    0.69981   0.80856   0.75026       444\n",
      "           2    0.39286   0.33772   0.36321       228\n",
      "\n",
      "    accuracy                        0.65091      1100\n",
      "   macro avg    0.60292   0.60016   0.59908      1100\n",
      "weighted avg    0.64253   0.65091   0.64416      1100\n",
      "\n",
      "Macro-F1:  0.5990764879834496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:05 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70792   0.66822   0.68750       428\n",
      "           1    0.69786   0.80631   0.74817       444\n",
      "           2    0.40984   0.32895   0.36496       228\n",
      "\n",
      "    accuracy                        0.65364      1100\n",
      "   macro avg    0.60520   0.60116   0.60021      1100\n",
      "weighted avg    0.64207   0.65364   0.64514      1100\n",
      "\n",
      "Macro-F1:  0.6002116241702197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:08 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70854   0.65888   0.68281       428\n",
      "           1    0.70577   0.79955   0.74974       444\n",
      "           2    0.39196   0.34211   0.36534       228\n",
      "\n",
      "    accuracy                        0.65000      1100\n",
      "   macro avg    0.60209   0.60018   0.59929      1100\n",
      "weighted avg    0.64180   0.65000   0.64402      1100\n",
      "\n",
      "Macro-F1:  0.5992947678696947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:10 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70968   0.66822   0.68833       428\n",
      "           1    0.69804   0.80180   0.74633       444\n",
      "           2    0.40107   0.32895   0.36145       228\n",
      "\n",
      "    accuracy                        0.65182      1100\n",
      "   macro avg    0.60293   0.59966   0.59870      1100\n",
      "weighted avg    0.64101   0.65182   0.64399      1100\n",
      "\n",
      "Macro-F1:  0.598701445505322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:13 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70433   0.68458   0.69431       428\n",
      "           1    0.70000   0.80405   0.74843       444\n",
      "           2    0.41954   0.32018   0.36318       228\n",
      "\n",
      "    accuracy                        0.65727      1100\n",
      "   macro avg    0.60796   0.60294   0.60197      1100\n",
      "weighted avg    0.64355   0.65727   0.64752      1100\n",
      "\n",
      "Macro-F1:  0.6019748495888319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:15 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72222   0.63785   0.67742       428\n",
      "           1    0.69763   0.79505   0.74316       444\n",
      "           2    0.37037   0.35088   0.36036       228\n",
      "\n",
      "    accuracy                        0.64182      1100\n",
      "   macro avg    0.59674   0.59459   0.59365      1100\n",
      "weighted avg    0.63937   0.64182   0.63824      1100\n",
      "\n",
      "Macro-F1:  0.5936458699786374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:18 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71835   0.64953   0.68221       428\n",
      "           1    0.69981   0.80856   0.75026       444\n",
      "           2    0.38000   0.33333   0.35514       228\n",
      "\n",
      "    accuracy                        0.64818      1100\n",
      "   macro avg    0.59938   0.59714   0.59587      1100\n",
      "weighted avg    0.64073   0.64818   0.64188      1100\n",
      "\n",
      "Macro-F1:  0.5958700029642655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:21 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72032   0.63785   0.67658       428\n",
      "           1    0.68798   0.79955   0.73958       444\n",
      "           2    0.37561   0.33772   0.35566       228\n",
      "\n",
      "    accuracy                        0.64091      1100\n",
      "   macro avg    0.59464   0.59171   0.59061      1100\n",
      "weighted avg    0.63582   0.64091   0.63549      1100\n",
      "\n",
      "Macro-F1:  0.5906071525327367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:23 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70918   0.64953   0.67805       428\n",
      "           1    0.70385   0.78153   0.74066       444\n",
      "           2    0.37209   0.35088   0.36117       228\n",
      "\n",
      "    accuracy                        0.64091      1100\n",
      "   macro avg    0.59504   0.59398   0.59329      1100\n",
      "weighted avg    0.63716   0.64091   0.63764      1100\n",
      "\n",
      "Macro-F1:  0.5932947605396274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:25 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71574   0.65888   0.68613       428\n",
      "           1    0.70482   0.79054   0.74522       444\n",
      "           2    0.37981   0.34649   0.36239       228\n",
      "\n",
      "    accuracy                        0.64727      1100\n",
      "   macro avg    0.60012   0.59864   0.59791      1100\n",
      "weighted avg    0.64170   0.64727   0.64288      1100\n",
      "\n",
      "Macro-F1:  0.5979132126328456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:28 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72081   0.66355   0.69100       428\n",
      "           1    0.69941   0.80180   0.74711       444\n",
      "           2    0.39594   0.34211   0.36706       228\n",
      "\n",
      "    accuracy                        0.65273      1100\n",
      "   macro avg    0.60539   0.60249   0.60172      1100\n",
      "weighted avg    0.64484   0.65273   0.64650      1100\n",
      "\n",
      "Macro-F1:  0.6017235886984037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:30 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72800   0.63785   0.67995       428\n",
      "           1    0.69505   0.79054   0.73973       444\n",
      "           2    0.36364   0.35088   0.35714       228\n",
      "\n",
      "    accuracy                        0.64000      1100\n",
      "   macro avg    0.59556   0.59309   0.59227      1100\n",
      "weighted avg    0.63918   0.64000   0.63717      1100\n",
      "\n",
      "Macro-F1:  0.5922730237798731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:33 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72798   0.65654   0.69042       428\n",
      "           1    0.69863   0.80405   0.74764       444\n",
      "           2    0.38424   0.34211   0.36195       228\n",
      "\n",
      "    accuracy                        0.65091      1100\n",
      "   macro avg    0.60362   0.60090   0.60000      1100\n",
      "weighted avg    0.64488   0.65091   0.64543      1100\n",
      "\n",
      "Macro-F1:  0.6000035417972517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 17:36:35 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.73757   0.62383   0.67595       428\n",
      "           1    0.70000   0.78829   0.74153       444\n",
      "           2    0.34874   0.36404   0.35622       228\n",
      "\n",
      "    accuracy                        0.63636      1100\n",
      "   macro avg    0.59544   0.59205   0.59123      1100\n",
      "weighted avg    0.64181   0.63636   0.63615      1100\n",
      "\n",
      "Macro-F1:  0.5912326555943621\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for _ in range(int(num_train_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    # pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, _ = model(input_features, labels=label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # pbar.set_postfix({'train_loss': loss.tolist()})\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            logger.info(\"***** Evaluation Interval Hit *****\")\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_label_ids = []\n",
    "            with torch.no_grad():\n",
    "                # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "                for step, batch in enumerate(validation_dataloader):\n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    input_features, label_ids = batch\n",
    "                    \n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        input_features = input_features.to(device)\n",
    "                        label_ids = label_ids.to(device)\n",
    "                    \n",
    "                    loss, logits = model(input_features, labels=label_ids)\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = label_ids.to('cpu').numpy()\n",
    "                    outputs = np.argmax(logits, axis=1)\n",
    "                    all_logits.append(outputs)\n",
    "                    all_label_ids.append(label_ids)\n",
    "                    \n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "            all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "            result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "            print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "            print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])\n",
    "                    \n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations with frequency-matched scrambling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 21:33:15 - INFO - __main__ - ***** Loading pre-loaded datasets from the disk directly! *****\n",
      "03/17/2021 21:33:15 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "03/17/2021 21:33:15 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/17/2021 21:33:15 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n",
      "100%|██████████| 1100/1100 [00:00<00:00, 1299.60it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "03/17/2021 21:33:16 - INFO - __main__ - ***** Evaluation With Corrupt Data *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.35072   0.28271   0.31307       428\n",
      "           1    0.39341   0.40315   0.39822       444\n",
      "           2    0.22333   0.29386   0.25379       228\n",
      "\n",
      "    accuracy                        0.33364      1100\n",
      "   macro avg    0.32249   0.32657   0.32169      1100\n",
      "weighted avg    0.34155   0.33364   0.33515      1100\n",
      "\n",
      "Macro-F1:  0.3216913667394437\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "corrupt_method = \"S2\"\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "corrupt_datasets = get_dataset(f\"../data-files/{data_file_name}-corrupted-{corrupt_method}\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "\n",
    "corrupt_validation_input_features = []\n",
    "corrupt_validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(corrupt_datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    corrupt_validation_input_features.append(bow_feature)\n",
    "    corrupt_validation_label_ids.append(example[\"label\"])\n",
    "    \n",
    "corrupt_validation_input_features = torch.stack(corrupt_validation_input_features, dim=0)\n",
    "corrupt_validation_input_features = torch.tensor(corrupt_validation_input_features, dtype=torch.float)\n",
    "corrupt_validation_label_ids = torch.tensor(corrupt_validation_label_ids, dtype=torch.long)\n",
    "corrupt_validation_data = TensorDataset(corrupt_validation_input_features, corrupt_validation_label_ids)\n",
    "corrupt_validation_dataloader = DataLoader(corrupt_validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)\n",
    "\n",
    "logger.info(\"***** Evaluation With Corrupt Data *****\")\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_label_ids = []\n",
    "with torch.no_grad():\n",
    "    # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(corrupt_validation_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, logits = model(input_features, labels=label_ids)\n",
    "        logits = F.softmax(logits, dim=-1)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        all_logits.append(outputs)\n",
    "        all_label_ids.append(label_ids)\n",
    "\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations with frequency-unmatched scrambling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 21:34:34 - INFO - __main__ - ***** Loading pre-loaded datasets from the disk directly! *****\n",
      "03/17/2021 21:34:34 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "03/17/2021 21:34:34 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/17/2021 21:34:34 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n",
      "100%|██████████| 1100/1100 [00:01<00:00, 1073.98it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "03/17/2021 21:34:35 - INFO - __main__ - ***** Evaluation With Corrupt Data *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.48649   0.04206   0.07742       428\n",
      "           1    0.38462   0.07883   0.13084       444\n",
      "           2    0.20576   0.87719   0.33333       228\n",
      "\n",
      "    accuracy                        0.23000      1100\n",
      "   macro avg    0.35895   0.33269   0.18053      1100\n",
      "weighted avg    0.38718   0.23000   0.15203      1100\n",
      "\n",
      "Macro-F1:  0.18053126988912335\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "corrupt_method = \"S3\"\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "corrupt_datasets = get_dataset(f\"../data-files/{data_file_name}-corrupted-{corrupt_method}\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "\n",
    "corrupt_validation_input_features = []\n",
    "corrupt_validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(corrupt_datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    corrupt_validation_input_features.append(bow_feature)\n",
    "    corrupt_validation_label_ids.append(example[\"label\"])\n",
    "    \n",
    "corrupt_validation_input_features = torch.stack(corrupt_validation_input_features, dim=0)\n",
    "corrupt_validation_input_features = torch.tensor(corrupt_validation_input_features, dtype=torch.float)\n",
    "corrupt_validation_label_ids = torch.tensor(corrupt_validation_label_ids, dtype=torch.long)\n",
    "corrupt_validation_data = TensorDataset(corrupt_validation_input_features, corrupt_validation_label_ids)\n",
    "corrupt_validation_dataloader = DataLoader(corrupt_validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)\n",
    "\n",
    "logger.info(\"***** Evaluation With Corrupt Data *****\")\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_label_ids = []\n",
    "with torch.no_grad():\n",
    "    # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(corrupt_validation_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, logits = model(input_features, labels=label_ids)\n",
    "        logits = F.softmax(logits, dim=-1)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        all_logits.append(outputs)\n",
    "        all_label_ids.append(label_ids)\n",
    "\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random guessing baseline\n",
    "If we randomly guess the lables, what is the performance now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.38519   0.36449   0.37455       428\n",
      "           1    0.40088   0.40991   0.40535       444\n",
      "           2    0.20747   0.21930   0.21322       228\n",
      "\n",
      "    accuracy                        0.35273      1100\n",
      "   macro avg    0.33118   0.33123   0.33104      1100\n",
      "weighted avg    0.35468   0.35273   0.35354      1100\n",
      "\n",
      "AVG over 100 runs mF1: 0.331816.\n"
     ]
    }
   ],
   "source": [
    "# getting avg mF1 on the dataset with a dummy classifier\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "mf1s = []\n",
    "runs = 100\n",
    "for i in range(runs):\n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "    dummy_clf.fit(validation_input_features, validation_label_ids)\n",
    "    dummy_labels = dummy_clf.predict(validation_input_features)\n",
    "\n",
    "    # dummy performance\n",
    "    # print(classification_report(validation_label_ids, dummy_labels, digits=5))\n",
    "    result_to_save = classification_report(validation_label_ids, dummy_labels, digits=5, output_dict=True)\n",
    "    mf1s += [result_to_save[\"macro avg\"][\"f1-score\"]]\n",
    "print(classification_report(validation_label_ids, dummy_labels, digits=5))\n",
    "print(f\"AVG over {runs} runs mF1: {round(sum(mf1s)/len(mf1s), 6)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FrequencyBoW classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task setups\n",
    "task_name = \"sst3\"\n",
    "num_labels = 3\n",
    "FILENAME_CONFIG = {\n",
    "    \"sst3\" : \"sst-tenary\"\n",
    "}\n",
    "\n",
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                    f\"{FILENAME_CONFIG[task_name]}-train.tsv\"), \n",
    "                       delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-dev.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-test.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing #10000 example...\n",
      "processing #20000 example...\n",
      "processing #30000 example...\n",
      "processing #40000 example...\n",
      "processing #50000 example...\n",
      "processing #60000 example...\n",
      "processing #70000 example...\n",
      "processing #80000 example...\n",
      "processing #90000 example...\n",
      "processing #100000 example...\n",
      "processing #110000 example...\n",
      "processing #120000 example...\n",
      "processing #130000 example...\n",
      "processing #140000 example...\n",
      "processing #150000 example...\n"
     ]
    }
   ],
   "source": [
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "label_vocab_map = {}\n",
    "token_frequency_map = {} # overwrite this everytime for a new dataset\n",
    "for i, example in enumerate(train_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        if label not in label_vocab_map.keys():\n",
    "            label_vocab_map[label] = tokens\n",
    "        else:\n",
    "            for t in tokens:\n",
    "                label_vocab_map[label].append(t)\n",
    "        for t in tokens:\n",
    "            if t in token_frequency_map.keys():\n",
    "                token_frequency_map[t] = token_frequency_map[t] + 1\n",
    "            else:\n",
    "                token_frequency_map[t] = 1\n",
    "for i, example in enumerate(eval_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        if label not in label_vocab_map.keys():\n",
    "            label_vocab_map[label] = tokens\n",
    "        else:\n",
    "            for t in tokens:\n",
    "                label_vocab_map[label].append(t)\n",
    "        for t in tokens:\n",
    "            if t in token_frequency_map.keys():\n",
    "                token_frequency_map[t] = token_frequency_map[t] + 1\n",
    "            else:\n",
    "                token_frequency_map[t] = 1\n",
    "for i, example in enumerate(test_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        if label not in label_vocab_map.keys():\n",
    "            label_vocab_map[label] = tokens\n",
    "        else:\n",
    "            for t in tokens:\n",
    "                label_vocab_map[label].append(t)\n",
    "        for t in tokens:\n",
    "            if t in token_frequency_map.keys():\n",
    "                token_frequency_map[t] = token_frequency_map[t] + 1\n",
    "            else:\n",
    "                token_frequency_map[t] = 1\n",
    "task_token_frequency_map = sorted(token_frequency_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "task_token_frequency_map = OrderedDict(task_token_frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training BoW with 1st order frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq and bucket mappings\n",
    "freq_set = set([])\n",
    "for k, v in task_token_frequency_map.items():\n",
    "    freq_set.add(v)\n",
    "freq_set = list(freq_set)\n",
    "freq_set.sort()\n",
    "freq_bucket = np.logspace(math.log(freq_set[0], 10), math.log(freq_set[-1], 10), 25, endpoint=True)\n",
    "freq_bucket = freq_bucket[:-1]\n",
    "freq_bucket = [math.ceil(n) for n in freq_bucket]\n",
    "# finally the bucket is a map between freq and bucket number\n",
    "def find_bucket_number(freq, freq_bucket):\n",
    "    for i in range(len(freq_bucket)):\n",
    "        if freq > freq_bucket[i]:\n",
    "            continue\n",
    "        else:\n",
    "            return i+1\n",
    "    return len(freq_bucket)\n",
    "\n",
    "freq_bucket_map = {}\n",
    "for freq in freq_set:\n",
    "    bucket_num = find_bucket_number(freq, freq_bucket)\n",
    "    freq_bucket_map[freq] = bucket_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 614/159274 [00:00<00:53, 2962.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: This is one of the year's best films.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0.,\n",
      "        0., 1., 1., 1., 1., 4.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50554/159274 [00:13<00:30, 3534.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: there is plenty of room for editing, and a much shorter cut surely would have resulted in a smoother, more focused narrative without sacrificing any of the cultural intrigue\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 1., 2., 2., 0., 2., 4., 0., 1., 1., 2., 2.,\n",
      "        1., 1., 1., 1., 1., 8.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 100385/159274 [00:29<00:20, 2856.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: his name was, uh, Michael Zaidan, was supposed to have like written the screenplay or something\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 2., 2., 1., 0., 3., 0.,\n",
      "        2., 2., 0., 0., 0., 5.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 150760/159274 [00:43<00:01, 4396.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: They just don't work in concert.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 2., 1.,\n",
      "        0., 1., 0., 0., 1., 2.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:45<00:00, 3536.70it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for train split\n",
    "train_input_features = []\n",
    "train_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(train_df)):\n",
    "    bow_feature = torch.zeros(len(freq_bucket))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[freq_bucket_map[token_frequency_map[t]]-1] += 1 # bucket count\n",
    "    if ex_index % 50000 == 0:\n",
    "        print(\"Example sentence: \" + sentence_combined)\n",
    "        print(bow_feature)\n",
    "    train_input_features.append(bow_feature)\n",
    "    train_label_ids.append(example[\"label\"])\n",
    "    \n",
    "train_input_features = torch.stack(train_input_features, dim=0)\n",
    "train_input_features = torch.tensor(train_input_features, dtype=torch.float)\n",
    "train_label_ids = torch.tensor(train_label_ids, dtype=torch.long)\n",
    "train_data = TensorDataset(train_input_features, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:00<00:00, 2025.39it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for validation split\n",
    "validation_input_features = []\n",
    "validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(eval_df)):\n",
    "    bow_feature = torch.zeros(len(freq_bucket))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[freq_bucket_map[token_frequency_map[t]]-1] += 1 # bucket count\n",
    "    validation_input_features.append(bow_feature)\n",
    "    validation_label_ids.append(example[\"label\"])\n",
    "\n",
    "validation_input_features = torch.stack(validation_input_features, dim=0)\n",
    "validation_input_features = torch.tensor(validation_input_features, dtype=torch.float)\n",
    "validation_label_ids = torch.tensor(validation_label_ids, dtype=torch.long)\n",
    "validation_data = TensorDataset(validation_input_features, validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=per_device_train_batch_size*n_gpu)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some overriding fun stuffs!\n",
    "lr = 1e-3\n",
    "num_train_epochs = 20\n",
    "model = MockBERTBOWClassifier(len(validation_label_ids.unique()), len(freq_bucket))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.33223386421234674\n",
      "Macro-F1:  0.3805544756072737\n",
      "Macro-F1:  0.356948319862475\n",
      "Macro-F1:  0.3555990893841361\n",
      "Macro-F1:  0.39446552989943057\n",
      "Macro-F1:  0.3906249688153513\n",
      "Macro-F1:  0.38934033014915365\n",
      "Macro-F1:  0.3760070440843222\n",
      "Macro-F1:  0.3825677745637313\n",
      "Macro-F1:  0.3538522860773374\n",
      "Macro-F1:  0.38005641677439445\n",
      "Macro-F1:  0.38470228402965184\n",
      "Macro-F1:  0.3856994336330514\n",
      "Macro-F1:  0.3926025252613996\n",
      "Macro-F1:  0.39569701985863376\n",
      "Macro-F1:  0.38227380974097724\n",
      "Macro-F1:  0.3841402824793416\n",
      "Macro-F1:  0.39446810499442075\n",
      "Macro-F1:  0.3856967204688082\n",
      "Macro-F1:  0.3832368082368082\n",
      "Macro-F1:  0.39444279074348504\n",
      "Macro-F1:  0.3833216466303751\n",
      "Macro-F1:  0.38526457790194985\n",
      "Macro-F1:  0.3817557359042263\n",
      "Macro-F1:  0.3744302639659564\n",
      "Macro-F1:  0.394874510845099\n",
      "Macro-F1:  0.38138840217504494\n",
      "Macro-F1:  0.39483517480892644\n",
      "Macro-F1:  0.3921838397993976\n",
      "Macro-F1:  0.3883187852313321\n",
      "Macro-F1:  0.38071647390118724\n",
      "Macro-F1:  0.39052132308947235\n",
      "Macro-F1:  0.38301773569932335\n",
      "Macro-F1:  0.3893260213694106\n",
      "Macro-F1:  0.38720747623323204\n",
      "Macro-F1:  0.3893009187433371\n",
      "Macro-F1:  0.39563246859488793\n",
      "Macro-F1:  0.3562823910475205\n",
      "Macro-F1:  0.38482166956947167\n",
      "Macro-F1:  0.3867463726315623\n",
      "Macro-F1:  0.39182974223760275\n",
      "Macro-F1:  0.39365690766462375\n",
      "Macro-F1:  0.38391918878218906\n",
      "Macro-F1:  0.39522854891094744\n",
      "Macro-F1:  0.38464675600466286\n",
      "Macro-F1:  0.3941793173904106\n",
      "Macro-F1:  0.39669738133687976\n",
      "Macro-F1:  0.3889975187477202\n",
      "Macro-F1:  0.39236657496754646\n",
      "Macro-F1:  0.3642617323125228\n",
      "Best Macro-F1:  0.39669738133687976\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "max_score = -1\n",
    "for _ in range(int(num_train_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    # pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, _ = model(input_features, labels=label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # pbar.set_postfix({'train_loss': loss.tolist()})\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            # logger.info(\"***** Evaluation Interval Hit *****\")\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_label_ids = []\n",
    "            with torch.no_grad():\n",
    "                # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "                for step, batch in enumerate(validation_dataloader):\n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    input_features, label_ids = batch\n",
    "                    \n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        input_features = input_features.to(device)\n",
    "                        label_ids = label_ids.to(device)\n",
    "                    \n",
    "                    loss, logits = model(input_features, labels=label_ids)\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = label_ids.to('cpu').numpy()\n",
    "                    outputs = np.argmax(logits, axis=1)\n",
    "                    all_logits.append(outputs)\n",
    "                    all_label_ids.append(label_ids)\n",
    "                    \n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "            all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "            result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "            # print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "            print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])\n",
    "            if result_to_save[\"macro avg\"][\"f1-score\"] > max_score:\n",
    "                max_score = result_to_save[\"macro avg\"][\"f1-score\"]\n",
    "                    \n",
    "        global_step += 1\n",
    "print(\"Best Macro-F1: \", max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training BoW with 1st and 2nd order frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition the first order information\n",
    "second_order_freq_set = set([])\n",
    "for k, v in task_token_frequency_map.items():\n",
    "    second_order_freq_set.add(v)\n",
    "second_order_freq_set = list(second_order_freq_set)\n",
    "second_order_freq_set.sort()\n",
    "bucket_count = 36\n",
    "second_order_freq_bucket = np.logspace(math.log(second_order_freq_set[0], 10), \n",
    "                          math.log(second_order_freq_set[-1], 10), bucket_count+1, \n",
    "                          endpoint=True)\n",
    "second_order_freq_bucket = second_order_freq_bucket[:-1]\n",
    "second_order_freq_bucket = [math.ceil(n) for n in second_order_freq_bucket]\n",
    "# finally the bucket is a map between freq and bucket number\n",
    "def find_bucket_number(freq, freq_bucket):\n",
    "    for i in range(len(freq_bucket)):\n",
    "        if freq > freq_bucket[i]:\n",
    "            continue\n",
    "        else:\n",
    "            return i+1\n",
    "    return len(freq_bucket)\n",
    "\n",
    "second_order_freq_bucket_map = {}\n",
    "for freq in freq_set:\n",
    "    bucket_num = find_bucket_number(freq, second_order_freq_bucket)\n",
    "    second_order_freq_bucket_map[freq] = bucket_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1,\n",
       " 2: 2,\n",
       " 3: 4,\n",
       " 4: 5,\n",
       " 5: 6,\n",
       " 6: 7,\n",
       " 7: 7,\n",
       " 8: 8,\n",
       " 9: 8,\n",
       " 10: 9,\n",
       " 11: 9,\n",
       " 12: 9,\n",
       " 13: 10,\n",
       " 14: 10,\n",
       " 15: 10,\n",
       " 16: 10,\n",
       " 17: 11,\n",
       " 18: 11,\n",
       " 19: 11,\n",
       " 20: 11,\n",
       " 21: 11,\n",
       " 22: 12,\n",
       " 23: 12,\n",
       " 24: 12,\n",
       " 25: 12,\n",
       " 26: 12,\n",
       " 27: 12,\n",
       " 28: 12,\n",
       " 29: 12,\n",
       " 30: 13,\n",
       " 31: 13,\n",
       " 32: 13,\n",
       " 33: 13,\n",
       " 34: 13,\n",
       " 35: 13,\n",
       " 36: 13,\n",
       " 37: 13,\n",
       " 38: 13,\n",
       " 39: 13,\n",
       " 40: 14,\n",
       " 41: 14,\n",
       " 42: 14,\n",
       " 43: 14,\n",
       " 44: 14,\n",
       " 45: 14,\n",
       " 46: 14,\n",
       " 47: 14,\n",
       " 48: 14,\n",
       " 49: 14,\n",
       " 50: 14,\n",
       " 51: 14,\n",
       " 52: 14,\n",
       " 53: 15,\n",
       " 54: 15,\n",
       " 55: 15,\n",
       " 56: 15,\n",
       " 57: 15,\n",
       " 58: 15,\n",
       " 59: 15,\n",
       " 60: 15,\n",
       " 61: 15,\n",
       " 62: 15,\n",
       " 63: 15,\n",
       " 64: 15,\n",
       " 65: 15,\n",
       " 66: 15,\n",
       " 67: 15,\n",
       " 68: 15,\n",
       " 69: 15,\n",
       " 70: 15,\n",
       " 71: 16,\n",
       " 72: 16,\n",
       " 73: 16,\n",
       " 74: 16,\n",
       " 75: 16,\n",
       " 76: 16,\n",
       " 77: 16,\n",
       " 78: 16,\n",
       " 79: 16,\n",
       " 80: 16,\n",
       " 81: 16,\n",
       " 82: 16,\n",
       " 83: 16,\n",
       " 84: 16,\n",
       " 85: 16,\n",
       " 86: 16,\n",
       " 87: 16,\n",
       " 88: 16,\n",
       " 89: 16,\n",
       " 90: 16,\n",
       " 91: 16,\n",
       " 92: 16,\n",
       " 93: 16,\n",
       " 94: 16,\n",
       " 95: 16,\n",
       " 96: 17,\n",
       " 97: 17,\n",
       " 98: 17,\n",
       " 99: 17,\n",
       " 100: 17,\n",
       " 101: 17,\n",
       " 102: 17,\n",
       " 103: 17,\n",
       " 104: 17,\n",
       " 105: 17,\n",
       " 106: 17,\n",
       " 107: 17,\n",
       " 108: 17,\n",
       " 109: 17,\n",
       " 110: 17,\n",
       " 111: 17,\n",
       " 112: 17,\n",
       " 113: 17,\n",
       " 114: 17,\n",
       " 115: 17,\n",
       " 116: 17,\n",
       " 117: 17,\n",
       " 118: 17,\n",
       " 119: 17,\n",
       " 120: 17,\n",
       " 121: 17,\n",
       " 122: 17,\n",
       " 123: 17,\n",
       " 124: 17,\n",
       " 125: 17,\n",
       " 126: 17,\n",
       " 127: 17,\n",
       " 128: 17,\n",
       " 129: 17,\n",
       " 130: 18,\n",
       " 131: 18,\n",
       " 132: 18,\n",
       " 133: 18,\n",
       " 134: 18,\n",
       " 135: 18,\n",
       " 136: 18,\n",
       " 137: 18,\n",
       " 138: 18,\n",
       " 139: 18,\n",
       " 140: 18,\n",
       " 141: 18,\n",
       " 142: 18,\n",
       " 143: 18,\n",
       " 144: 18,\n",
       " 145: 18,\n",
       " 146: 18,\n",
       " 147: 18,\n",
       " 148: 18,\n",
       " 149: 18,\n",
       " 150: 18,\n",
       " 151: 18,\n",
       " 152: 18,\n",
       " 153: 18,\n",
       " 154: 18,\n",
       " 155: 18,\n",
       " 156: 18,\n",
       " 157: 18,\n",
       " 158: 18,\n",
       " 159: 18,\n",
       " 160: 18,\n",
       " 161: 18,\n",
       " 162: 18,\n",
       " 163: 18,\n",
       " 164: 18,\n",
       " 165: 18,\n",
       " 166: 18,\n",
       " 167: 18,\n",
       " 168: 18,\n",
       " 169: 18,\n",
       " 170: 18,\n",
       " 171: 18,\n",
       " 172: 18,\n",
       " 173: 18,\n",
       " 174: 18,\n",
       " 175: 19,\n",
       " 176: 19,\n",
       " 177: 19,\n",
       " 178: 19,\n",
       " 179: 19,\n",
       " 180: 19,\n",
       " 181: 19,\n",
       " 182: 19,\n",
       " 183: 19,\n",
       " 184: 19,\n",
       " 185: 19,\n",
       " 186: 19,\n",
       " 187: 19,\n",
       " 188: 19,\n",
       " 189: 19,\n",
       " 190: 19,\n",
       " 191: 19,\n",
       " 192: 19,\n",
       " 193: 19,\n",
       " 195: 19,\n",
       " 196: 19,\n",
       " 197: 19,\n",
       " 198: 19,\n",
       " 199: 19,\n",
       " 200: 19,\n",
       " 201: 19,\n",
       " 202: 19,\n",
       " 203: 19,\n",
       " 204: 19,\n",
       " 205: 19,\n",
       " 206: 19,\n",
       " 207: 19,\n",
       " 208: 19,\n",
       " 210: 19,\n",
       " 211: 19,\n",
       " 212: 19,\n",
       " 213: 19,\n",
       " 214: 19,\n",
       " 215: 19,\n",
       " 216: 19,\n",
       " 217: 19,\n",
       " 218: 19,\n",
       " 219: 19,\n",
       " 220: 19,\n",
       " 221: 19,\n",
       " 222: 19,\n",
       " 224: 19,\n",
       " 225: 19,\n",
       " 226: 19,\n",
       " 227: 19,\n",
       " 228: 19,\n",
       " 229: 19,\n",
       " 230: 19,\n",
       " 231: 19,\n",
       " 232: 19,\n",
       " 234: 19,\n",
       " 235: 19,\n",
       " 236: 20,\n",
       " 237: 20,\n",
       " 238: 20,\n",
       " 239: 20,\n",
       " 240: 20,\n",
       " 241: 20,\n",
       " 242: 20,\n",
       " 243: 20,\n",
       " 244: 20,\n",
       " 245: 20,\n",
       " 246: 20,\n",
       " 247: 20,\n",
       " 248: 20,\n",
       " 249: 20,\n",
       " 250: 20,\n",
       " 251: 20,\n",
       " 252: 20,\n",
       " 253: 20,\n",
       " 254: 20,\n",
       " 256: 20,\n",
       " 257: 20,\n",
       " 258: 20,\n",
       " 259: 20,\n",
       " 261: 20,\n",
       " 262: 20,\n",
       " 263: 20,\n",
       " 264: 20,\n",
       " 265: 20,\n",
       " 266: 20,\n",
       " 268: 20,\n",
       " 269: 20,\n",
       " 271: 20,\n",
       " 272: 20,\n",
       " 275: 20,\n",
       " 277: 20,\n",
       " 278: 20,\n",
       " 279: 20,\n",
       " 280: 20,\n",
       " 281: 20,\n",
       " 282: 20,\n",
       " 283: 20,\n",
       " 284: 20,\n",
       " 285: 20,\n",
       " 286: 20,\n",
       " 288: 20,\n",
       " 290: 20,\n",
       " 291: 20,\n",
       " 292: 20,\n",
       " 293: 20,\n",
       " 295: 20,\n",
       " 296: 20,\n",
       " 297: 20,\n",
       " 298: 20,\n",
       " 300: 20,\n",
       " 301: 20,\n",
       " 302: 20,\n",
       " 303: 20,\n",
       " 304: 20,\n",
       " 305: 20,\n",
       " 308: 20,\n",
       " 309: 20,\n",
       " 310: 20,\n",
       " 311: 20,\n",
       " 312: 20,\n",
       " 313: 20,\n",
       " 314: 20,\n",
       " 315: 20,\n",
       " 317: 20,\n",
       " 318: 20,\n",
       " 320: 21,\n",
       " 321: 21,\n",
       " 322: 21,\n",
       " 323: 21,\n",
       " 324: 21,\n",
       " 325: 21,\n",
       " 326: 21,\n",
       " 328: 21,\n",
       " 329: 21,\n",
       " 330: 21,\n",
       " 331: 21,\n",
       " 332: 21,\n",
       " 333: 21,\n",
       " 335: 21,\n",
       " 336: 21,\n",
       " 338: 21,\n",
       " 339: 21,\n",
       " 340: 21,\n",
       " 341: 21,\n",
       " 342: 21,\n",
       " 344: 21,\n",
       " 346: 21,\n",
       " 347: 21,\n",
       " 348: 21,\n",
       " 349: 21,\n",
       " 350: 21,\n",
       " 354: 21,\n",
       " 355: 21,\n",
       " 358: 21,\n",
       " 359: 21,\n",
       " 364: 21,\n",
       " 365: 21,\n",
       " 366: 21,\n",
       " 367: 21,\n",
       " 368: 21,\n",
       " 370: 21,\n",
       " 372: 21,\n",
       " 373: 21,\n",
       " 374: 21,\n",
       " 378: 21,\n",
       " 381: 21,\n",
       " 382: 21,\n",
       " 383: 21,\n",
       " 384: 21,\n",
       " 385: 21,\n",
       " 387: 21,\n",
       " 389: 21,\n",
       " 390: 21,\n",
       " 391: 21,\n",
       " 392: 21,\n",
       " 394: 21,\n",
       " 395: 21,\n",
       " 397: 21,\n",
       " 399: 21,\n",
       " 402: 21,\n",
       " 403: 21,\n",
       " 405: 21,\n",
       " 406: 21,\n",
       " 407: 21,\n",
       " 410: 21,\n",
       " 411: 21,\n",
       " 413: 21,\n",
       " 414: 21,\n",
       " 417: 21,\n",
       " 418: 21,\n",
       " 419: 21,\n",
       " 420: 21,\n",
       " 422: 21,\n",
       " 423: 21,\n",
       " 424: 21,\n",
       " 427: 21,\n",
       " 430: 21,\n",
       " 432: 22,\n",
       " 433: 22,\n",
       " 436: 22,\n",
       " 437: 22,\n",
       " 438: 22,\n",
       " 439: 22,\n",
       " 440: 22,\n",
       " 441: 22,\n",
       " 442: 22,\n",
       " 445: 22,\n",
       " 448: 22,\n",
       " 449: 22,\n",
       " 452: 22,\n",
       " 457: 22,\n",
       " 460: 22,\n",
       " 463: 22,\n",
       " 464: 22,\n",
       " 466: 22,\n",
       " 467: 22,\n",
       " 471: 22,\n",
       " 474: 22,\n",
       " 476: 22,\n",
       " 478: 22,\n",
       " 479: 22,\n",
       " 480: 22,\n",
       " 484: 22,\n",
       " 487: 22,\n",
       " 493: 22,\n",
       " 494: 22,\n",
       " 499: 22,\n",
       " 500: 22,\n",
       " 505: 22,\n",
       " 506: 22,\n",
       " 507: 22,\n",
       " 512: 22,\n",
       " 513: 22,\n",
       " 514: 22,\n",
       " 517: 22,\n",
       " 522: 22,\n",
       " 528: 22,\n",
       " 531: 22,\n",
       " 532: 22,\n",
       " 536: 22,\n",
       " 537: 22,\n",
       " 538: 22,\n",
       " 539: 22,\n",
       " 540: 22,\n",
       " 544: 22,\n",
       " 545: 22,\n",
       " 546: 22,\n",
       " 547: 22,\n",
       " 559: 22,\n",
       " 565: 22,\n",
       " 566: 22,\n",
       " 570: 22,\n",
       " 576: 22,\n",
       " 577: 22,\n",
       " 578: 22,\n",
       " 579: 22,\n",
       " 580: 22,\n",
       " 586: 23,\n",
       " 587: 23,\n",
       " 591: 23,\n",
       " 599: 23,\n",
       " 602: 23,\n",
       " 604: 23,\n",
       " 605: 23,\n",
       " 609: 23,\n",
       " 614: 23,\n",
       " 615: 23,\n",
       " 617: 23,\n",
       " 619: 23,\n",
       " 621: 23,\n",
       " 631: 23,\n",
       " 632: 23,\n",
       " 645: 23,\n",
       " 646: 23,\n",
       " 647: 23,\n",
       " 651: 23,\n",
       " 655: 23,\n",
       " 657: 23,\n",
       " 660: 23,\n",
       " 665: 23,\n",
       " 667: 23,\n",
       " 670: 23,\n",
       " 673: 23,\n",
       " 675: 23,\n",
       " 680: 23,\n",
       " 682: 23,\n",
       " 683: 23,\n",
       " 684: 23,\n",
       " 686: 23,\n",
       " 690: 23,\n",
       " 691: 23,\n",
       " 696: 23,\n",
       " 698: 23,\n",
       " 699: 23,\n",
       " 707: 23,\n",
       " 712: 23,\n",
       " 724: 23,\n",
       " 741: 23,\n",
       " 748: 23,\n",
       " 751: 23,\n",
       " 753: 23,\n",
       " 754: 23,\n",
       " 763: 23,\n",
       " 764: 23,\n",
       " 769: 23,\n",
       " 784: 23,\n",
       " 789: 23,\n",
       " 794: 24,\n",
       " 797: 24,\n",
       " 799: 24,\n",
       " 804: 24,\n",
       " 806: 24,\n",
       " 822: 24,\n",
       " 824: 24,\n",
       " 828: 24,\n",
       " 831: 24,\n",
       " 833: 24,\n",
       " 838: 24,\n",
       " 845: 24,\n",
       " 846: 24,\n",
       " 851: 24,\n",
       " 854: 24,\n",
       " 867: 24,\n",
       " 868: 24,\n",
       " 873: 24,\n",
       " 876: 24,\n",
       " 914: 24,\n",
       " 915: 24,\n",
       " 918: 24,\n",
       " 919: 24,\n",
       " 921: 24,\n",
       " 934: 24,\n",
       " 938: 24,\n",
       " 939: 24,\n",
       " 944: 24,\n",
       " 946: 24,\n",
       " 949: 24,\n",
       " 953: 24,\n",
       " 956: 24,\n",
       " 962: 24,\n",
       " 965: 24,\n",
       " 980: 24,\n",
       " 986: 24,\n",
       " 1025: 24,\n",
       " 1040: 24,\n",
       " 1045: 24,\n",
       " 1047: 24,\n",
       " 1049: 24,\n",
       " 1051: 24,\n",
       " 1058: 24,\n",
       " 1066: 24,\n",
       " 1070: 24,\n",
       " 1073: 25,\n",
       " 1074: 25,\n",
       " 1091: 25,\n",
       " 1102: 25,\n",
       " 1130: 25,\n",
       " 1146: 25,\n",
       " 1150: 25,\n",
       " 1159: 25,\n",
       " 1162: 25,\n",
       " 1186: 25,\n",
       " 1195: 25,\n",
       " 1197: 25,\n",
       " 1202: 25,\n",
       " 1205: 25,\n",
       " 1222: 25,\n",
       " 1225: 25,\n",
       " 1231: 25,\n",
       " 1234: 25,\n",
       " 1243: 25,\n",
       " 1244: 25,\n",
       " 1264: 25,\n",
       " 1288: 25,\n",
       " 1301: 25,\n",
       " 1302: 25,\n",
       " 1331: 25,\n",
       " 1344: 25,\n",
       " 1354: 25,\n",
       " 1366: 25,\n",
       " 1389: 25,\n",
       " 1391: 25,\n",
       " 1398: 25,\n",
       " 1414: 25,\n",
       " 1426: 25,\n",
       " 1451: 26,\n",
       " 1463: 26,\n",
       " 1493: 26,\n",
       " 1507: 26,\n",
       " 1509: 26,\n",
       " 1517: 26,\n",
       " 1548: 26,\n",
       " 1565: 26,\n",
       " 1630: 26,\n",
       " 1642: 26,\n",
       " 1661: 26,\n",
       " 1682: 26,\n",
       " 1708: 26,\n",
       " 1754: 26,\n",
       " 1799: 26,\n",
       " 1822: 26,\n",
       " 1856: 26,\n",
       " 1892: 26,\n",
       " 1948: 26,\n",
       " 1977: 27,\n",
       " 1978: 27,\n",
       " 1997: 27,\n",
       " 2003: 27,\n",
       " 2020: 27,\n",
       " 2064: 27,\n",
       " 2069: 27,\n",
       " 2174: 27,\n",
       " 2267: 27,\n",
       " 2302: 27,\n",
       " 2324: 27,\n",
       " 2359: 27,\n",
       " 2400: 27,\n",
       " 2415: 27,\n",
       " 2517: 27,\n",
       " 2570: 27,\n",
       " 2614: 27,\n",
       " 2714: 28,\n",
       " 2935: 28,\n",
       " 3051: 28,\n",
       " 3094: 28,\n",
       " 3190: 28,\n",
       " 3244: 28,\n",
       " 3461: 28,\n",
       " 3612: 29,\n",
       " 3672: 29,\n",
       " 3701: 29,\n",
       " 3715: 29,\n",
       " 3802: 29,\n",
       " 3814: 29,\n",
       " 3918: 29,\n",
       " 3993: 29,\n",
       " 4031: 29,\n",
       " 4118: 29,\n",
       " 4275: 29,\n",
       " 4387: 29,\n",
       " 5208: 30,\n",
       " 5251: 30,\n",
       " 5267: 30,\n",
       " 5333: 30,\n",
       " 5621: 30,\n",
       " 6172: 30,\n",
       " 6657: 31,\n",
       " 6945: 31,\n",
       " 7235: 31,\n",
       " 7536: 31,\n",
       " 7992: 31,\n",
       " 8239: 31,\n",
       " 9218: 32,\n",
       " 12815: 33,\n",
       " 13132: 33,\n",
       " 13787: 33,\n",
       " 14873: 33,\n",
       " 18300: 34,\n",
       " 24133: 35,\n",
       " 29097: 35,\n",
       " 29699: 35,\n",
       " 34293: 36,\n",
       " 34761: 36,\n",
       " 38798: 36,\n",
       " 42048: 36,\n",
       " 45195: 36,\n",
       " 55154: 36}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_order_freq_bucket_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "token_freq_freq_map = {} # overwrite this everytime for a new dataset\n",
    "for i, example in enumerate(train_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        for i in range(len(tokens)-1):\n",
    "            for j in range(i+1, len(tokens)):\n",
    "                t1 = sentence_tokens[i]\n",
    "                t2 = sentence_tokens[j]\n",
    "                index_tuple = [second_order_freq_bucket_map[token_frequency_map[t1]], \n",
    "                               second_order_freq_bucket_map[token_frequency_map[t2]]]\n",
    "                index_tuple.sort()\n",
    "                index_tuple = tuple(index_tuple)\n",
    "                \n",
    "                \n",
    "task_token_frequency_map = sorted(token_frequency_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "task_token_frequency_map = OrderedDict(task_token_frequency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_bucket_index = {}\n",
    "index = 0\n",
    "for i in range(1, len(freq_bucket)+1):\n",
    "    for j in range(i+1, len(freq_bucket)+1):\n",
    "        second_order_bucket_index[(i,j)] = index\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 83/159274 [00:00<03:11, 829.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: This is one of the year's best films.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 0.,\n",
      "        0., 1., 1., 1., 1., 4., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 1., 1., 1.,\n",
      "        1., 4., 0., 0., 2., 2., 2., 2., 8., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 4., 1., 1., 4., 1., 4., 4.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50191/159274 [00:45<01:26, 1262.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: there is plenty of room for editing, and a much shorter cut surely would have resulted in a smoother, more focused narrative without sacrificing any of the cultural intrigue\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  2.,  2.,  0.,  2.,  4.,  0.,\n",
      "         1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  8.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  2.,  2.,  0.,  2.,  4.,  0.,\n",
      "         1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  8.,  2.,  2.,  0.,  2.,\n",
      "         4.,  0.,  1.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  8.,  4.,  0.,\n",
      "         4.,  8.,  0.,  2.,  2.,  4.,  4.,  2.,  2.,  2.,  2.,  2., 16.,  0.,\n",
      "         4.,  8.,  0.,  2.,  2.,  4.,  4.,  2.,  2.,  2.,  2.,  2., 16.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  8.,  0.,\n",
      "         2.,  2.,  4.,  4.,  2.,  2.,  2.,  2.,  2., 16.,  0.,  4.,  4.,  8.,\n",
      "         8.,  4.,  4.,  4.,  4.,  4., 32.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  1.,  2.,  2.,  1.,  1.,  1.,  1.,  1.,  8.,  2.,  2.,\n",
      "         1.,  1.,  1.,  1.,  1.,  8.,  4.,  2.,  2.,  2.,  2.,  2., 16.,  2.,\n",
      "         2.,  2.,  2.,  2., 16.,  1.,  1.,  1.,  1.,  8.,  1.,  1.,  1.,  8.,\n",
      "         1.,  1.,  8.,  1.,  8.,  8.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 100124/159274 [01:27<00:49, 1195.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: his name was, uh, Michael Zaidan, was supposed to have like written the screenplay or something\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  2.,  2.,\n",
      "         1.,  0.,  3.,  0.,  2.,  2.,  0.,  0.,  0.,  5.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
      "         2.,  2.,  1.,  0.,  3.,  0.,  2.,  2.,  0.,  0.,  0.,  5.,  0.,  0.,\n",
      "         0.,  2.,  2.,  1.,  0.,  3.,  0.,  2.,  2.,  0.,  0.,  0.,  5.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  2.,  0.,  6.,\n",
      "         0.,  4.,  4.,  0.,  0.,  0., 10.,  2.,  0.,  6.,  0.,  4.,  4.,  0.,\n",
      "         0.,  0., 10.,  0.,  3.,  0.,  2.,  2.,  0.,  0.,  0.,  5.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  6.,  6.,  0.,  0.,  0., 15.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  4.,  0.,  0.,  0., 10.,  0.,  0.,  0., 10.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 150143/159274 [02:10<00:07, 1176.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: They just don't work in concert.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 2., 1.,\n",
      "        0., 1., 0., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 2., 1., 0., 1., 0., 0., 1., 2., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 0., 1., 0., 0.,\n",
      "        1., 2., 2., 0., 2., 0., 0., 2., 4., 0., 1., 0., 0., 1., 2., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 2., 0., 0., 0., 0., 0., 2.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [02:18<00:00, 1148.13it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for train split (2nd order = 1st order concat with 2nd order)\n",
    "# Note that the concatenation might not be necessary? as it might be catched with as 帝王让train_label_ids = []21\n",
    "for (ex_index, example) in enumerate(tqdm(train_df)):\n",
    "    bow_feature = torch.zeros(len(freq_bucket) + len(second_order_bucket_index)) # up-to 2nd feature map\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    # first order here!\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[freq_bucket_map[token_frequency_map[t]]-1] += 1 # bucket count\n",
    "    # second order here!\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        for j in range(i+1, len(sentence_tokens)):\n",
    "            t1 = sentence_tokens[i]\n",
    "            t2 = sentence_tokens[j]\n",
    "            if freq_bucket_map[token_frequency_map[t1]] != freq_bucket_map[token_frequency_map[t2]]:\n",
    "                index_tuple = [freq_bucket_map[token_frequency_map[t1]], freq_bucket_map[token_frequency_map[t2]]]\n",
    "                index_tuple.sort()\n",
    "                index_tuple = tuple(index_tuple)\n",
    "                bow_feature[len(freq_bucket) + second_order_bucket_index[index_tuple]] += 1 # pair of freq bucket count\n",
    "\n",
    "    if ex_index % 50000 == 0:\n",
    "        print(\"Example sentence: \" + sentence_combined)\n",
    "        print(bow_feature)\n",
    "    train_input_features.append(bow_feature)\n",
    "    train_label_ids.append(example[\"label\"])\n",
    "    \n",
    "train_input_features = torch.stack(train_input_features, dim=0)\n",
    "train_input_features = torch.tensor(train_input_features, dtype=torch.float)\n",
    "train_label_ids = torch.tensor(train_label_ids, dtype=torch.long)\n",
    "train_data = TensorDataset(train_input_features, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:03<00:00, 323.29it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for validation split\n",
    "validation_input_features = []\n",
    "validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(eval_df)):\n",
    "    bow_feature = torch.zeros(len(freq_bucket) + len(second_order_bucket_index)) # up-to 2nd feature map\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    # first order here!\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[freq_bucket_map[token_frequency_map[t]]-1] += 1 # bucket count\n",
    "    # second order here!\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        for j in range(i+1, len(sentence_tokens)):\n",
    "            t1 = sentence_tokens[i]\n",
    "            t2 = sentence_tokens[j]\n",
    "            if freq_bucket_map[token_frequency_map[t1]] != freq_bucket_map[token_frequency_map[t2]]:\n",
    "                index_tuple = [freq_bucket_map[token_frequency_map[t1]], freq_bucket_map[token_frequency_map[t2]]]\n",
    "                index_tuple.sort()\n",
    "                index_tuple = tuple(index_tuple)\n",
    "                bow_feature[len(freq_bucket) + second_order_bucket_index[index_tuple]] += 1 # pair of freq bucket count\n",
    "    validation_input_features.append(bow_feature)\n",
    "    validation_label_ids.append(example[\"label\"])\n",
    "\n",
    "validation_input_features = torch.stack(validation_input_features, dim=0)\n",
    "validation_input_features = torch.tensor(validation_input_features, dtype=torch.float)\n",
    "validation_label_ids = torch.tensor(validation_label_ids, dtype=torch.long)\n",
    "validation_data = TensorDataset(validation_input_features, validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=per_device_train_batch_size*n_gpu)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the model\n",
    "model = MockBERTBOWClassifier(len(validation_label_ids.unique()), len(freq_bucket) + len(second_order_bucket_index))\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:36 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.26465374257277996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:37 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3849117003935271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:37 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.33312358005787224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:38 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.39216997619791044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:39 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3827080906711444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:39 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3861029770092592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:40 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3954782550915313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:40 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3753367702737842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:41 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36673013836654283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:41 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.35828267237856926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:42 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37487471430197505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:43 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36452234585587134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:43 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.38704398321802674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:44 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.367700667611971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:44 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.38717515220992155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:45 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36287540014192454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:46 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36750048573257316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:46 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3718295722896891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:47 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3739680472101336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:47 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.38340047871281807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:48 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3691166509543012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:48 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37794799435597665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:49 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3666181968966593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:50 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.395071985780241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:50 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3677363088444458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:51 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3766712085835382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:51 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3877688893757536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:52 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3631892905007484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:53 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.38161909108015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:53 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3851847525216518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:54 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37694433006385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:54 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3857505577414764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:55 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3720237188351562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:56 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3763660262857029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:56 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37583857877467547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:57 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3705145153005207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:57 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3704313730701782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:58 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3757634899204885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:59 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36472363503876076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:01:59 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3806590858920995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:00 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3668103781633774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:00 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3679673385403159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:01 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3623292037339973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:01 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3838446616080775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:02 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3715017360159756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:03 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3679756071669295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:03 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.377877290754333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:04 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37782638241334965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:04 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37002411614700487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:05 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3757617523678523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:06 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3764214873793564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:06 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3648619533210982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:07 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3698294346978557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:07 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3668667356658683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:08 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3818227559212691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:08 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37243588531613203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:09 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3734812079204645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:10 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36514063906301414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:10 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37553082800842663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:11 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37376907866285713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:11 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36766964759214166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:12 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36530776730272274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:12 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36889535024523123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:13 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3670527614915886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:14 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36469871782151464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:14 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36265205992886934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:15 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.38097602540963105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:15 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36690868361755974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:16 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.360459484467454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:17 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3598470695749172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:17 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37509714556858326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:18 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3792722418898222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:18 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3755737599835904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:19 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.375396841438925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:19 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3745164972889789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:20 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.35750326409610506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:21 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36198536830042577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:21 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37563723572871455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:22 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.363106544886971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:22 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3691028876872433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:23 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3663022345897045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:24 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3686544404040137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:24 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3683450147735862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:25 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37586938897422767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:25 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3629765571884113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:26 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36711890301925676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:26 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.35520302704097473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:27 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3693716135159315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:28 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37051645672335326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:28 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3807557809993179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:29 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3749062386219674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:29 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3791587896584281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:30 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36552569590898504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:30 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3623121834165863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:30 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37007271816137594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:31 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3731623747242603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:31 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36285073593730105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:32 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.366116928897007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:32 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37087576942195605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:32 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37157199930503954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:33 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3766432386013007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:33 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3821629810170542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:34 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3742825809314792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:34 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37215180162703904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:35 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3554314624982869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:35 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37297693401731374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:35 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3727088617213217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:36 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3626779306800032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:36 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3615576317476243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:37 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37154811440348584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:37 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37321845919184643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:37 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3619379678499402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:38 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.38211189291375053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:38 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37036493826671113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:39 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36707585990117475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:39 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3702204855951103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:39 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3779103108984862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:40 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3747806566178673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:40 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3659171795499605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:41 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36732148949869997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:41 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37605479277158627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:42 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3692183329520324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:42 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37211444738144284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:42 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3713098980465808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:43 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37510074246819486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:43 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3666555254758361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:44 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37592902753808594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:44 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36429995159975936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:45 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36959043246065715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:45 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37086920388194317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:45 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3773994795184092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:46 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.378786287284303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:46 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36103637926655335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:47 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36963656094131175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:47 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36776747393524384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:48 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3744869256717142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:48 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36694792782119684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:49 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36960970577905355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:49 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.371662518683557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:49 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3709019987901385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:50 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37771517264288007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:50 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3684753974207289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:51 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37153759090269217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:51 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36820217604947186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:52 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3852839121273919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:52 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3691268885316039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:52 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36272535500007436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:53 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.3698808576129977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:53 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.37190842649302375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 22:02:54 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.36907919912370213\n",
      "Best Macro-F1:  0.3954782550915313\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "num_train_epochs = 30\n",
    "max_score = -1\n",
    "for _ in range(int(num_train_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    # pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, _ = model(input_features, labels=label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # pbar.set_postfix({'train_loss': loss.tolist()})\n",
    "\n",
    "        if global_step % 250 == 0:\n",
    "            logger.info(\"***** Evaluation Interval Hit *****\")\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_label_ids = []\n",
    "            with torch.no_grad():\n",
    "                # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "                for step, batch in enumerate(validation_dataloader):\n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    input_features, label_ids = batch\n",
    "                    \n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        input_features = input_features.to(device)\n",
    "                        label_ids = label_ids.to(device)\n",
    "                    \n",
    "                    loss, logits = model(input_features, labels=label_ids)\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = label_ids.to('cpu').numpy()\n",
    "                    outputs = np.argmax(logits, axis=1)\n",
    "                    all_logits.append(outputs)\n",
    "                    all_label_ids.append(label_ids)\n",
    "                    \n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "            all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "            result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "            # print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "            print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])\n",
    "            if result_to_save[\"macro avg\"][\"f1-score\"] > max_score:\n",
    "                max_score = result_to_save[\"macro avg\"][\"f1-score\"]\n",
    "                    \n",
    "        global_step += 1\n",
    "print(\"Best Macro-F1: \", max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
