{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab_mismatch_utils import *\n",
    "from data_formatter_utils import *\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Load modules, mainly huggingface basic model handlers.\n",
    "# Make sure you install huggingface and other packages properly.\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"../huggingface_cache/\" # Not overload common dir \n",
    "                                                           # if run in shared resources.\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import is_main_process, EvaluationStrategy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "font = {'family' : 'Times New Roman',\n",
    "        'size'   : 30}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(inoculation_data_path, eval_data_path=None, test_data_path=None,\n",
    "                inoculation_step_sample_size=1.0, \n",
    "                eval_sample_limit=-1, seed=42):\n",
    "    \"\"\"\n",
    "    eval_data_path is not needed if it is a saved_to_disk \n",
    "    huggingface dataset.\n",
    "    \n",
    "    return type is already a huggingface dataset.\n",
    "    \"\"\"\n",
    "    pd_format = True\n",
    "    if inoculation_data_path.split(\".\")[-1] != \"tsv\":\n",
    "        if len(inoculation_data_path.split(\".\")) > 1:\n",
    "            logger.info(f\"***** Loading pre-loaded datasets from the disk directly! *****\")\n",
    "            pd_format = False\n",
    "            datasets = DatasetDict.load_from_disk(inoculation_data_path)\n",
    "            inoculation_step_sample_size = int(len(datasets[\"train\"]) * inoculation_step_sample_size)\n",
    "            logger.info(f\"***** Inoculation Sample Count: %s *****\"%(inoculation_step_sample_size))\n",
    "            # this may not always start for zero inoculation\n",
    "            datasets[\"train\"] = datasets[\"train\"].shuffle(seed=seed)\n",
    "            inoculation_train_df = datasets[\"train\"].select(range(inoculation_step_sample_size))\n",
    "            eval_df = datasets[\"validation\"]\n",
    "            datasets[\"validation\"] = datasets[\"validation\"].shuffle(seed=seed)\n",
    "            if eval_sample_limit != -1:\n",
    "                datasets[\"validation\"] = datasets[\"validation\"].select(range(eval_sample_limit))\n",
    "        else:\n",
    "            logger.info(f\"***** Loading downloaded huggingface datasets: {inoculation_data_path}! *****\")\n",
    "            pd_format = False\n",
    "            if inoculation_data_path in [\"sst3\", \"cola\", \"mnli\", \"snli\", \"mrps\", \"qnli\"]:\n",
    "                pass\n",
    "            raise NotImplementedError()\n",
    "    else:\n",
    "        train_df = pd.read_csv(inoculation_data_path, delimiter=\"\\t\")\n",
    "        eval_df = pd.read_csv(eval_data_path, delimiter=\"\\t\")\n",
    "        test_df = pd.read_csv(test_data_path, delimiter=\"\\t\")\n",
    "        inoculation_step_sample_size = int(len(train_df) * inoculation_step_sample_size)\n",
    "        logger.info(f\"***** Inoculation Sample Count: %s *****\"%(inoculation_step_sample_size))\n",
    "        # this may not always start for zero inoculation\n",
    "        inoculation_train_df = train_df.sample(n=inoculation_step_sample_size, \n",
    "                                               replace=False, \n",
    "                                               random_state=seed) # seed here could not a little annoying.\n",
    "    if pd_format:\n",
    "        datasets = {}\n",
    "        datasets[\"train\"] = Dataset.from_pandas(inoculation_train_df)\n",
    "        datasets[\"validation\"] = Dataset.from_pandas(eval_df)\n",
    "        datasets[\"test\"] = Dataset.from_pandas(test_df)\n",
    "    else:\n",
    "        datasets = {}\n",
    "        datasets[\"train\"] = inoculation_train_df\n",
    "        datasets[\"validation\"] = eval_df\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_CONFIG = {\n",
    "    \"sst3\": (\"text\", None),\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"snli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\")\n",
    "}\n",
    "# WARNING: you dont need BERT tokenizer\n",
    "# original_vocab = load_bert_vocab(\"../data-files/bert_vocab.txt\")\n",
    "# original_tokenizer = transformers.BertTokenizer(\n",
    "#     vocab_file=\"../data-files/bert_vocab.txt\")\n",
    "# Just use some basic white space tokenizor here!\n",
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "max_length = 128\n",
    "per_device_train_batch_size = 128\n",
    "per_device_eval_batch_size = 128\n",
    "no_cuda = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count() if not no_cuda else 1 # 1 means just on cpu\n",
    "seed = 42\n",
    "lr = 1e-3\n",
    "num_train_epochs = 10\n",
    "task_name = \"sst3\"\n",
    "sentence1_key, sentence2_key = TASK_CONFIG[task_name]\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n",
      "03/17/2021 15:41:03 - INFO - __main__ - ***** Test Sample Count (Verify): 2210 *****\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "datasets = get_dataset(f\"../data-files/{data_file_name}/{data_file_name}-train.tsv\", \n",
    "                       f\"../data-files/{data_file_name}/{data_file_name}-dev.tsv\", \n",
    "                       f\"../data-files/{data_file_name}/{data_file_name}-test.tsv\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "logger.info(f\"***** Test Sample Count (Verify): %s *****\"%(len(datasets[\"test\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:26<00:00, 5916.54it/s]\n",
      "100%|██████████| 1100/1100 [00:00<00:00, 2903.59it/s]\n",
      "100%|██████████| 2210/2210 [00:00<00:00, 2851.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the vocab file\n",
    "vocab_index = 0\n",
    "original_vocab = OrderedDict()\n",
    "if \"train\" in datasets:\n",
    "    for (ex_index, example) in enumerate(tqdm(datasets[\"train\"])):\n",
    "        if sentence2_key is None:\n",
    "            sentence_combined = example[sentence1_key]\n",
    "        else:\n",
    "            sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "        sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "        for token in sentence_tokens:\n",
    "            if token not in original_vocab.keys():\n",
    "                original_vocab[token] = vocab_index\n",
    "                vocab_index += 1\n",
    "train_data_only = False\n",
    "if not train_data_only:\n",
    "    if \"validation\" in datasets:\n",
    "        for (ex_index, example) in enumerate(tqdm(datasets[\"validation\"])):\n",
    "            if sentence2_key is None:\n",
    "                sentence_combined = example[sentence1_key]\n",
    "            else:\n",
    "                sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "            sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "            for token in sentence_tokens:\n",
    "                if token not in original_vocab.keys():\n",
    "                    original_vocab[token] = vocab_index\n",
    "                    vocab_index += 1\n",
    "\n",
    "    if \"test\" in datasets:\n",
    "        for (ex_index, example) in enumerate(tqdm(datasets[\"test\"])):\n",
    "            if sentence2_key is None:\n",
    "                sentence_combined = example[sentence1_key]\n",
    "            else:\n",
    "                sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "            sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "            for token in sentence_tokens:\n",
    "                if token not in original_vocab.keys():\n",
    "                    original_vocab[token] = vocab_index\n",
    "                    vocab_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 572/159274 [00:00<00:55, 2851.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: Surprisingly, considering that Baird is a former film editor, the movie is rather choppy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50479/159274 [00:15<00:26, 4127.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: achronological\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 100487/159274 [00:27<00:14, 4011.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: Show\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 150776/159274 [00:40<00:02, 4030.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: picked me up ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:42<00:00, 3767.60it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# BoW feature vectors for train split\n",
    "train_input_features = []\n",
    "train_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(datasets[\"train\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    if ex_index % 50000 == 0:\n",
    "        print(\"Example sentence: \" + sentence_combined)\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    train_input_features.append(bow_feature)\n",
    "    train_label_ids.append(example[\"label\"])\n",
    "    \n",
    "train_input_features = torch.stack(train_input_features, dim=0)\n",
    "train_input_features = torch.tensor(train_input_features, dtype=torch.float)\n",
    "train_label_ids = torch.tensor(train_label_ids, dtype=torch.long)\n",
    "train_data = TensorDataset(train_input_features, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:00<00:00, 1953.40it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# BoW feature vectors for validation split\n",
    "validation_input_features = []\n",
    "validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        if t in original_vocab.keys():\n",
    "            bow_feature[original_vocab[t]] += 1\n",
    "    validation_input_features.append(bow_feature)\n",
    "    validation_label_ids.append(example[\"label\"])\n",
    "\n",
    "validation_input_features = torch.stack(validation_input_features, dim=0)\n",
    "validation_input_features = torch.tensor(validation_input_features, dtype=torch.float)\n",
    "validation_label_ids = torch.tensor(validation_label_ids, dtype=torch.long)\n",
    "validation_data = TensorDataset(validation_input_features, validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=per_device_train_batch_size*n_gpu)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.classifier = nn.Linear(vocab_size, num_labels, bias=True)\n",
    "    def forward(self, x, labels=None):\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockBERTBOWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(MockBERTBOWClassifier, self).__init__()\n",
    "        hidden_dim = 32\n",
    "        self.mock_bert = nn.Linear(vocab_size, hidden_dim, bias=False)\n",
    "        self.mock_activation = nn.Tanh()\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels, bias=False)\n",
    "    def forward(self, x, labels=None):\n",
    "        cls = self.mock_activation(self.mock_bert(x))\n",
    "        logits = self.classifier(cls)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some overriding fun stuffs!\n",
    "lr = 1e-3\n",
    "num_train_epochs = 10\n",
    "model = BOWClassifier(len(validation_label_ids.unique()), len(original_vocab))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:40 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.37189   0.48832   0.42222       428\n",
      "           1    0.51042   0.11036   0.18148       444\n",
      "           2    0.20362   0.39474   0.26866       228\n",
      "\n",
      "    accuracy                        0.31636      1100\n",
      "   macro avg    0.36197   0.33114   0.29079      1100\n",
      "weighted avg    0.39293   0.31636   0.29322      1100\n",
      "\n",
      "Macro-F1:  0.2907868067072047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:42 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67913   0.50935   0.58211       428\n",
      "           1    0.63620   0.78378   0.70232       444\n",
      "           2    0.33621   0.34211   0.33913       228\n",
      "\n",
      "    accuracy                        0.58545      1100\n",
      "   macro avg    0.55051   0.54508   0.54119      1100\n",
      "weighted avg    0.59072   0.58545   0.58027      1100\n",
      "\n",
      "Macro-F1:  0.541186934026759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:45 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.65316   0.60280   0.62697       428\n",
      "           1    0.66224   0.78604   0.71885       444\n",
      "           2    0.35393   0.27632   0.31034       228\n",
      "\n",
      "    accuracy                        0.60909      1100\n",
      "   macro avg    0.55645   0.55505   0.55206      1100\n",
      "weighted avg    0.59480   0.60909   0.59843      1100\n",
      "\n",
      "Macro-F1:  0.5520552870437704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:47 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66114   0.65187   0.65647       428\n",
      "           1    0.69231   0.79054   0.73817       444\n",
      "           2    0.36257   0.27193   0.31078       228\n",
      "\n",
      "    accuracy                        0.62909      1100\n",
      "   macro avg    0.57201   0.57145   0.56847      1100\n",
      "weighted avg    0.61183   0.62909   0.61779      1100\n",
      "\n",
      "Macro-F1:  0.5684726258647795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:49 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67143   0.65888   0.66509       428\n",
      "           1    0.69583   0.78829   0.73918       444\n",
      "           2    0.36158   0.28070   0.31605       228\n",
      "\n",
      "    accuracy                        0.63273      1100\n",
      "   macro avg    0.57628   0.57596   0.57344      1100\n",
      "weighted avg    0.61705   0.63273   0.62265      1100\n",
      "\n",
      "Macro-F1:  0.5734400228985358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:52 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67734   0.64252   0.65947       428\n",
      "           1    0.68750   0.79279   0.73640       444\n",
      "           2    0.36264   0.28947   0.32195       228\n",
      "\n",
      "    accuracy                        0.63000      1100\n",
      "   macro avg    0.57583   0.57493   0.57261      1100\n",
      "weighted avg    0.61621   0.63000   0.62056      1100\n",
      "\n",
      "Macro-F1:  0.5726084384049042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:54 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67619   0.66355   0.66981       428\n",
      "           1    0.69643   0.79054   0.74051       444\n",
      "           2    0.35227   0.27193   0.30693       228\n",
      "\n",
      "    accuracy                        0.63364      1100\n",
      "   macro avg    0.57496   0.57534   0.57242      1100\n",
      "weighted avg    0.61722   0.63364   0.62313      1100\n",
      "\n",
      "Macro-F1:  0.5724161143126493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:56 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68780   0.65888   0.67303       428\n",
      "           1    0.69685   0.79730   0.74370       444\n",
      "           2    0.37363   0.29825   0.33171       228\n",
      "\n",
      "    accuracy                        0.64000      1100\n",
      "   macro avg    0.58609   0.58481   0.58281      1100\n",
      "weighted avg    0.62634   0.64000   0.63081      1100\n",
      "\n",
      "Macro-F1:  0.5828119407725835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:46:58 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68841   0.66589   0.67696       428\n",
      "           1    0.69591   0.80405   0.74608       444\n",
      "           2    0.39306   0.29825   0.33915       228\n",
      "\n",
      "    accuracy                        0.64545      1100\n",
      "   macro avg    0.59246   0.58940   0.58740      1100\n",
      "weighted avg    0.63022   0.64545   0.63484      1100\n",
      "\n",
      "Macro-F1:  0.5873977481184788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:01 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68720   0.67757   0.68235       428\n",
      "           1    0.70217   0.80180   0.74869       444\n",
      "           2    0.40351   0.30263   0.34586       228\n",
      "\n",
      "    accuracy                        0.65000      1100\n",
      "   macro avg    0.59763   0.59400   0.59230      1100\n",
      "weighted avg    0.63444   0.65000   0.63938      1100\n",
      "\n",
      "Macro-F1:  0.5923010656473559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:03 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70250   0.65654   0.67874       428\n",
      "           1    0.69844   0.80856   0.74948       444\n",
      "           2    0.39785   0.32456   0.35749       228\n",
      "\n",
      "    accuracy                        0.64909      1100\n",
      "   macro avg    0.59960   0.59655   0.59524      1100\n",
      "weighted avg    0.63772   0.64909   0.64071      1100\n",
      "\n",
      "Macro-F1:  0.5952366544633042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:06 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70647   0.66355   0.68434       428\n",
      "           1    0.70276   0.80405   0.75000       444\n",
      "           2    0.41053   0.34211   0.37321       228\n",
      "\n",
      "    accuracy                        0.65364      1100\n",
      "   macro avg    0.60658   0.60324   0.60251      1100\n",
      "weighted avg    0.64363   0.65364   0.64635      1100\n",
      "\n",
      "Macro-F1:  0.6025143636747948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:08 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71287   0.67290   0.69231       428\n",
      "           1    0.70238   0.79730   0.74684       444\n",
      "           2    0.40104   0.33772   0.36667       228\n",
      "\n",
      "    accuracy                        0.65364      1100\n",
      "   macro avg    0.60543   0.60264   0.60194      1100\n",
      "weighted avg    0.64400   0.65364   0.64682      1100\n",
      "\n",
      "Macro-F1:  0.6019366006707779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:10 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71679   0.66822   0.69166       428\n",
      "           1    0.69960   0.79730   0.74526       444\n",
      "           2    0.39487   0.33772   0.36407       228\n",
      "\n",
      "    accuracy                        0.65182      1100\n",
      "   macro avg    0.60376   0.60108   0.60033      1100\n",
      "weighted avg    0.64313   0.65182   0.64539      1100\n",
      "\n",
      "Macro-F1:  0.6003286472776027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:13 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71429   0.64252   0.67651       428\n",
      "           1    0.69396   0.80180   0.74399       444\n",
      "           2    0.39109   0.34649   0.36744       228\n",
      "\n",
      "    accuracy                        0.64545      1100\n",
      "   macro avg    0.59978   0.59694   0.59598      1100\n",
      "weighted avg    0.63909   0.64545   0.63969      1100\n",
      "\n",
      "Macro-F1:  0.595980088692044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:15 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70951   0.64486   0.67564       428\n",
      "           1    0.70400   0.79279   0.74576       444\n",
      "           2    0.37441   0.34649   0.35991       228\n",
      "\n",
      "    accuracy                        0.64273      1100\n",
      "   macro avg    0.59597   0.59471   0.59377      1100\n",
      "weighted avg    0.63783   0.64273   0.63850      1100\n",
      "\n",
      "Macro-F1:  0.5937713968501757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:17 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71538   0.65187   0.68215       428\n",
      "           1    0.69201   0.79955   0.74190       444\n",
      "           2    0.38579   0.33333   0.35765       228\n",
      "\n",
      "    accuracy                        0.64545      1100\n",
      "   macro avg    0.59773   0.59492   0.59390      1100\n",
      "weighted avg    0.63763   0.64545   0.63901      1100\n",
      "\n",
      "Macro-F1:  0.5939001414833728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:20 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71354   0.64019   0.67488       428\n",
      "           1    0.69841   0.79279   0.74262       444\n",
      "           2    0.37736   0.35088   0.36364       228\n",
      "\n",
      "    accuracy                        0.64182      1100\n",
      "   macro avg    0.59644   0.59462   0.59371      1100\n",
      "weighted avg    0.63775   0.64182   0.63771      1100\n",
      "\n",
      "Macro-F1:  0.5937097482274262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:22 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70025   0.66589   0.68263       428\n",
      "           1    0.70200   0.79054   0.74364       444\n",
      "           2    0.38342   0.32456   0.35154       228\n",
      "\n",
      "    accuracy                        0.64545      1100\n",
      "   macro avg    0.59522   0.59366   0.59261      1100\n",
      "weighted avg    0.63528   0.64545   0.63863      1100\n",
      "\n",
      "Macro-F1:  0.5926075804428022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:25 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70558   0.64953   0.67640       428\n",
      "           1    0.70281   0.78829   0.74310       444\n",
      "           2    0.37019   0.33772   0.35321       228\n",
      "\n",
      "    accuracy                        0.64091      1100\n",
      "   macro avg    0.59286   0.59185   0.59090      1100\n",
      "weighted avg    0.63495   0.64091   0.63633      1100\n",
      "\n",
      "Macro-F1:  0.590903274541359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:27 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70927   0.66121   0.68440       428\n",
      "           1    0.69739   0.78378   0.73807       444\n",
      "           2    0.38119   0.33772   0.35814       228\n",
      "\n",
      "    accuracy                        0.64364      1100\n",
      "   macro avg    0.59595   0.59424   0.59354      1100\n",
      "weighted avg    0.63648   0.64364   0.63844      1100\n",
      "\n",
      "Macro-F1:  0.5935369917690262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:30 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71684   0.65654   0.68537       428\n",
      "           1    0.70200   0.79054   0.74364       444\n",
      "           2    0.37981   0.34649   0.36239       228\n",
      "\n",
      "    accuracy                        0.64636      1100\n",
      "   macro avg    0.59955   0.59786   0.59713      1100\n",
      "weighted avg    0.64099   0.64636   0.64194      1100\n",
      "\n",
      "Macro-F1:  0.597131747518688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:32 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72280   0.65187   0.68550       428\n",
      "           1    0.70120   0.79279   0.74419       444\n",
      "           2    0.37736   0.35088   0.36364       228\n",
      "\n",
      "    accuracy                        0.64636      1100\n",
      "   macro avg    0.60045   0.59851   0.59778      1100\n",
      "weighted avg    0.64248   0.64636   0.64248      1100\n",
      "\n",
      "Macro-F1:  0.5977753652172257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:34 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72487   0.64019   0.67990       428\n",
      "           1    0.70868   0.77252   0.73922       444\n",
      "           2    0.36975   0.38596   0.37768       228\n",
      "\n",
      "    accuracy                        0.64091      1100\n",
      "   macro avg    0.60110   0.59956   0.59894      1100\n",
      "weighted avg    0.64473   0.64091   0.64120      1100\n",
      "\n",
      "Macro-F1:  0.5989357619271282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:47:37 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72040   0.66822   0.69333       428\n",
      "           1    0.70363   0.78604   0.74255       444\n",
      "           2    0.38164   0.34649   0.36322       228\n",
      "\n",
      "    accuracy                        0.64909      1100\n",
      "   macro avg    0.60189   0.60025   0.59970      1100\n",
      "weighted avg    0.64342   0.64909   0.64478      1100\n",
      "\n",
      "Macro-F1:  0.599701638542431\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for _ in range(int(num_train_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    # pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, _ = model(input_features, labels=label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # pbar.set_postfix({'train_loss': loss.tolist()})\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            logger.info(\"***** Evaluation Interval Hit *****\")\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_label_ids = []\n",
    "            with torch.no_grad():\n",
    "                # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "                for step, batch in enumerate(validation_dataloader):\n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    input_features, label_ids = batch\n",
    "                    \n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        input_features = input_features.to(device)\n",
    "                        label_ids = label_ids.to(device)\n",
    "                    \n",
    "                    loss, logits = model(input_features, labels=label_ids)\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = label_ids.to('cpu').numpy()\n",
    "                    outputs = np.argmax(logits, axis=1)\n",
    "                    all_logits.append(outputs)\n",
    "                    all_label_ids.append(label_ids)\n",
    "                    \n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "            all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "            result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "            print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "            print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])\n",
    "                    \n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations with frequency-matched scrambling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:43:20 - INFO - __main__ - ***** Loading pre-loaded datasets from the disk directly! *****\n",
      "03/18/2021 02:43:20 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "Loading cached shuffled indices for dataset at ../data-files/sst-tenary-corrupted-S2/train/cache-3726aee8a4df65db.arrow\n",
      "Loading cached shuffled indices for dataset at ../data-files/sst-tenary-corrupted-S2/validation/cache-a383bac85d9577d4.arrow\n",
      "03/18/2021 02:43:20 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/18/2021 02:43:20 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n",
      "100%|██████████| 1100/1100 [00:00<00:00, 1359.52it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "03/18/2021 02:43:21 - INFO - __main__ - ***** Evaluation With Corrupt Data *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.33585   0.20794   0.25685       428\n",
      "           1    0.37057   0.30631   0.33539       444\n",
      "           2    0.19872   0.40789   0.26724       228\n",
      "\n",
      "    accuracy                        0.28909      1100\n",
      "   macro avg    0.30171   0.30738   0.28649      1100\n",
      "weighted avg    0.32144   0.28909   0.29071      1100\n",
      "\n",
      "Macro-F1:  0.28649468184524945\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "corrupt_method = \"S2\"\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "corrupt_datasets = get_dataset(f\"../data-files/{data_file_name}-corrupted-{corrupt_method}\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "\n",
    "corrupt_validation_input_features = []\n",
    "corrupt_validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(corrupt_datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    corrupt_validation_input_features.append(bow_feature)\n",
    "    corrupt_validation_label_ids.append(example[\"label\"])\n",
    "    \n",
    "corrupt_validation_input_features = torch.stack(corrupt_validation_input_features, dim=0)\n",
    "corrupt_validation_input_features = torch.tensor(corrupt_validation_input_features, dtype=torch.float)\n",
    "corrupt_validation_label_ids = torch.tensor(corrupt_validation_label_ids, dtype=torch.long)\n",
    "corrupt_validation_data = TensorDataset(corrupt_validation_input_features, corrupt_validation_label_ids)\n",
    "corrupt_validation_dataloader = DataLoader(corrupt_validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)\n",
    "\n",
    "logger.info(\"***** Evaluation With Corrupt Data *****\")\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_label_ids = []\n",
    "with torch.no_grad():\n",
    "    # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(corrupt_validation_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, logits = model(input_features, labels=label_ids)\n",
    "        logits = F.softmax(logits, dim=-1)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        all_logits.append(outputs)\n",
    "        all_label_ids.append(label_ids)\n",
    "\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluations with frequency-unmatched scrambling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/18/2021 02:43:14 - INFO - __main__ - ***** Loading pre-loaded datasets from the disk directly! *****\n",
      "03/18/2021 02:43:14 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "Loading cached shuffled indices for dataset at ../data-files/sst-tenary-corrupted-S3/train/cache-cd54032ebd6257b5.arrow\n",
      "Loading cached shuffled indices for dataset at ../data-files/sst-tenary-corrupted-S3/validation/cache-4581ae0bb7454f12.arrow\n",
      "03/18/2021 02:43:15 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/18/2021 02:43:15 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n",
      "100%|██████████| 1100/1100 [00:01<00:00, 1088.40it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "03/18/2021 02:43:16 - INFO - __main__ - ***** Evaluation With Corrupt Data *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.56250   0.02103   0.04054       428\n",
      "           1    0.30435   0.03153   0.05714       444\n",
      "           2    0.20520   0.93421   0.33649       228\n",
      "\n",
      "    accuracy                        0.21455      1100\n",
      "   macro avg    0.35735   0.32892   0.14473      1100\n",
      "weighted avg    0.38424   0.21455   0.10858      1100\n",
      "\n",
      "Macro-F1:  0.14472542955955278\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "corrupt_method = \"S3\"\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "corrupt_datasets = get_dataset(f\"../data-files/{data_file_name}-corrupted-{corrupt_method}\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "\n",
    "corrupt_validation_input_features = []\n",
    "corrupt_validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(corrupt_datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    corrupt_validation_input_features.append(bow_feature)\n",
    "    corrupt_validation_label_ids.append(example[\"label\"])\n",
    "    \n",
    "corrupt_validation_input_features = torch.stack(corrupt_validation_input_features, dim=0)\n",
    "corrupt_validation_input_features = torch.tensor(corrupt_validation_input_features, dtype=torch.float)\n",
    "corrupt_validation_label_ids = torch.tensor(corrupt_validation_label_ids, dtype=torch.long)\n",
    "corrupt_validation_data = TensorDataset(corrupt_validation_input_features, corrupt_validation_label_ids)\n",
    "corrupt_validation_dataloader = DataLoader(corrupt_validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)\n",
    "\n",
    "logger.info(\"***** Evaluation With Corrupt Data *****\")\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_label_ids = []\n",
    "with torch.no_grad():\n",
    "    # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(corrupt_validation_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, logits = model(input_features, labels=label_ids)\n",
    "        logits = F.softmax(logits, dim=-1)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        all_logits.append(outputs)\n",
    "        all_label_ids.append(label_ids)\n",
    "\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random guessing baseline\n",
    "If we randomly guess the lables, what is the performance now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.38519   0.36449   0.37455       428\n",
      "           1    0.40088   0.40991   0.40535       444\n",
      "           2    0.20747   0.21930   0.21322       228\n",
      "\n",
      "    accuracy                        0.35273      1100\n",
      "   macro avg    0.33118   0.33123   0.33104      1100\n",
      "weighted avg    0.35468   0.35273   0.35354      1100\n",
      "\n",
      "AVG over 100 runs mF1: 0.331816.\n"
     ]
    }
   ],
   "source": [
    "# getting avg mF1 on the dataset with a dummy classifier\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "mf1s = []\n",
    "runs = 100\n",
    "for i in range(runs):\n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "    dummy_clf.fit(validation_input_features, validation_label_ids)\n",
    "    dummy_labels = dummy_clf.predict(validation_input_features)\n",
    "\n",
    "    # dummy performance\n",
    "    # print(classification_report(validation_label_ids, dummy_labels, digits=5))\n",
    "    result_to_save = classification_report(validation_label_ids, dummy_labels, digits=5, output_dict=True)\n",
    "    mf1s += [result_to_save[\"macro avg\"][\"f1-score\"]]\n",
    "print(classification_report(validation_label_ids, dummy_labels, digits=5))\n",
    "print(f\"AVG over {runs} runs mF1: {round(sum(mf1s)/len(mf1s), 6)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FrequencyBoW classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task setups\n",
    "task_name = \"sst3\"\n",
    "num_labels = 3\n",
    "FILENAME_CONFIG = {\n",
    "    \"sst3\" : \"sst-tenary\"\n",
    "}\n",
    "\n",
    "# let us corrupt SST3 in the same way as before\n",
    "train_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                    f\"{FILENAME_CONFIG[task_name]}-train.tsv\"), \n",
    "                       delimiter=\"\\t\")\n",
    "eval_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-dev.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(os.path.join(external_output_dirname, FILENAME_CONFIG[task_name], \n",
    "                                   f\"{FILENAME_CONFIG[task_name]}-test.tsv\"), \n",
    "                      delimiter=\"\\t\")\n",
    "\n",
    "train_df = Dataset.from_pandas(train_df)\n",
    "eval_df = Dataset.from_pandas(eval_df)\n",
    "test_df = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing #10000 example...\n",
      "processing #20000 example...\n",
      "processing #30000 example...\n",
      "processing #40000 example...\n",
      "processing #50000 example...\n",
      "processing #60000 example...\n",
      "processing #70000 example...\n",
      "processing #80000 example...\n",
      "processing #90000 example...\n",
      "processing #100000 example...\n",
      "processing #110000 example...\n",
      "processing #120000 example...\n",
      "processing #130000 example...\n",
      "processing #140000 example...\n",
      "processing #150000 example...\n"
     ]
    }
   ],
   "source": [
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "label_vocab_map = {}\n",
    "token_frequency_map = {} # overwrite this everytime for a new dataset\n",
    "for i, example in enumerate(train_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        if label not in label_vocab_map.keys():\n",
    "            label_vocab_map[label] = tokens\n",
    "        else:\n",
    "            for t in tokens:\n",
    "                label_vocab_map[label].append(t)\n",
    "        for t in tokens:\n",
    "            if t in token_frequency_map.keys():\n",
    "                token_frequency_map[t] = token_frequency_map[t] + 1\n",
    "            else:\n",
    "                token_frequency_map[t] = 1\n",
    "for i, example in enumerate(eval_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        if label not in label_vocab_map.keys():\n",
    "            label_vocab_map[label] = tokens\n",
    "        else:\n",
    "            for t in tokens:\n",
    "                label_vocab_map[label].append(t)\n",
    "        for t in tokens:\n",
    "            if t in token_frequency_map.keys():\n",
    "                token_frequency_map[t] = token_frequency_map[t] + 1\n",
    "            else:\n",
    "                token_frequency_map[t] = 1\n",
    "for i, example in enumerate(test_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        if label not in label_vocab_map.keys():\n",
    "            label_vocab_map[label] = tokens\n",
    "        else:\n",
    "            for t in tokens:\n",
    "                label_vocab_map[label].append(t)\n",
    "        for t in tokens:\n",
    "            if t in token_frequency_map.keys():\n",
    "                token_frequency_map[t] = token_frequency_map[t] + 1\n",
    "            else:\n",
    "                token_frequency_map[t] = 1\n",
    "task_token_frequency_map = sorted(token_frequency_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "task_token_frequency_map = OrderedDict(task_token_frequency_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training BoW with 1st order frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq and bucket mappings\n",
    "freq_set = set([])\n",
    "for k, v in task_token_frequency_map.items():\n",
    "    freq_set.add(v)\n",
    "freq_set = list(freq_set)\n",
    "freq_set.sort()\n",
    "bucket_count = 256\n",
    "freq_bucket = np.logspace(math.log(freq_set[0], 10), math.log(freq_set[-1], 10), bucket_count, endpoint=True)\n",
    "freq_bucket = freq_bucket[:-1]\n",
    "freq_bucket = [math.ceil(n) for n in freq_bucket]\n",
    "# finally the bucket is a map between freq and bucket number\n",
    "def find_bucket_number(freq, freq_bucket):\n",
    "    for i in range(len(freq_bucket)):\n",
    "        if freq > freq_bucket[i]:\n",
    "            continue\n",
    "        else:\n",
    "            return i+1\n",
    "    return len(freq_bucket)\n",
    "\n",
    "new_bucket_idx = 0\n",
    "freq_bucket_map = {}\n",
    "for freq in freq_set:\n",
    "    # bucket_num = find_bucket_number(freq, freq_bucket)\n",
    "    freq_bucket_map[freq] = new_bucket_idx\n",
    "    new_bucket_idx += 1\n",
    "\n",
    "bucket_length = new_bucket_idx # len(freq_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 611/159274 [00:00<00:53, 2963.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: This is one of the year's best films.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50484/159274 [00:16<00:35, 3065.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: there is plenty of room for editing, and a much shorter cut surely would have resulted in a smoother, more focused narrative without sacrificing any of the cultural intrigue\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 2., 2., 0., 2., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 100566/159274 [00:31<00:13, 4436.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: his name was, uh, Michael Zaidan, was supposed to have like written the screenplay or something\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 2., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 150612/159274 [00:43<00:02, 4325.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: They just don't work in concert.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:45<00:00, 3508.71it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for train split\n",
    "train_input_features = []\n",
    "train_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(train_df)):\n",
    "    bow_feature = torch.zeros(bucket_length)\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[freq_bucket_map[token_frequency_map[t]]] += 1 # bucket count\n",
    "    if ex_index % 50000 == 0:\n",
    "        print(\"Example sentence: \" + sentence_combined)\n",
    "        print(bow_feature)\n",
    "    train_input_features.append(bow_feature)\n",
    "    train_label_ids.append(example[\"label\"])\n",
    "    \n",
    "train_input_features = torch.stack(train_input_features, dim=0)\n",
    "train_input_features = torch.tensor(train_input_features, dtype=torch.float)\n",
    "train_label_ids = torch.tensor(train_label_ids, dtype=torch.long)\n",
    "train_data = TensorDataset(train_input_features, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:00<00:00, 2071.74it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for validation split\n",
    "validation_input_features = []\n",
    "validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(eval_df)):\n",
    "    bow_feature = torch.zeros(bucket_length)\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[freq_bucket_map[token_frequency_map[t]]] += 1 # bucket count\n",
    "    validation_input_features.append(bow_feature)\n",
    "    validation_label_ids.append(example[\"label\"])\n",
    "\n",
    "validation_input_features = torch.stack(validation_input_features, dim=0)\n",
    "validation_input_features = torch.tensor(validation_input_features, dtype=torch.float)\n",
    "validation_label_ids = torch.tensor(validation_label_ids, dtype=torch.long)\n",
    "validation_data = TensorDataset(validation_input_features, validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=per_device_train_batch_size*n_gpu)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some overriding fun stuffs!\n",
    "lr = 1e-3\n",
    "num_train_epochs = 20\n",
    "model = BOWClassifier(len(validation_label_ids.unique()), bucket_length)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.29362903858075845\n",
      "Macro-F1:  0.4815528782310475\n",
      "Macro-F1:  0.505620890184787\n",
      "Macro-F1:  0.5055478521616278\n",
      "Macro-F1:  0.5158001378977433\n",
      "Macro-F1:  0.5153792389952998\n",
      "Macro-F1:  0.5238301864251756\n",
      "Macro-F1:  0.5163669868353168\n",
      "Macro-F1:  0.5132942135306541\n",
      "Macro-F1:  0.5161602720484956\n",
      "Macro-F1:  0.5219554069668625\n",
      "Macro-F1:  0.5283716932135551\n",
      "Macro-F1:  0.5217954766846714\n",
      "Macro-F1:  0.5182836617860059\n",
      "Macro-F1:  0.5201747288124605\n",
      "Macro-F1:  0.5246612808098992\n",
      "Macro-F1:  0.5194165512632037\n",
      "Macro-F1:  0.5180882931234276\n",
      "Macro-F1:  0.5197036593727948\n",
      "Macro-F1:  0.5228242647642586\n",
      "Macro-F1:  0.5251983411497888\n",
      "Macro-F1:  0.516990662176519\n",
      "Macro-F1:  0.5166227877916107\n",
      "Macro-F1:  0.5219430344564765\n",
      "Macro-F1:  0.5201008589034574\n",
      "Macro-F1:  0.5265580483728647\n",
      "Macro-F1:  0.5194328847335722\n",
      "Macro-F1:  0.5227367621249202\n",
      "Macro-F1:  0.515849535539506\n",
      "Macro-F1:  0.5192073275478656\n",
      "Macro-F1:  0.5241944157222207\n",
      "Macro-F1:  0.5199166068415538\n",
      "Macro-F1:  0.5201548005406526\n",
      "Macro-F1:  0.5179335683684317\n",
      "Macro-F1:  0.5223764951962291\n",
      "Macro-F1:  0.521198101587783\n",
      "Macro-F1:  0.5210438243636987\n",
      "Macro-F1:  0.5210490327832514\n",
      "Macro-F1:  0.5256212701754023\n",
      "Macro-F1:  0.5245855951549342\n",
      "Macro-F1:  0.5151990068960353\n",
      "Macro-F1:  0.5228873688211207\n",
      "Macro-F1:  0.5159533873528664\n",
      "Macro-F1:  0.5244900508738645\n",
      "Macro-F1:  0.5228356644659079\n",
      "Macro-F1:  0.5155175237570764\n",
      "Macro-F1:  0.5168043962103798\n",
      "Macro-F1:  0.5217510479962388\n",
      "Macro-F1:  0.5200652048421532\n",
      "Macro-F1:  0.5211362240270331\n",
      "Best Macro-F1:  0.5283716932135551\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "max_score = -1\n",
    "for _ in range(int(num_train_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    # pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, _ = model(input_features, labels=label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # pbar.set_postfix({'train_loss': loss.tolist()})\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            # logger.info(\"***** Evaluation Interval Hit *****\")\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_label_ids = []\n",
    "            with torch.no_grad():\n",
    "                # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "                for step, batch in enumerate(validation_dataloader):\n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    input_features, label_ids = batch\n",
    "                    \n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        input_features = input_features.to(device)\n",
    "                        label_ids = label_ids.to(device)\n",
    "                    \n",
    "                    loss, logits = model(input_features, labels=label_ids)\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = label_ids.to('cpu').numpy()\n",
    "                    outputs = np.argmax(logits, axis=1)\n",
    "                    all_logits.append(outputs)\n",
    "                    all_label_ids.append(label_ids)\n",
    "                    \n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "            all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "            result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "            # print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "            print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])\n",
    "            if result_to_save[\"macro avg\"][\"f1-score\"] > max_score:\n",
    "                max_score = result_to_save[\"macro avg\"][\"f1-score\"]\n",
    "                    \n",
    "        global_step += 1\n",
    "print(\"Best Macro-F1: \", max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training BoW with 1st and 2nd order frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition the first order information\n",
    "second_order_freq_set = set([])\n",
    "for k, v in task_token_frequency_map.items():\n",
    "    second_order_freq_set.add(v)\n",
    "second_order_freq_set = list(second_order_freq_set)\n",
    "second_order_freq_set.sort()\n",
    "temp_bucket_count = 24\n",
    "second_order_freq_bucket = np.logspace(math.log(second_order_freq_set[0], 10), \n",
    "                          math.log(second_order_freq_set[-1], 10), temp_bucket_count+1, \n",
    "                          endpoint=True)\n",
    "second_order_freq_bucket = second_order_freq_bucket[:-1]\n",
    "second_order_freq_bucket = [math.ceil(n) for n in second_order_freq_bucket]\n",
    "# finally the bucket is a map between freq and bucket number\n",
    "def find_bucket_number(freq, freq_bucket):\n",
    "    for i in range(len(freq_bucket)):\n",
    "        if freq > freq_bucket[i]:\n",
    "            continue\n",
    "        else:\n",
    "            return i+1\n",
    "    return len(freq_bucket)\n",
    "\n",
    "second_order_freq_bucket_map = {}\n",
    "for freq in second_order_freq_set:\n",
    "    bucket_num = find_bucket_number(freq, second_order_freq_bucket)\n",
    "    second_order_freq_bucket_map[freq] = bucket_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing #10000 example...\n",
      "processing #20000 example...\n",
      "processing #30000 example...\n",
      "processing #40000 example...\n",
      "processing #50000 example...\n",
      "processing #60000 example...\n",
      "processing #70000 example...\n",
      "processing #80000 example...\n",
      "processing #90000 example...\n",
      "processing #100000 example...\n",
      "processing #110000 example...\n",
      "processing #120000 example...\n",
      "processing #130000 example...\n",
      "processing #140000 example...\n",
      "processing #150000 example...\n"
     ]
    }
   ],
   "source": [
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "token_freq_freq_map = {} # overwrite this everytime for a new dataset\n",
    "for i, example in enumerate(train_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        for i in range(len(tokens)-1):\n",
    "            for j in range(i+1, len(tokens)):\n",
    "                t1 = tokens[i]\n",
    "                t2 = tokens[j]\n",
    "                index_tuple = [second_order_freq_bucket_map[token_frequency_map[t1]], \n",
    "                               second_order_freq_bucket_map[token_frequency_map[t2]]]\n",
    "                index_tuple.sort()\n",
    "                index_tuple = tuple(index_tuple)\n",
    "                if index_tuple in token_freq_freq_map.keys():\n",
    "                    token_freq_freq_map[index_tuple] += 1\n",
    "                else:\n",
    "                    token_freq_freq_map[index_tuple] = 1\n",
    "                    \n",
    "for i, example in enumerate(eval_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        for i in range(len(tokens)-1):\n",
    "            for j in range(i+1, len(tokens)):\n",
    "                t1 = tokens[i]\n",
    "                t2 = tokens[j]\n",
    "                index_tuple = [second_order_freq_bucket_map[token_frequency_map[t1]], \n",
    "                               second_order_freq_bucket_map[token_frequency_map[t2]]]\n",
    "                index_tuple.sort()\n",
    "                index_tuple = tuple(index_tuple)\n",
    "                if index_tuple in token_freq_freq_map.keys():\n",
    "                    token_freq_freq_map[index_tuple] += 1\n",
    "                else:\n",
    "                    token_freq_freq_map[index_tuple] = 1\n",
    "                    \n",
    "for i, example in enumerate(test_df):\n",
    "    if i % 10000 == 0 and i != 0:\n",
    "        print(f\"processing #{i} example...\")\n",
    "    original_sentence = example['text']\n",
    "    label = example['label']\n",
    "    if len(original_sentence.strip()) != 0:\n",
    "        tokens = modified_basic_tokenizer.tokenize(original_sentence)\n",
    "        for i in range(len(tokens)-1):\n",
    "            for j in range(i+1, len(tokens)):\n",
    "                t1 = tokens[i]\n",
    "                t2 = tokens[j]\n",
    "                index_tuple = [second_order_freq_bucket_map[token_frequency_map[t1]], \n",
    "                               second_order_freq_bucket_map[token_frequency_map[t2]]]\n",
    "                index_tuple.sort()\n",
    "                index_tuple = tuple(index_tuple)\n",
    "                if index_tuple in token_freq_freq_map.keys():\n",
    "                    token_freq_freq_map[index_tuple] += 1\n",
    "                else:\n",
    "                    token_freq_freq_map[index_tuple] = 1\n",
    "                    \n",
    "task_token_freq_freq_map = sorted(token_freq_freq_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "task_token_freq_freq_map = OrderedDict(task_token_freq_freq_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition the first order information\n",
    "second_order_freq_freq_set = set([])\n",
    "for k, v in task_token_freq_freq_map.items():\n",
    "    second_order_freq_freq_set.add(v)\n",
    "second_order_freq_freq_set = list(second_order_freq_freq_set)\n",
    "second_order_freq_freq_set.sort()\n",
    "# second_order_freq_freq_set = second_order_freq_freq_set[::-1]\n",
    "# bucket_count = 48\n",
    "# second_order_freq_freq_bucket = np.logspace(0, \n",
    "#                           math.log(len(second_order_freq_freq_set), 10), bucket_count, \n",
    "#                           endpoint=True)\n",
    "# second_order_freq_freq_bucket = second_order_freq_freq_bucket[:-1]\n",
    "# second_order_freq_freq_bucket = [math.ceil(n) for n in second_order_freq_freq_bucket]\n",
    "# for i in range(1, len(second_order_freq_freq_bucket)):\n",
    "#     if second_order_freq_freq_bucket[i] == second_order_freq_freq_bucket[i-1]:\n",
    "#         second_order_freq_freq_bucket[i] += 1\n",
    "# second_order_freq_freq_bucket += [len(second_order_freq_freq_set)]\n",
    "# start = 0\n",
    "# bucket_count = 0\n",
    "# second_order_freq_freq_bucket_map = {}\n",
    "# for i in range(len(second_order_freq_freq_bucket)):\n",
    "#     end = second_order_freq_freq_bucket[i]\n",
    "#     bucket_freqs = second_order_freq_freq_set[start:second_order_freq_freq_bucket[i]]\n",
    "#     for freq in bucket_freqs:\n",
    "#         second_order_freq_freq_bucket_map[freq] = bucket_count+1\n",
    "#     bucket_count += 1\n",
    "#     start = second_order_freq_freq_bucket[i]\n",
    "second_order_freq_freq_bucket_map = {}\n",
    "new_bucket_idx = 0\n",
    "freq_bucket_map = {}\n",
    "for freq in second_order_freq_freq_set:\n",
    "    # bucket_num = find_bucket_number(freq, freq_bucket)\n",
    "    second_order_freq_freq_bucket_map[freq] = new_bucket_idx\n",
    "    new_bucket_idx += 1\n",
    "\n",
    "bucket_length = new_bucket_idx # len(freq_bucket)\n",
    "# the code above create second order buckets, now we can create second order BoW vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 223/159274 [00:00<02:19, 1144.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: This is one of the year's best films.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 2., 0., 0., 1., 0., 0., 0., 0., 2., 0., 1., 0., 0., 0.,\n",
      "        1., 2., 0., 4., 0., 0., 0., 0., 4., 0., 4., 0., 0., 0., 0., 0., 4., 8.,\n",
      "        0., 4., 6.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50226/159274 [00:45<01:29, 1215.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: there is plenty of room for editing, and a much shorter cut surely would have resulted in a smoother, more focused narrative without sacrificing any of the cultural intrigue\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
      "         1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  2.,  6.,  1.,  0.,  2.,  2.,\n",
      "         2.,  1.,  1.,  1.,  2.,  0.,  0.,  2.,  2.,  1.,  0.,  0.,  0.,  1.,\n",
      "         2.,  1.,  2.,  4.,  1.,  5.,  1.,  1.,  0.,  4.,  1.,  0.,  4.,  1.,\n",
      "         2.,  2.,  2.,  1.,  0.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  0.,\n",
      "         2.,  0.,  0.,  1.,  1.,  8.,  4.,  2.,  2.,  1.,  4.,  2.,  2.,  4.,\n",
      "         0.,  2.,  2.,  1.,  0.,  1.,  2.,  8.,  2.,  1.,  4.,  0.,  0.,  0.,\n",
      "         1.,  0.,  4.,  0.,  1.,  8.,  0.,  0.,  0.,  0.,  0.,  4.,  2.,  2.,\n",
      "         0.,  0.,  2.,  1.,  4.,  0.,  4.,  1.,  2.,  4.,  2.,  4.,  0.,  2.,\n",
      "         2.,  0.,  2.,  1.,  4.,  2.,  0.,  4.,  8.,  1.,  0.,  1.,  2.,  0.,\n",
      "         0.,  4.,  2.,  0.,  2.,  2.,  8.,  1.,  4.,  2.,  0.,  0.,  2.,  4.,\n",
      "         1.,  1.,  2.,  2.,  1.,  2.,  0.,  8.,  8.,  8.,  8., 16.,  8., 16.,\n",
      "         8.,  8., 16.,  0., 32.,  0.,  8., 16., 16.,  8., 28.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 100247/159274 [01:27<00:47, 1247.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: his name was, uh, Michael Zaidan, was supposed to have like written the screenplay or something\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  2.,  0.,\n",
      "         0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  2.,  2.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  4.,\n",
      "         3.,  2.,  0.,  2.,  3.,  2.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.,  4.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  2.,  2.,  0.,  6.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.,  1.,  0.,  2.,  0.,\n",
      "         0.,  0.,  0.,  6.,  0.,  3.,  6.,  0.,  0.,  0.,  0.,  4.,  0.,  4.,\n",
      "         0.,  2.,  0.,  0.,  0.,  6.,  0.,  0.,  0.,  5., 10.,  5.,  0.,  0.,\n",
      "         0.,  5.,  0., 10., 10.,  0.,  0., 15.,  0., 10., 10.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 150178/159274 [02:12<00:07, 1167.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: They just don't work in concert.\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 2., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 2., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 2., 0., 0., 0., 1., 0.,\n",
      "        1., 2., 0., 2., 0., 0., 0., 0., 0., 2., 0., 0., 2., 0., 0., 0., 2., 4.,\n",
      "        0., 2., 1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [02:22<00:00, 1119.18it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for train split (2nd order = 1st order concat with 2nd order)\n",
    "train_input_features = []\n",
    "train_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(train_df)):\n",
    "    bow_feature = torch.zeros(bucket_length) # up-to 2nd feature map\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    # first order here!\n",
    "#     for t in sentence_tokens:\n",
    "#         bow_feature[freq_bucket_map[token_frequency_map[t]]-1] += 1 # bucket count\n",
    "    # awesome :) second order here!\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        for j in range(i+1, len(sentence_tokens)):\n",
    "            t1 = sentence_tokens[i]\n",
    "            t2 = sentence_tokens[j]\n",
    "            index_tuple = [second_order_freq_bucket_map[token_frequency_map[t1]], second_order_freq_bucket_map[token_frequency_map[t2]]]\n",
    "            index_tuple.sort()\n",
    "            index_tuple = tuple(index_tuple)\n",
    "            second_order_bucket = second_order_freq_freq_bucket_map[task_token_freq_freq_map[index_tuple]]\n",
    "            bow_feature[second_order_bucket] += 1 # bucket count\n",
    "\n",
    "    if ex_index % 50000 == 0:\n",
    "        print(\"Example sentence: \" + sentence_combined)\n",
    "        print(bow_feature)\n",
    "    train_input_features.append(bow_feature)\n",
    "    train_label_ids.append(example[\"label\"])\n",
    "    \n",
    "train_input_features = torch.stack(train_input_features, dim=0)\n",
    "train_input_features = torch.tensor(train_input_features, dtype=torch.float)\n",
    "train_label_ids = torch.tensor(train_label_ids, dtype=torch.long)\n",
    "train_data = TensorDataset(train_input_features, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:03<00:00, 275.74it/s]\n",
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# FBoW feature vectors for validation split\n",
    "validation_input_features = []\n",
    "validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(eval_df)):\n",
    "    bow_feature = torch.zeros(bucket_length) # up-to 2nd feature map\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    # first order here!\n",
    "#     for t in sentence_tokens:\n",
    "#         bow_feature[freq_bucket_map[token_frequency_map[t]]] += 1 # bucket count\n",
    "    # awesome :) second order here!\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        for j in range(i+1, len(sentence_tokens)):\n",
    "            t1 = sentence_tokens[i]\n",
    "            t2 = sentence_tokens[j]\n",
    "            index_tuple = [second_order_freq_bucket_map[token_frequency_map[t1]], second_order_freq_bucket_map[token_frequency_map[t2]]]\n",
    "            index_tuple.sort()\n",
    "            index_tuple = tuple(index_tuple)\n",
    "            second_order_bucket = second_order_freq_freq_bucket_map[task_token_freq_freq_map[index_tuple]]\n",
    "            bow_feature[second_order_bucket] += 1 # bucket count\n",
    "\n",
    "    validation_input_features.append(bow_feature)\n",
    "    validation_label_ids.append(example[\"label\"])\n",
    "\n",
    "validation_input_features = torch.stack(validation_input_features, dim=0)\n",
    "validation_input_features = torch.tensor(validation_input_features, dtype=torch.float)\n",
    "validation_label_ids = torch.tensor(validation_label_ids, dtype=torch.long)\n",
    "validation_data = TensorDataset(validation_input_features, validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=per_device_train_batch_size*n_gpu)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart the model\n",
    "model = BOWClassifier(len(validation_label_ids.unique()), \n",
    "                      bucket_length)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1:  0.25873279749620853\n",
      "Macro-F1:  0.3433746682801147\n",
      "Macro-F1:  0.31520884150069767\n",
      "Macro-F1:  0.3291332569683085\n",
      "Macro-F1:  0.3463714061424099\n",
      "Macro-F1:  0.32128340114559434\n",
      "Macro-F1:  0.3415247552395213\n",
      "Macro-F1:  0.34479962453282703\n",
      "Macro-F1:  0.36502200120066014\n",
      "Macro-F1:  0.34001388113453407\n",
      "Macro-F1:  0.3493049053985164\n",
      "Macro-F1:  0.3487897051540782\n",
      "Macro-F1:  0.3479800452794131\n",
      "Macro-F1:  0.3324020343123328\n",
      "Macro-F1:  0.3597146385637638\n",
      "Macro-F1:  0.33780756363589975\n",
      "Macro-F1:  0.3486081302794075\n",
      "Macro-F1:  0.3541380630972311\n",
      "Macro-F1:  0.3556082327706669\n",
      "Macro-F1:  0.36584699421534056\n",
      "Macro-F1:  0.33734529846593353\n",
      "Macro-F1:  0.3325437764001331\n",
      "Macro-F1:  0.3524552669686695\n",
      "Macro-F1:  0.366254387697229\n",
      "Macro-F1:  0.33063860088555175\n",
      "Macro-F1:  0.3578676157061473\n",
      "Macro-F1:  0.3384012497650528\n",
      "Macro-F1:  0.35435357464465356\n",
      "Macro-F1:  0.3376253936250881\n",
      "Macro-F1:  0.34471741509248205\n",
      "Macro-F1:  0.33771097987796383\n",
      "Macro-F1:  0.3350617336387885\n",
      "Macro-F1:  0.36355816549105113\n",
      "Macro-F1:  0.34557309638831374\n",
      "Macro-F1:  0.3481790095293551\n",
      "Macro-F1:  0.3435822863122713\n",
      "Macro-F1:  0.33579769626161965\n",
      "Macro-F1:  0.35293276039649607\n",
      "Macro-F1:  0.3516192912149508\n",
      "Macro-F1:  0.36027660345589824\n",
      "Macro-F1:  0.34602603548874794\n",
      "Macro-F1:  0.3558222613848873\n",
      "Macro-F1:  0.35675582923175003\n",
      "Macro-F1:  0.3693923073275656\n",
      "Macro-F1:  0.35653979921386875\n",
      "Macro-F1:  0.3483328946657153\n",
      "Macro-F1:  0.342942347319168\n",
      "Macro-F1:  0.37103860198518274\n",
      "Macro-F1:  0.3325623601587352\n",
      "Macro-F1:  0.33842571644278546\n",
      "Best Macro-F1:  0.37103860198518274\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "num_train_epochs = 20\n",
    "max_score = -1\n",
    "for _ in range(int(num_train_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    # pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, _ = model(input_features, labels=label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # pbar.set_postfix({'train_loss': loss.tolist()})\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            # logger.info(\"***** Evaluation Interval Hit *****\")\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_label_ids = []\n",
    "            with torch.no_grad():\n",
    "                # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "                for step, batch in enumerate(validation_dataloader):\n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    input_features, label_ids = batch\n",
    "                    \n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        input_features = input_features.to(device)\n",
    "                        label_ids = label_ids.to(device)\n",
    "                    \n",
    "                    loss, logits = model(input_features, labels=label_ids)\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = label_ids.to('cpu').numpy()\n",
    "                    outputs = np.argmax(logits, axis=1)\n",
    "                    all_logits.append(outputs)\n",
    "                    all_label_ids.append(label_ids)\n",
    "                    \n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "            all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "            result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "            # print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "            print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])\n",
    "            if result_to_save[\"macro avg\"][\"f1-score\"] > max_score:\n",
    "                max_score = result_to_save[\"macro avg\"][\"f1-score\"]\n",
    "                    \n",
    "        global_step += 1\n",
    "print(\"Best Macro-F1: \", max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
