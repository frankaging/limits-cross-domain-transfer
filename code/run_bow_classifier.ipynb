{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab_mismatch_utils import *\n",
    "from data_formatter_utils import *\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from collections import OrderedDict \n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Load modules, mainly huggingface basic model handlers.\n",
    "# Make sure you install huggingface and other packages properly.\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"../huggingface_cache/\" # Not overload common dir \n",
    "                                                           # if run in shared resources.\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import is_main_process, EvaluationStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(inoculation_data_path, eval_data_path=None, test_data_path=None,\n",
    "                inoculation_step_sample_size=1.0, \n",
    "                eval_sample_limit=-1, seed=42):\n",
    "    \"\"\"\n",
    "    eval_data_path is not needed if it is a saved_to_disk \n",
    "    huggingface dataset.\n",
    "    \n",
    "    return type is already a huggingface dataset.\n",
    "    \"\"\"\n",
    "    pd_format = True\n",
    "    if inoculation_data_path.split(\".\")[-1] != \"tsv\":\n",
    "        if len(inoculation_data_path.split(\".\")) > 1:\n",
    "            logger.info(f\"***** Loading pre-loaded datasets from the disk directly! *****\")\n",
    "            pd_format = False\n",
    "            datasets = DatasetDict.load_from_disk(inoculation_data_path)\n",
    "            inoculation_step_sample_size = int(len(datasets[\"train\"]) * inoculation_step_sample_size)\n",
    "            logger.info(f\"***** Inoculation Sample Count: %s *****\"%(inoculation_step_sample_size))\n",
    "            # this may not always start for zero inoculation\n",
    "            datasets[\"train\"] = datasets[\"train\"].shuffle(seed=seed)\n",
    "            inoculation_train_df = datasets[\"train\"].select(range(inoculation_step_sample_size))\n",
    "            eval_df = datasets[\"validation\"]\n",
    "            datasets[\"validation\"] = datasets[\"validation\"].shuffle(seed=seed)\n",
    "            if eval_sample_limit != -1:\n",
    "                datasets[\"validation\"] = datasets[\"validation\"].select(range(eval_sample_limit))\n",
    "        else:\n",
    "            logger.info(f\"***** Loading downloaded huggingface datasets: {inoculation_data_path}! *****\")\n",
    "            pd_format = False\n",
    "            if inoculation_data_path in [\"sst3\", \"cola\", \"mnli\", \"snli\", \"mrps\", \"qnli\"]:\n",
    "                pass\n",
    "            raise NotImplementedError()\n",
    "    else:\n",
    "        train_df = pd.read_csv(inoculation_data_path, delimiter=\"\\t\")\n",
    "        eval_df = pd.read_csv(eval_data_path, delimiter=\"\\t\")\n",
    "        test_df = pd.read_csv(test_data_path, delimiter=\"\\t\")\n",
    "        inoculation_step_sample_size = int(len(train_df) * inoculation_step_sample_size)\n",
    "        logger.info(f\"***** Inoculation Sample Count: %s *****\"%(inoculation_step_sample_size))\n",
    "        # this may not always start for zero inoculation\n",
    "        inoculation_train_df = train_df.sample(n=inoculation_step_sample_size, \n",
    "                                               replace=False, \n",
    "                                               random_state=seed) # seed here could not a little annoying.\n",
    "    if pd_format:\n",
    "        datasets = {}\n",
    "        datasets[\"train\"] = Dataset.from_pandas(inoculation_train_df)\n",
    "        datasets[\"validation\"] = Dataset.from_pandas(eval_df)\n",
    "        datasets[\"test\"] = Dataset.from_pandas(test_df)\n",
    "    else:\n",
    "        datasets = {}\n",
    "        datasets[\"train\"] = inoculation_train_df\n",
    "        datasets[\"validation\"] = eval_df\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_CONFIG = {\n",
    "    \"sst3\": (\"text\", None),\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"snli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\")\n",
    "}\n",
    "# WARNING: you dont need BERT tokenizer\n",
    "# original_vocab = load_bert_vocab(\"../data-files/bert_vocab.txt\")\n",
    "# original_tokenizer = transformers.BertTokenizer(\n",
    "#     vocab_file=\"../data-files/bert_vocab.txt\")\n",
    "# Just use some basic white space tokenizor here!\n",
    "modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "max_length = 128\n",
    "per_device_train_batch_size = 128\n",
    "per_device_eval_batch_size = 128\n",
    "no_cuda = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count() if not no_cuda else 1 # 1 means just on cpu\n",
    "seed = 42\n",
    "lr = 1e-3\n",
    "num_train_epochs = 10\n",
    "task_name = \"sst3\"\n",
    "sentence1_key, sentence2_key = TASK_CONFIG[task_name]\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/14/2021 23:57:02 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "03/14/2021 23:57:02 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/14/2021 23:57:02 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n",
      "03/14/2021 23:57:02 - INFO - __main__ - ***** Test Sample Count (Verify): 2210 *****\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "datasets = get_dataset(f\"../data-files/{data_file_name}/{data_file_name}-train.tsv\", \n",
    "                       f\"../data-files/{data_file_name}/{data_file_name}-dev.tsv\", \n",
    "                       f\"../data-files/{data_file_name}/{data_file_name}-test.tsv\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "logger.info(f\"***** Test Sample Count (Verify): %s *****\"%(len(datasets[\"test\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:18<00:00, 8413.97it/s]\n",
      "100%|██████████| 1100/1100 [00:00<00:00, 4097.65it/s]\n",
      "100%|██████████| 2210/2210 [00:00<00:00, 4186.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# create the vocab file\n",
    "vocab_index = 0\n",
    "original_vocab = OrderedDict()\n",
    "if \"train\" in datasets:\n",
    "    for (ex_index, example) in enumerate(tqdm(datasets[\"train\"])):\n",
    "        if sentence2_key is None:\n",
    "            sentence_combined = example[sentence1_key]\n",
    "        else:\n",
    "            sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "        sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "        for token in sentence_tokens:\n",
    "            if token not in original_vocab.keys():\n",
    "                original_vocab[token] = vocab_index\n",
    "                vocab_index += 1\n",
    "train_data_only = False\n",
    "if not train_data_only:\n",
    "    if \"validation\" in datasets:\n",
    "        for (ex_index, example) in enumerate(tqdm(datasets[\"validation\"])):\n",
    "            if sentence2_key is None:\n",
    "                sentence_combined = example[sentence1_key]\n",
    "            else:\n",
    "                sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "            sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "            for token in sentence_tokens:\n",
    "                if token not in original_vocab.keys():\n",
    "                    original_vocab[token] = vocab_index\n",
    "                    vocab_index += 1\n",
    "\n",
    "    if \"test\" in datasets:\n",
    "        for (ex_index, example) in enumerate(tqdm(datasets[\"test\"])):\n",
    "            if sentence2_key is None:\n",
    "                sentence_combined = example[sentence1_key]\n",
    "            else:\n",
    "                sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "            sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "            for token in sentence_tokens:\n",
    "                if token not in original_vocab.keys():\n",
    "                    original_vocab[token] = vocab_index\n",
    "                    vocab_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 317/159274 [00:00<00:59, 2652.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: Surprisingly, considering that Baird is a former film editor, the movie is rather choppy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 50471/159274 [00:13<00:29, 3718.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: achronological\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 100714/159274 [00:26<00:16, 3649.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: Show\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 150740/159274 [00:40<00:02, 3691.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: picked me up ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159274/159274 [00:43<00:00, 3692.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_input_features = []\n",
    "train_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(datasets[\"train\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    if ex_index % 50000 == 0:\n",
    "        print(\"Example sentence: \" + sentence_combined)\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    train_input_features.append(bow_feature)\n",
    "    train_label_ids.append(example[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_input_features = torch.stack(train_input_features, dim=0)\n",
    "train_input_features = torch.tensor(train_input_features, dtype=torch.float)\n",
    "train_label_ids = torch.tensor(train_label_ids, dtype=torch.long)\n",
    "train_data = TensorDataset(train_input_features, train_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:00<00:00, 1962.63it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_input_features = []\n",
    "validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        if t in original_vocab.keys():\n",
    "            bow_feature[original_vocab[t]] += 1\n",
    "    validation_input_features.append(bow_feature)\n",
    "    validation_label_ids.append(example[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "validation_input_features = torch.stack(validation_input_features, dim=0)\n",
    "validation_input_features = torch.tensor(validation_input_features, dtype=torch.float)\n",
    "validation_label_ids = torch.tensor(validation_label_ids, dtype=torch.long)\n",
    "validation_data = TensorDataset(validation_input_features, validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=per_device_train_batch_size*n_gpu)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.classifier = nn.Linear(vocab_size, num_labels)\n",
    "    def forward(self, x, labels=None):\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BOWClassifier(len(validation_label_ids.unique()), len(original_vocab))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if n_gpu > 0 and not no_cuda:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:34 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.41014   0.62383   0.49490       428\n",
      "           1    0.39572   0.16667   0.23455       444\n",
      "           2    0.23664   0.27193   0.25306       228\n",
      "\n",
      "    accuracy                        0.36636      1100\n",
      "   macro avg    0.34750   0.35414   0.32750      1100\n",
      "weighted avg    0.36836   0.36636   0.33969      1100\n",
      "\n",
      "Macro-F1:  0.3275040827127372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:36 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.69416   0.47196   0.56189       428\n",
      "           1    0.62500   0.77703   0.69277       444\n",
      "           2    0.31518   0.35526   0.33402       228\n",
      "\n",
      "    accuracy                        0.57091      1100\n",
      "   macro avg    0.54478   0.53475   0.52956      1100\n",
      "weighted avg    0.58769   0.57091   0.56749      1100\n",
      "\n",
      "Macro-F1:  0.5295610729628291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:39 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66754   0.59579   0.62963       428\n",
      "           1    0.66414   0.78829   0.72091       444\n",
      "           2    0.35602   0.29825   0.32458       228\n",
      "\n",
      "    accuracy                        0.61182      1100\n",
      "   macro avg    0.56257   0.56078   0.55837      1100\n",
      "weighted avg    0.60160   0.61182   0.60324      1100\n",
      "\n",
      "Macro-F1:  0.5583727502383646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:41 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66919   0.61916   0.64320       428\n",
      "           1    0.67692   0.79279   0.73029       444\n",
      "           2    0.35326   0.28509   0.31553       228\n",
      "\n",
      "    accuracy                        0.62000      1100\n",
      "   macro avg    0.56646   0.56568   0.56301      1100\n",
      "weighted avg    0.60683   0.62000   0.61044      1100\n",
      "\n",
      "Macro-F1:  0.5630094401697351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:44 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.66430   0.65654   0.66040       428\n",
      "           1    0.69261   0.78153   0.73439       444\n",
      "           2    0.35227   0.27193   0.30693       228\n",
      "\n",
      "    accuracy                        0.62727      1100\n",
      "   macro avg    0.56973   0.57000   0.56724      1100\n",
      "weighted avg    0.61106   0.62727   0.61700      1100\n",
      "\n",
      "Macro-F1:  0.5672405858085295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:48 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.67990   0.64019   0.65945       428\n",
      "           1    0.68582   0.80631   0.74120       444\n",
      "           2    0.35429   0.27193   0.30769       228\n",
      "\n",
      "    accuracy                        0.63091      1100\n",
      "   macro avg    0.57334   0.57281   0.56945      1100\n",
      "weighted avg    0.61480   0.63091   0.61954      1100\n",
      "\n",
      "Macro-F1:  0.5694465286366087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:52 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68550   0.65187   0.66826       428\n",
      "           1    0.69246   0.80631   0.74506       444\n",
      "           2    0.38068   0.29386   0.33168       228\n",
      "\n",
      "    accuracy                        0.64000      1100\n",
      "   macro avg    0.58621   0.58401   0.58167      1100\n",
      "weighted avg    0.62513   0.64000   0.62950      1100\n",
      "\n",
      "Macro-F1:  0.5816679578068906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:04:57 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.68599   0.66355   0.67458       428\n",
      "           1    0.69941   0.80180   0.74711       444\n",
      "           2    0.37853   0.29386   0.33086       228\n",
      "\n",
      "    accuracy                        0.64273      1100\n",
      "   macro avg    0.58798   0.58640   0.58419      1100\n",
      "weighted avg    0.62768   0.64273   0.63262      1100\n",
      "\n",
      "Macro-F1:  0.5841876320756892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:04 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70398   0.66121   0.68193       428\n",
      "           1    0.69364   0.81081   0.74766       444\n",
      "           2    0.39106   0.30702   0.34398       228\n",
      "\n",
      "    accuracy                        0.64818      1100\n",
      "   macro avg    0.59623   0.59301   0.59119      1100\n",
      "weighted avg    0.63495   0.64818   0.63841      1100\n",
      "\n",
      "Macro-F1:  0.5911905354085288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:10 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.69927   0.66822   0.68339       428\n",
      "           1    0.69591   0.80405   0.74608       444\n",
      "           2    0.39888   0.31140   0.34975       228\n",
      "\n",
      "    accuracy                        0.64909      1100\n",
      "   macro avg    0.59802   0.59456   0.59308      1100\n",
      "weighted avg    0.63565   0.64909   0.63954      1100\n",
      "\n",
      "Macro-F1:  0.5930760899244399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:15 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70218   0.67757   0.68966       428\n",
      "           1    0.70472   0.80631   0.75210       444\n",
      "           2    0.39665   0.31140   0.34889       228\n",
      "\n",
      "    accuracy                        0.65364      1100\n",
      "   macro avg    0.60118   0.59843   0.59688      1100\n",
      "weighted avg    0.63988   0.65364   0.64423      1100\n",
      "\n",
      "Macro-F1:  0.5968834538814255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:17 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71139   0.65654   0.68287       428\n",
      "           1    0.70541   0.79279   0.74655       444\n",
      "           2    0.38350   0.34649   0.36406       228\n",
      "\n",
      "    accuracy                        0.64727      1100\n",
      "   macro avg    0.60010   0.59861   0.59783      1100\n",
      "weighted avg    0.64101   0.64727   0.64249      1100\n",
      "\n",
      "Macro-F1:  0.5978254699156306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:19 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70707   0.65421   0.67961       428\n",
      "           1    0.69472   0.79955   0.74346       444\n",
      "           2    0.38342   0.32456   0.35154       228\n",
      "\n",
      "    accuracy                        0.64455      1100\n",
      "   macro avg    0.59507   0.59277   0.59154      1100\n",
      "weighted avg    0.63500   0.64455   0.63738      1100\n",
      "\n",
      "Macro-F1:  0.5915370302868367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:22 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70443   0.66822   0.68585       428\n",
      "           1    0.70766   0.79054   0.74681       444\n",
      "           2    0.39899   0.34649   0.37089       228\n",
      "\n",
      "    accuracy                        0.65091      1100\n",
      "   macro avg    0.60369   0.60175   0.60118      1100\n",
      "weighted avg    0.64243   0.65091   0.64517      1100\n",
      "\n",
      "Macro-F1:  0.6011839494541615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:24 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70976   0.67991   0.69451       428\n",
      "           1    0.69574   0.80856   0.74792       444\n",
      "           2    0.40805   0.31140   0.35323       228\n",
      "\n",
      "    accuracy                        0.65545      1100\n",
      "   macro avg    0.60451   0.59996   0.59855      1100\n",
      "weighted avg    0.64156   0.65545   0.64533      1100\n",
      "\n",
      "Macro-F1:  0.5985537457897466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:26 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71608   0.66589   0.69007       428\n",
      "           1    0.69709   0.80856   0.74870       444\n",
      "           2    0.41176   0.33772   0.37108       228\n",
      "\n",
      "    accuracy                        0.65545      1100\n",
      "   macro avg    0.60831   0.60406   0.60328      1100\n",
      "weighted avg    0.64534   0.65545   0.64762      1100\n",
      "\n",
      "Macro-F1:  0.6032845118300387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:29 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71867   0.65654   0.68620       428\n",
      "           1    0.69608   0.79955   0.74423       444\n",
      "           2    0.39196   0.34211   0.36534       228\n",
      "\n",
      "    accuracy                        0.64909      1100\n",
      "   macro avg    0.60224   0.59940   0.59859      1100\n",
      "weighted avg    0.64183   0.64909   0.64312      1100\n",
      "\n",
      "Macro-F1:  0.5985923551651977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:31 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70812   0.65187   0.67883       428\n",
      "           1    0.70935   0.78604   0.74573       444\n",
      "           2    0.37850   0.35526   0.36652       228\n",
      "\n",
      "    accuracy                        0.64455      1100\n",
      "   macro avg    0.59866   0.59772   0.59702      1100\n",
      "weighted avg    0.64030   0.64455   0.64110      1100\n",
      "\n",
      "Macro-F1:  0.5970248165396298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:33 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70781   0.65654   0.68121       428\n",
      "           1    0.70876   0.78378   0.74439       444\n",
      "           2    0.38208   0.35526   0.36818       228\n",
      "\n",
      "    accuracy                        0.64545      1100\n",
      "   macro avg    0.59955   0.59853   0.59793      1100\n",
      "weighted avg    0.64068   0.64545   0.64183      1100\n",
      "\n",
      "Macro-F1:  0.5979263220439691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:36 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.70812   0.65187   0.67883       428\n",
      "           1    0.71341   0.79054   0.75000       444\n",
      "           2    0.37383   0.35088   0.36199       228\n",
      "\n",
      "    accuracy                        0.64545      1100\n",
      "   macro avg    0.59846   0.59776   0.59694      1100\n",
      "weighted avg    0.64097   0.64545   0.64189      1100\n",
      "\n",
      "Macro-F1:  0.5969410223381885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:39 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72051   0.65654   0.68704       428\n",
      "           1    0.69608   0.79955   0.74423       444\n",
      "           2    0.38000   0.33333   0.35514       228\n",
      "\n",
      "    accuracy                        0.64727      1100\n",
      "   macro avg    0.59886   0.59647   0.59547      1100\n",
      "weighted avg    0.64007   0.64727   0.64133      1100\n",
      "\n",
      "Macro-F1:  0.5954721841822127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:41 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72063   0.64486   0.68064       428\n",
      "           1    0.70020   0.78378   0.73964       444\n",
      "           2    0.36364   0.35088   0.35714       228\n",
      "\n",
      "    accuracy                        0.64000      1100\n",
      "   macro avg    0.59482   0.59317   0.59247      1100\n",
      "weighted avg    0.63839   0.64000   0.63740      1100\n",
      "\n",
      "Macro-F1:  0.592474241039859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:44 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.71899   0.66355   0.69016       428\n",
      "           1    0.70243   0.78153   0.73987       444\n",
      "           2    0.37441   0.34649   0.35991       228\n",
      "\n",
      "    accuracy                        0.64545      1100\n",
      "   macro avg    0.59861   0.59719   0.59665      1100\n",
      "weighted avg    0.64088   0.64545   0.64177      1100\n",
      "\n",
      "Macro-F1:  0.5966463035816281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:46 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72379   0.66121   0.69109       428\n",
      "           1    0.70423   0.78829   0.74389       444\n",
      "           2    0.38208   0.35526   0.36818       228\n",
      "\n",
      "    accuracy                        0.64909      1100\n",
      "   macro avg    0.60336   0.60159   0.60105      1100\n",
      "weighted avg    0.64506   0.64909   0.64547      1100\n",
      "\n",
      "Macro-F1:  0.6010526628486246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:05:49 - INFO - __main__ - ***** Evaluation Interval Hit *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.73387   0.63785   0.68250       428\n",
      "           1    0.70341   0.79054   0.74443       444\n",
      "           2    0.36681   0.36842   0.36761       228\n",
      "\n",
      "    accuracy                        0.64364      1100\n",
      "   macro avg    0.60136   0.59894   0.59818      1100\n",
      "weighted avg    0.64549   0.64364   0.64223      1100\n",
      "\n",
      "Macro-F1:  0.5981825137892708\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "for _ in range(int(num_train_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    # pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, _ = model(input_features, labels=label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        # pbar.set_postfix({'train_loss': loss.tolist()})\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            logger.info(\"***** Evaluation Interval Hit *****\")\n",
    "            model.eval()\n",
    "            all_logits = []\n",
    "            all_label_ids = []\n",
    "            with torch.no_grad():\n",
    "                # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "                for step, batch in enumerate(validation_dataloader):\n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                    input_features, label_ids = batch\n",
    "                    \n",
    "                    if torch.cuda.is_available() and not no_cuda:\n",
    "                        input_features = input_features.to(device)\n",
    "                        label_ids = label_ids.to(device)\n",
    "                    \n",
    "                    loss, logits = model(input_features, labels=label_ids)\n",
    "                    logits = F.softmax(logits, dim=-1)\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    label_ids = label_ids.to('cpu').numpy()\n",
    "                    outputs = np.argmax(logits, axis=1)\n",
    "                    all_logits.append(outputs)\n",
    "                    all_label_ids.append(label_ids)\n",
    "                    \n",
    "            all_logits = np.concatenate(all_logits, axis=0)\n",
    "            all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "            result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "            print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "            print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])\n",
    "                    \n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate with corrupt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:18:06 - INFO - __main__ - ***** Loading pre-loaded datasets from the disk directly! *****\n",
      "03/15/2021 00:18:06 - INFO - __main__ - ***** Inoculation Sample Count: 159274 *****\n",
      "03/15/2021 00:18:06 - INFO - __main__ - ***** Train Sample Count (Verify): 159274 *****\n",
      "03/15/2021 00:18:06 - INFO - __main__ - ***** Valid Sample Count (Verify): 1100 *****\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "corrupt_method = \"S3\"\n",
    "data_file_name = task_name if task_name != \"sst3\" else \"sst-tenary\"\n",
    "corrupt_datasets = get_dataset(f\"../data-files/{data_file_name}-corrupted-{corrupt_method}\")\n",
    "logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:00<00:00, 1194.33it/s]\n"
     ]
    }
   ],
   "source": [
    "corrupt_validation_input_features = []\n",
    "corrupt_validation_label_ids = []\n",
    "for (ex_index, example) in enumerate(tqdm(corrupt_datasets[\"validation\"])):\n",
    "    bow_feature = torch.zeros(len(original_vocab))\n",
    "    if sentence2_key is None:\n",
    "        sentence_combined = example[sentence1_key]\n",
    "    else:\n",
    "        sentence_combined = example[sentence1_key] + \" [SEP] \" + example[sentence2_key]\n",
    "    sentence_tokens = modified_basic_tokenizer.tokenize(sentence_combined)\n",
    "    sentence_tokens = sentence_tokens[:max_length]\n",
    "    for t in sentence_tokens:\n",
    "        bow_feature[original_vocab[t]] += 1\n",
    "    corrupt_validation_input_features.append(bow_feature)\n",
    "    corrupt_validation_label_ids.append(example[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cs.stanford.edu/u/wuzhengx/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "corrupt_validation_input_features = torch.stack(corrupt_validation_input_features, dim=0)\n",
    "corrupt_validation_input_features = torch.tensor(corrupt_validation_input_features, dtype=torch.float)\n",
    "corrupt_validation_label_ids = torch.tensor(corrupt_validation_label_ids, dtype=torch.long)\n",
    "corrupt_validation_data = TensorDataset(corrupt_validation_input_features, corrupt_validation_label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_validation_dataloader = DataLoader(corrupt_validation_data, batch_size=per_device_eval_batch_size*n_gpu, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/15/2021 00:18:12 - INFO - __main__ - ***** Evaluation With Corrupt Data *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.42553   0.04673   0.08421       428\n",
      "           1    0.38776   0.04279   0.07708       444\n",
      "           2    0.21016   0.92544   0.34253       228\n",
      "\n",
      "    accuracy                        0.22727      1100\n",
      "   macro avg    0.34115   0.33832   0.16794      1100\n",
      "weighted avg    0.36564   0.22727   0.13488      1100\n",
      "\n",
      "Macro-F1:  0.16794070045110934\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Evaluation With Corrupt Data *****\")\n",
    "model.eval()\n",
    "all_logits = []\n",
    "all_label_ids = []\n",
    "with torch.no_grad():\n",
    "    # pbar = tqdm(validation_dataloader, desc=\"Iteration\")\n",
    "    for step, batch in enumerate(corrupt_validation_dataloader):\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        input_features, label_ids = batch\n",
    "\n",
    "        if torch.cuda.is_available() and not no_cuda:\n",
    "            input_features = input_features.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "        loss, logits = model(input_features, labels=label_ids)\n",
    "        logits = F.softmax(logits, dim=-1)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        outputs = np.argmax(logits, axis=1)\n",
    "        all_logits.append(outputs)\n",
    "        all_label_ids.append(label_ids)\n",
    "\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_label_ids = np.concatenate(all_label_ids, axis=0)\n",
    "result_to_save = classification_report(all_label_ids, all_logits, digits=5, output_dict=True)\n",
    "print(classification_report(all_label_ids, all_logits, digits=5))\n",
    "print(\"Macro-F1: \", result_to_save[\"macro avg\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate with other type corrupt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
